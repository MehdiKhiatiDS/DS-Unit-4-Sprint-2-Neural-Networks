diff --git a/.DS_Store b/.DS_Store
index 618694a..ad9a889 100644
Binary files a/.DS_Store and b/.DS_Store differ
diff --git a/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb b/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb
index af0d88b..c1fb68c 100644
--- a/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb
+++ b/module1-Intro-to-Neural-Networks/LS_DS_421_Intro_to_NN_Assignment.ipynb
@@ -26,13 +26,13 @@
     "## Define the Following:\n",
     "You can add image, diagrams, whatever you need to ensure that you understand the concepts below.\n",
     "\n",
-    "### Input Layer:\n",
-    "### Hidden Layer:\n",
-    "### Output Layer:\n",
-    "### Neuron:\n",
-    "### Weight:\n",
-    "### Activation Function:\n",
-    "### Node Map:\n",
+    "### Input Layer:  Input neurones,  they'll be two input neurons that taking our model or each of the raw data\n",
+    "### Hidden Layer: Where our ensemble of predictive models is stored\n",
+    "### Output Layer: one output neuron for each variable we want to predict\n",
+    "### Neuron: cell\n",
+    "### Weight: numbers, paramters the network connection between input neuron and second layer neuron should have\n",
+    "### Activation Function: a combination (sum) of all activations from first layer and their weights, negatives used to show dark nodes (sigmoid function) thats comes up with a number between 0 and 1, well for each neuron!\n",
+    "### Node Map: diagrams showcasing inputs, hiden cells, and output layers \n",
     "### Perceptron:\n"
    ]
   },
@@ -55,7 +55,7 @@
     "id": "PlSwIJMC0A8F"
    },
    "source": [
-    "#### Your Answer Here"
+    "#### Let's say We have bunch of numbers that we make into an arrays then a vector, basically input layer takes all that infomation in numbers, depending on the weights it'll lit up the next layer minus teh bias through the activation function calculation later (but for faster training the relu functions makes it faster and simpler to implement!) all the way to the ouput layer."
    ]
   },
   {
@@ -77,11 +77,12 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 13,
    "metadata": {},
    "outputs": [],
    "source": [
     "import pandas as pd\n",
+    "import numpy as np\n",
     "data = { 'x1': [0,1,0,1],\n",
     "         'x2': [0,0,1,1],\n",
     "         'y':  [1,1,1,0]\n",
@@ -92,24 +93,325 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 65,
    "metadata": {
     "colab": {},
     "colab_type": "code",
     "id": "Sgh7VFGwnXGH"
    },
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>x1</th>\n",
+       "      <th>x2</th>\n",
+       "      <th>y</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>1</td>\n",
+       "      <td>1</td>\n",
+       "      <td>0</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "   x1  x2  y\n",
+       "0   0   0  1\n",
+       "1   1   0  1\n",
+       "2   0   1  1\n",
+       "3   1   1  0"
+      ]
+     },
+     "execution_count": 65,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "df.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 78,
+   "metadata": {},
    "outputs": [],
    "source": [
-    "##### Your Code Here #####\n",
-    "\n"
+    "correct_outputs = [[1], [1], [1], [0]]"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 79,
    "metadata": {},
    "outputs": [],
-   "source": []
+   "source": [
+    "def sigmoid(x):\n",
+    "    return 1 / (1 + np.exp(-x))\n",
+    "\n",
+    "def sigmoid_derivative(x):\n",
+    "    sx = sigmoid(x)\n",
+    "    return sx * (1 - sx)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 80,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# initialze random weights for inputs"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 81,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "weights = 2 * np.random.random((3,1)) - 1"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 82,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[ 0.53937821],\n",
+       "       [ 0.60096248],\n",
+       "       [-0.44532557]])"
+      ]
+     },
+     "execution_count": 82,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "weights"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 83,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# Calculate the weighted sum of inputs ans weights"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 84,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[-0.44532557],\n",
+       "       [ 0.09405264],\n",
+       "       [ 0.1556369 ],\n",
+       "       [ 1.14034069]])"
+      ]
+     },
+     "execution_count": 84,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "weighted_sum = np.dot(df, weights)\n",
+    "weighted_sum"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 85,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.39047273],\n",
+       "       [0.52349584],\n",
+       "       [0.53883087],\n",
+       "       [0.75774218]])"
+      ]
+     },
+     "execution_count": 85,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "# Activated value \n",
+    "\n",
+    "activated_output = sigmoid(weighted_sum)\n",
+    "activated_output "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 86,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[ 0.60952727],\n",
+       "       [ 0.47650416],\n",
+       "       [ 0.46116913],\n",
+       "       [-0.75774218]])"
+      ]
+     },
+     "execution_count": 86,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "error = correct_outputs - activated_output\n",
+    "error"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 87,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.24070772],\n",
+       "       [0.233625  ],\n",
+       "       [0.23269712],\n",
+       "       [0.21728845]])"
+      ]
+     },
+     "execution_count": 87,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "adjusted = error = sigmoid_derivative(activated_output)\n",
+    "adjusted"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 88,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[0.99029166],\n",
+       "       [1.05094805],\n",
+       "       [0.26170426]])"
+      ]
+     },
+     "execution_count": 88,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "weights += np.dot(df.T, adjusted)\n",
+    "weights"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 89,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "weights after training\n",
+      "[[-4.95403962]\n",
+      " [-4.95395383]\n",
+      " [15.13425447]]\n",
+      "Output after training\n",
+      "[[9.99999733e-01]\n",
+      " [9.99962088e-01]\n",
+      " [9.99962091e-01]\n",
+      " [4.97732313e-05]]\n"
+     ]
+    }
+   ],
+   "source": [
+    "for iteration in range (100000):\n",
+    "    weighted_sum = np.dot(df, weights)\n",
+    "    \n",
+    "    activated_output = sigmoid(weighted_sum)\n",
+    "    \n",
+    "    error = correct_ouputs  - activated_output\n",
+    "    adjustments = error * sigmoid_derivative(activated_output)\n",
+    "    \n",
+    "    weights += np.dot(df.T, adjustments)\n",
+    " \n",
+    "print('weights after training')\n",
+    "print(weights)\n",
+    "\n",
+    "print('Output after training')\n",
+    "print(activated_output)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 90,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#and that ladies and gentlmen is 99%"
+   ]
   },
   {
    "cell_type": "markdown",
@@ -126,7 +428,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 93,
    "metadata": {},
    "outputs": [
     {
@@ -163,7 +465,7 @@
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
-       "      <th>0</th>\n",
+       "      <td>0</td>\n",
        "      <td>6</td>\n",
        "      <td>148</td>\n",
        "      <td>72</td>\n",
@@ -175,7 +477,7 @@
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>1</th>\n",
+       "      <td>1</td>\n",
        "      <td>1</td>\n",
        "      <td>85</td>\n",
        "      <td>66</td>\n",
@@ -187,7 +489,7 @@
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>2</th>\n",
+       "      <td>2</td>\n",
        "      <td>8</td>\n",
        "      <td>183</td>\n",
        "      <td>64</td>\n",
@@ -199,7 +501,7 @@
        "      <td>1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>3</th>\n",
+       "      <td>3</td>\n",
        "      <td>1</td>\n",
        "      <td>89</td>\n",
        "      <td>66</td>\n",
@@ -211,7 +513,7 @@
        "      <td>0</td>\n",
        "    </tr>\n",
        "    <tr>\n",
-       "      <th>4</th>\n",
+       "      <td>4</td>\n",
        "      <td>0</td>\n",
        "      <td>137</td>\n",
        "      <td>40</td>\n",
@@ -242,7 +544,7 @@
        "4                     2.288   33        1  "
       ]
      },
-     "execution_count": 4,
+     "execution_count": 93,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -261,7 +563,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 10,
+   "execution_count": 98,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -269,54 +571,188 @@
     "\n",
     "feats = list(diabetes)[:-1]\n",
     "\n",
-    "X = ..."
+    "scaler = MinMaxScaler()\n",
+    "\n",
+    "X = scaler.fit_transform(diabetes[feats])"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 99,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
+       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
+       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.])"
+      ]
+     },
+     "execution_count": 99,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "y = diabetes.values[:,-1]\n",
+    "y[:50]"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 113,
    "metadata": {
     "colab": {},
     "colab_type": "code",
     "id": "-W0tiX1F1hh2"
    },
-   "outputs": [],
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'self' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-113-39dc2c21bb9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##### Update this Class #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPerceptron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mniter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m<ipython-input-113-39dc2c21bb9e>\u001b[0m in \u001b[0;36mPerceptron\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mniter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Weighted sum of inputs / weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mweighted_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
+     ]
+    }
+   ],
    "source": [
     "##### Update this Class #####\n",
     "\n",
-    "class Perceptron(object):\n",
+    "class Perceptron:\n",
     "    \n",
     "    def __init__(self, niter = 10):\n",
-    "    self.niter = niter\n",
+    "        self.niter = 'niter'\n",
     "    \n",
     "    def __sigmoid(self, x):\n",
-    "        return None\n",
+    "        return 1 / (1 + np.exp(-x))\n",
     "    \n",
     "    def __sigmoid_derivative(self, x):\n",
-    "        return None\n",
+    "        sx = sigmoid(x)\n",
+    "        return sx * (1 - sx)\n",
     "\n",
     "    def fit(self, X, y):\n",
-    "    \"\"\"Fit training data\n",
-    "    X : Training vectors, X.shape : [#samples, #features]\n",
-    "    y : Target values, y.shape : [#samples]\n",
-    "    \"\"\"\n",
-    "\n",
-    "    # Randomly Initialize Weights\n",
-    "    weights = ...\n",
+    "        \"\"\"Fit training data\n",
+    "        X : Training vectors, X.shape : [#samples, #features]\n",
+    "        y : Target values, y.shape : [#samples]\n",
+    "        \"\"\"\n",
+    "#       add in bias\n",
+    "#       X = np.append(X_, np.ones_like(X_[0]), axis=1)\n",
+    "        # Randomly Initialize Weights\n",
+    "        self.weights = np.random.random((X.shape[1], 1))\n",
     "\n",
     "    for i in range(self.niter):\n",
     "        # Weighted sum of inputs / weights\n",
-    "\n",
+    "        weighted_sum = np.dot(X, self.weights)\n",
     "        # Activate!\n",
-    "\n",
+    "        activated_ouput = self.__sigmoid(weighted_sum)\n",
     "        # Cac error\n",
-    "\n",
+    "        error = y - activated_output\n",
     "        # Update the Weights\n",
     "\n",
     "\n",
     "    def predict(self, X):\n",
-    "    \"\"\"Return class label after unit step\"\"\"\n",
-    "        return None"
+    "        \"\"\"Return class label after unit step\"\"\"\n",
+    "    \n",
+    "        weighted_sum = np.dot(X, self.weights)\n",
+    "        activated_output = self.__signoid(weighted_sum)\n",
+    "        \n",
+    "        return activated_output"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "p = Perceptron()\n",
+    "p.fit(X,y)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "p.perdict(X)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "y - p.predict(X)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 3,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "class Perceptron:\n",
+    "    def __init__(self):\n",
+    "        \n",
+    "        self.inputs = 2 \n",
+    "        self.outputs = 1\n",
+    "        \n",
+    "        self.weights = np.random.randn(self.inputs, sefl.outputs)\n",
+    "        bias = 1\n",
+    "        \n",
+    "    def sigmoid(self, s):\n",
+    "        return 1 / (1+np.exp(-s))\n",
+    "    \n",
+    "    def sigmoidPrime(self, s):\n",
+    "        return s * (1 - s)\n",
+    "    \n",
+    "    def feed_forward(self, X):\n",
+    "        \"\"\" Calculate the NN inference using feed forward aka predict\"\"\"\n",
+    "        \n",
+    "        # Weighted sum of inputs +> hidden layer\n",
+    "        self.weighted_sum = np.dot(X, self.weights) + bias\n",
+    "        "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Using TensorFlow backend.\n"
+     ]
+    }
+   ],
+   "source": [
+    "from keras.models import Sequential"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 108,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import tensorflow as tf"
    ]
   },
   {
@@ -357,9 +793,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
diff --git a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
index 307b203..f0307d4 100644
--- a/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
+++ b/module2-backpropagation/LS_DS_422_Backprop_Assignment.ipynb
@@ -73,7 +73,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 39,
+   "execution_count": 1,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -94,9 +94,18 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 31,
+   "execution_count": 3,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
+      "11493376/11490434 [==============================] - 3s 0us/step\n"
+     ]
+    }
+   ],
    "source": [
     "# the data, split between train and test sets\n",
     "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
@@ -242,9 +251,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
diff --git a/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb b/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
index 2457723..bb8cc78 100644
--- a/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
+++ b/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
@@ -32,7 +32,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 56,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -40,9 +40,1818 @@
    },
    "outputs": [],
    "source": [
-    "##### Your Code Here #####"
+    "##### Your Code Here #####\n",
+    "# fit 2 model, regression and an image\n",
+    "# normalize data no need for activation for last layer\n",
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Dense\n",
+    "import pandas as pd\n",
+    "from sklearn.model_selection import train_test_split"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 2,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Using TensorFlow backend.\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz\n",
+      "57344/57026 [==============================] - 0s 3us/step\n"
+     ]
+    }
+   ],
+   "source": [
+    "from keras.datasets import boston_housing\n",
+    "\n",
+    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 11,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>[[1.23247, 0.0, 8.14, 0.0, 0.538, 6.142, 91.7,...</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>[15.2, 42.3, 50.0, 21.1, 17.7, 18.5, 11.3, 15....</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "                                                   0\n",
+       "0  [[1.23247, 0.0, 8.14, 0.0, 0.538, 6.142, 91.7,...\n",
+       "1  [15.2, 42.3, 50.0, 21.1, 17.7, 18.5, 11.3, 15...."
+      ]
+     },
+     "execution_count": 11,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "df = pd.DataFrame((x_train, y_train))\n",
+    "df.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 207,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
+      " 'B' 'LSTAT']\n"
+     ]
+    }
+   ],
+   "source": [
+    "from sklearn.datasets import load_boston\n",
+    "boston = load_boston()\n",
+    "print(boston.feature_names)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 208,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>0</th>\n",
+       "      <th>1</th>\n",
+       "      <th>2</th>\n",
+       "      <th>3</th>\n",
+       "      <th>4</th>\n",
+       "      <th>5</th>\n",
+       "      <th>6</th>\n",
+       "      <th>7</th>\n",
+       "      <th>8</th>\n",
+       "      <th>9</th>\n",
+       "      <th>10</th>\n",
+       "      <th>11</th>\n",
+       "      <th>12</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.00632</td>\n",
+       "      <td>18.0</td>\n",
+       "      <td>2.31</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.538</td>\n",
+       "      <td>6.575</td>\n",
+       "      <td>65.2</td>\n",
+       "      <td>4.0900</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>296.0</td>\n",
+       "      <td>15.3</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>4.98</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.02731</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>7.07</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.469</td>\n",
+       "      <td>6.421</td>\n",
+       "      <td>78.9</td>\n",
+       "      <td>4.9671</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>242.0</td>\n",
+       "      <td>17.8</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>9.14</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0.02729</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>7.07</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.469</td>\n",
+       "      <td>7.185</td>\n",
+       "      <td>61.1</td>\n",
+       "      <td>4.9671</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>242.0</td>\n",
+       "      <td>17.8</td>\n",
+       "      <td>392.83</td>\n",
+       "      <td>4.03</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0.03237</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2.18</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.458</td>\n",
+       "      <td>6.998</td>\n",
+       "      <td>45.8</td>\n",
+       "      <td>6.0622</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>222.0</td>\n",
+       "      <td>18.7</td>\n",
+       "      <td>394.63</td>\n",
+       "      <td>2.94</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>0.06905</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2.18</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.458</td>\n",
+       "      <td>7.147</td>\n",
+       "      <td>54.2</td>\n",
+       "      <td>6.0622</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>222.0</td>\n",
+       "      <td>18.7</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>5.33</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
+       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
+       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
+       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
+       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
+       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
+       "\n",
+       "       11    12  \n",
+       "0  396.90  4.98  \n",
+       "1  396.90  9.14  \n",
+       "2  392.83  4.03  \n",
+       "3  394.63  2.94  \n",
+       "4  396.90  5.33  "
+      ]
+     },
+     "execution_count": 208,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "boston1 = pd.DataFrame(boston.data)\n",
+    "boston1.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 209,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>CRIM</th>\n",
+       "      <th>ZN</th>\n",
+       "      <th>INDUS</th>\n",
+       "      <th>CHAS</th>\n",
+       "      <th>NOX</th>\n",
+       "      <th>RM</th>\n",
+       "      <th>AGE</th>\n",
+       "      <th>DIS</th>\n",
+       "      <th>RAD</th>\n",
+       "      <th>TAX</th>\n",
+       "      <th>PTRATIO</th>\n",
+       "      <th>B</th>\n",
+       "      <th>LSTAT</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.00632</td>\n",
+       "      <td>18.0</td>\n",
+       "      <td>2.31</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.538</td>\n",
+       "      <td>6.575</td>\n",
+       "      <td>65.2</td>\n",
+       "      <td>4.0900</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>296.0</td>\n",
+       "      <td>15.3</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>4.98</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.02731</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>7.07</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.469</td>\n",
+       "      <td>6.421</td>\n",
+       "      <td>78.9</td>\n",
+       "      <td>4.9671</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>242.0</td>\n",
+       "      <td>17.8</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>9.14</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0.02729</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>7.07</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.469</td>\n",
+       "      <td>7.185</td>\n",
+       "      <td>61.1</td>\n",
+       "      <td>4.9671</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>242.0</td>\n",
+       "      <td>17.8</td>\n",
+       "      <td>392.83</td>\n",
+       "      <td>4.03</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0.03237</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2.18</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.458</td>\n",
+       "      <td>6.998</td>\n",
+       "      <td>45.8</td>\n",
+       "      <td>6.0622</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>222.0</td>\n",
+       "      <td>18.7</td>\n",
+       "      <td>394.63</td>\n",
+       "      <td>2.94</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>0.06905</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2.18</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.458</td>\n",
+       "      <td>7.147</td>\n",
+       "      <td>54.2</td>\n",
+       "      <td>6.0622</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>222.0</td>\n",
+       "      <td>18.7</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>5.33</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
+       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
+       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
+       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
+       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
+       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
+       "\n",
+       "   PTRATIO       B  LSTAT  \n",
+       "0     15.3  396.90   4.98  \n",
+       "1     17.8  396.90   9.14  \n",
+       "2     17.8  392.83   4.03  \n",
+       "3     18.7  394.63   2.94  \n",
+       "4     18.7  396.90   5.33  "
+      ]
+     },
+     "execution_count": 209,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "boston1.columns = boston.feature_names\n",
+    "boston1.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 263,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "<div>\n",
+       "<style scoped>\n",
+       "    .dataframe tbody tr th:only-of-type {\n",
+       "        vertical-align: middle;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe tbody tr th {\n",
+       "        vertical-align: top;\n",
+       "    }\n",
+       "\n",
+       "    .dataframe thead th {\n",
+       "        text-align: right;\n",
+       "    }\n",
+       "</style>\n",
+       "<table border=\"1\" class=\"dataframe\">\n",
+       "  <thead>\n",
+       "    <tr style=\"text-align: right;\">\n",
+       "      <th></th>\n",
+       "      <th>CRIM</th>\n",
+       "      <th>ZN</th>\n",
+       "      <th>INDUS</th>\n",
+       "      <th>CHAS</th>\n",
+       "      <th>NOX</th>\n",
+       "      <th>RM</th>\n",
+       "      <th>AGE</th>\n",
+       "      <th>DIS</th>\n",
+       "      <th>RAD</th>\n",
+       "      <th>TAX</th>\n",
+       "      <th>PTRATIO</th>\n",
+       "      <th>B</th>\n",
+       "      <th>LSTAT</th>\n",
+       "      <th>PRICE</th>\n",
+       "    </tr>\n",
+       "  </thead>\n",
+       "  <tbody>\n",
+       "    <tr>\n",
+       "      <td>0</td>\n",
+       "      <td>0.00632</td>\n",
+       "      <td>18.0</td>\n",
+       "      <td>2.31</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.538</td>\n",
+       "      <td>6.575</td>\n",
+       "      <td>65.2</td>\n",
+       "      <td>4.0900</td>\n",
+       "      <td>1.0</td>\n",
+       "      <td>296.0</td>\n",
+       "      <td>15.3</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>4.98</td>\n",
+       "      <td>24.0</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>1</td>\n",
+       "      <td>0.02731</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>7.07</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.469</td>\n",
+       "      <td>6.421</td>\n",
+       "      <td>78.9</td>\n",
+       "      <td>4.9671</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>242.0</td>\n",
+       "      <td>17.8</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>9.14</td>\n",
+       "      <td>21.6</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>2</td>\n",
+       "      <td>0.02729</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>7.07</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.469</td>\n",
+       "      <td>7.185</td>\n",
+       "      <td>61.1</td>\n",
+       "      <td>4.9671</td>\n",
+       "      <td>2.0</td>\n",
+       "      <td>242.0</td>\n",
+       "      <td>17.8</td>\n",
+       "      <td>392.83</td>\n",
+       "      <td>4.03</td>\n",
+       "      <td>34.7</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>3</td>\n",
+       "      <td>0.03237</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2.18</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.458</td>\n",
+       "      <td>6.998</td>\n",
+       "      <td>45.8</td>\n",
+       "      <td>6.0622</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>222.0</td>\n",
+       "      <td>18.7</td>\n",
+       "      <td>394.63</td>\n",
+       "      <td>2.94</td>\n",
+       "      <td>33.4</td>\n",
+       "    </tr>\n",
+       "    <tr>\n",
+       "      <td>4</td>\n",
+       "      <td>0.06905</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>2.18</td>\n",
+       "      <td>0.0</td>\n",
+       "      <td>0.458</td>\n",
+       "      <td>7.147</td>\n",
+       "      <td>54.2</td>\n",
+       "      <td>6.0622</td>\n",
+       "      <td>3.0</td>\n",
+       "      <td>222.0</td>\n",
+       "      <td>18.7</td>\n",
+       "      <td>396.90</td>\n",
+       "      <td>5.33</td>\n",
+       "      <td>36.2</td>\n",
+       "    </tr>\n",
+       "  </tbody>\n",
+       "</table>\n",
+       "</div>"
+      ],
+      "text/plain": [
+       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
+       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
+       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
+       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
+       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
+       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
+       "\n",
+       "   PTRATIO       B  LSTAT  PRICE  \n",
+       "0     15.3  396.90   4.98   24.0  \n",
+       "1     17.8  396.90   9.14   21.6  \n",
+       "2     17.8  392.83   4.03   34.7  \n",
+       "3     18.7  394.63   2.94   33.4  \n",
+       "4     18.7  396.90   5.33   36.2  "
+      ]
+     },
+     "execution_count": 263,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "boston1['PRICE'] = boston.target\n",
+    "boston1.head()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 264,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# splitting data into X and y\n",
+    "\n",
+    "X = boston1.drop('PRICE', axis=1)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 265,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "((506, 13), (506,))"
+      ]
+     },
+     "execution_count": 265,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "y = boston1['PRICE']\n",
+    "X.shape, y.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 266,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
+    "# print(X_train.shape)\n",
+    "# print(X_test.shape)\n",
+    "# print(Y_train.shape)\n",
+    "# print(Y_test.shape)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 267,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "#  splitting the data to train and test\n",
+    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 268,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "((339, 13), (339,))"
+      ]
+     },
+     "execution_count": 268,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "X_train.shape, y_train.shape"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 269,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from sklearn.preprocessing import Normalizer\n",
+    "norm = Normalizer()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 270,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "X_train = norm.fit_transform(X_train)\n",
+    "# y_train = norm.fit_transform(y_train)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 271,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model = Sequential()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 272,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "model.add(Dense(13, input_dim=13, activation='relu', name='Dense'))\n",
+    "model.add(Dense(26, activation='sigmoid'))\n",
+    "model.add(Dense(15, activation='relu'))\n",
+    "model.add(Dense(7, activation='sigmoid'))\n",
+    "model.add(Dense(1, activation='sigmoid'))"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 273,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# model.add(Dense(1, input_dim=8, activation=\"sigmoid\")) #Relu is valid option. "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 274,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"sequential_17\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "Dense (Dense)                (None, 13)                182       \n",
+      "_________________________________________________________________\n",
+      "dense_77 (Dense)             (None, 26)                364       \n",
+      "_________________________________________________________________\n",
+      "dense_78 (Dense)             (None, 15)                405       \n",
+      "_________________________________________________________________\n",
+      "dense_79 (Dense)             (None, 7)                 112       \n",
+      "_________________________________________________________________\n",
+      "dense_80 (Dense)             (None, 1)                 8         \n",
+      "=================================================================\n",
+      "Total params: 1,071\n",
+      "Trainable params: 1,071\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['MSE'])\n",
+    "model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 275,
+   "metadata": {
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    }
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Train on 339 samples\n",
+      "Epoch 1/140\n",
+      "339/339 [==============================] - 0s 1ms/sample - loss: -2.7281 - MSE: 591.3746\n",
+      "Epoch 2/140\n",
+      "339/339 [==============================] - 0s 48us/sample - loss: -5.9266 - MSE: 589.7962\n",
+      "Epoch 3/140\n",
+      "339/339 [==============================] - 0s 53us/sample - loss: -8.9035 - MSE: 588.3561\n",
+      "Epoch 4/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -11.9302 - MSE: 586.9332\n",
+      "Epoch 5/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -15.0885 - MSE: 585.5038\n",
+      "Epoch 6/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -18.3186 - MSE: 584.1110\n",
+      "Epoch 7/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -21.6311 - MSE: 582.7644\n",
+      "Epoch 8/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -25.0031 - MSE: 581.4852\n",
+      "Epoch 9/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -28.3275 - MSE: 580.3185\n",
+      "Epoch 10/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -31.6752 - MSE: 579.2403\n",
+      "Epoch 11/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -34.9470 - MSE: 578.2795\n",
+      "Epoch 12/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -38.0870 - MSE: 577.4417\n",
+      "Epoch 13/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -41.2360 - MSE: 576.6818\n",
+      "Epoch 14/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -44.6095 - MSE: 575.9509\n",
+      "Epoch 15/140\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: -47.9431 - MSE: 575.3081\n",
+      "Epoch 16/140\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: -51.2565 - MSE: 574.7415\n",
+      "Epoch 17/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -54.4059 - MSE: 574.2639\n",
+      "Epoch 18/140\n",
+      "339/339 [==============================] - 0s 53us/sample - loss: -57.3755 - MSE: 573.8633\n",
+      "Epoch 19/140\n",
+      "339/339 [==============================] - 0s 52us/sample - loss: -60.0967 - MSE: 573.5350\n",
+      "Epoch 20/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -62.5790 - MSE: 573.2651\n",
+      "Epoch 21/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -64.9930 - MSE: 573.0274\n",
+      "Epoch 22/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -67.2026 - MSE: 572.8293\n",
+      "Epoch 23/140\n",
+      "339/339 [==============================] - ETA: 0s - loss: -75.0680 - MSE: 677.344 - 0s 57us/sample - loss: -69.2518 - MSE: 572.6614\n",
+      "Epoch 24/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -71.2233 - MSE: 572.5127\n",
+      "Epoch 25/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -73.1589 - MSE: 572.3782\n",
+      "Epoch 26/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -74.9954 - MSE: 572.2604\n",
+      "Epoch 27/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -76.7772 - MSE: 572.1547\n",
+      "Epoch 28/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -78.5438 - MSE: 572.0574\n",
+      "Epoch 29/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -80.2921 - MSE: 571.9681\n",
+      "Epoch 30/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -82.0150 - MSE: 571.8866\n",
+      "Epoch 31/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -83.7100 - MSE: 571.8121\n",
+      "Epoch 32/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -85.3899 - MSE: 571.7434\n",
+      "Epoch 33/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -87.0614 - MSE: 571.6800\n",
+      "Epoch 34/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -88.6999 - MSE: 571.6221\n",
+      "Epoch 35/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -90.3476 - MSE: 571.5680\n",
+      "Epoch 36/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -91.9758 - MSE: 571.5182\n",
+      "Epoch 37/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -93.5877 - MSE: 571.4722\n",
+      "Epoch 38/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -95.2210 - MSE: 571.4291\n",
+      "Epoch 39/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -96.8068 - MSE: 571.3899\n",
+      "Epoch 40/140\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: -98.4099 - MSE: 571.3531\n",
+      "Epoch 41/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -99.9999 - MSE: 571.3190\n",
+      "Epoch 42/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -101.5887 - MSE: 571.2874\n",
+      "Epoch 43/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -103.1638 - MSE: 571.2581\n",
+      "Epoch 44/140\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: -104.7434 - MSE: 571.2307\n",
+      "Epoch 45/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -106.3074 - MSE: 571.2055\n",
+      "Epoch 46/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -107.8797 - MSE: 571.1818\n",
+      "Epoch 47/140\n",
+      "339/339 [==============================] - 0s 53us/sample - loss: -109.4549 - MSE: 571.1597\n",
+      "Epoch 48/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -111.0114 - MSE: 571.1394\n",
+      "Epoch 49/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -112.5647 - MSE: 571.1204\n",
+      "Epoch 50/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -114.0927 - MSE: 571.1031\n",
+      "Epoch 51/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -115.6546 - MSE: 571.0865\n",
+      "Epoch 52/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -117.2029 - MSE: 571.0712\n",
+      "Epoch 53/140\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: -118.7347 - MSE: 571.0571\n",
+      "Epoch 54/140\n",
+      "339/339 [==============================] - 0s 61us/sample - loss: -120.2586 - MSE: 571.0439\n",
+      "Epoch 55/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -121.7960 - MSE: 571.0315\n",
+      "Epoch 56/140\n",
+      "339/339 [==============================] - 0s 63us/sample - loss: -123.3296 - MSE: 571.0200\n",
+      "Epoch 57/140\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: -124.8691 - MSE: 571.0093\n",
+      "Epoch 58/140\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: -126.3749 - MSE: 570.9994\n",
+      "Epoch 59/140\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: -127.9004 - MSE: 570.9901\n",
+      "Epoch 60/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -129.4013 - MSE: 570.9816\n",
+      "Epoch 61/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -130.9598 - MSE: 570.9733\n",
+      "Epoch 62/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -132.4612 - MSE: 570.9658\n",
+      "Epoch 63/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -133.9734 - MSE: 570.9589\n",
+      "Epoch 64/140\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: -135.4891 - MSE: 570.9523\n",
+      "Epoch 65/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -137.0060 - MSE: 570.9461\n",
+      "Epoch 66/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -138.5121 - MSE: 570.9405\n",
+      "Epoch 67/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -140.0198 - MSE: 570.9351\n",
+      "Epoch 68/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -141.5459 - MSE: 570.9301\n",
+      "Epoch 69/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -143.0392 - MSE: 570.9255\n",
+      "Epoch 70/140\n",
+      "339/339 [==============================] - 0s 61us/sample - loss: -144.5445 - MSE: 570.9212\n",
+      "Epoch 71/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -146.0624 - MSE: 570.9172\n",
+      "Epoch 72/140\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: -147.5485 - MSE: 570.9135\n",
+      "Epoch 73/140\n",
+      "339/339 [==============================] - 0s 61us/sample - loss: -149.0537 - MSE: 570.9099\n",
+      "Epoch 74/140\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: -150.5651 - MSE: 570.9066\n",
+      "Epoch 75/140\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: -152.0419 - MSE: 570.9037\n",
+      "Epoch 76/140\n",
+      "339/339 [==============================] - 0s 61us/sample - loss: -153.5513 - MSE: 570.9007\n",
+      "Epoch 77/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -155.0290 - MSE: 570.8981\n",
+      "Epoch 78/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -156.5430 - MSE: 570.8956\n",
+      "Epoch 79/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -158.0077 - MSE: 570.8934\n",
+      "Epoch 80/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -159.5199 - MSE: 570.8911\n",
+      "Epoch 81/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -161.0021 - MSE: 570.8891\n",
+      "Epoch 82/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -162.4962 - MSE: 570.8873\n",
+      "Epoch 83/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -164.0041 - MSE: 570.8854\n",
+      "Epoch 84/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -165.4791 - MSE: 570.8838\n",
+      "Epoch 85/140\n",
+      "339/339 [==============================] - 0s 53us/sample - loss: -166.9721 - MSE: 570.8823\n",
+      "Epoch 86/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -168.4612 - MSE: 570.8808\n",
+      "Epoch 87/140\n",
+      "339/339 [==============================] - 0s 53us/sample - loss: -169.9401 - MSE: 570.8795\n",
+      "Epoch 88/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -171.4334 - MSE: 570.8782\n",
+      "Epoch 89/140\n",
+      "339/339 [==============================] - 0s 61us/sample - loss: -172.9258 - MSE: 570.8771\n",
+      "Epoch 90/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -174.3991 - MSE: 570.8759\n",
+      "Epoch 91/140\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: -175.8906 - MSE: 570.8749\n",
+      "Epoch 92/140\n",
+      "339/339 [==============================] - 0s 70us/sample - loss: -177.3791 - MSE: 570.8740\n",
+      "Epoch 93/140\n",
+      "339/339 [==============================] - 0s 71us/sample - loss: -178.8604 - MSE: 570.8730\n",
+      "Epoch 94/140\n",
+      "339/339 [==============================] - 0s 67us/sample - loss: -180.3348 - MSE: 570.8723\n",
+      "Epoch 95/140\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: -181.8239 - MSE: 570.8714\n",
+      "Epoch 96/140\n",
+      "339/339 [==============================] - 0s 68us/sample - loss: -183.3196 - MSE: 570.8707\n",
+      "Epoch 97/140\n",
+      "339/339 [==============================] - 0s 70us/sample - loss: -184.7824 - MSE: 570.8700\n",
+      "Epoch 98/140\n",
+      "339/339 [==============================] - 0s 67us/sample - loss: -186.2725 - MSE: 570.8694\n",
+      "Epoch 99/140\n",
+      "339/339 [==============================] - 0s 70us/sample - loss: -187.7441 - MSE: 570.8688\n",
+      "Epoch 100/140\n",
+      "339/339 [==============================] - 0s 69us/sample - loss: -189.2353 - MSE: 570.8683\n",
+      "Epoch 101/140\n",
+      "339/339 [==============================] - 0s 67us/sample - loss: -190.7078 - MSE: 570.8677\n",
+      "Epoch 102/140\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -192.1809 - MSE: 570.8672\n",
+      "Epoch 103/140\n",
+      "339/339 [==============================] - 0s 71us/sample - loss: -193.6516 - MSE: 570.8668\n",
+      "Epoch 104/140\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -195.1351 - MSE: 570.8664\n",
+      "Epoch 105/140\n",
+      "339/339 [==============================] - 0s 69us/sample - loss: -196.6020 - MSE: 570.8660\n",
+      "Epoch 106/140\n",
+      "339/339 [==============================] - 0s 72us/sample - loss: -198.0699 - MSE: 570.8655\n",
+      "Epoch 107/140\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -199.5465 - MSE: 570.8652\n",
+      "Epoch 108/140\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: -201.0267 - MSE: 570.8649\n",
+      "Epoch 109/140\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: -202.4835 - MSE: 570.8647\n",
+      "Epoch 110/140\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -203.9669 - MSE: 570.8643\n",
+      "Epoch 111/140\n",
+      "339/339 [==============================] - 0s 71us/sample - loss: -205.4481 - MSE: 570.8640\n",
+      "Epoch 112/140\n",
+      "339/339 [==============================] - 0s 72us/sample - loss: -206.9167 - MSE: 570.8639\n",
+      "Epoch 113/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -208.3659 - MSE: 570.8636\n",
+      "Epoch 114/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -209.8579 - MSE: 570.8634\n",
+      "Epoch 115/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -211.3170 - MSE: 570.8632\n",
+      "Epoch 116/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -212.7854 - MSE: 570.8630\n",
+      "Epoch 117/140\n",
+      "339/339 [==============================] - 0s 63us/sample - loss: -214.2616 - MSE: 570.8629\n",
+      "Epoch 118/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -215.7296 - MSE: 570.8627\n",
+      "Epoch 119/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -217.2039 - MSE: 570.8625\n",
+      "Epoch 120/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -218.6727 - MSE: 570.8624\n",
+      "Epoch 121/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -220.1259 - MSE: 570.8623\n",
+      "Epoch 122/140\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: -221.5996 - MSE: 570.8621\n",
+      "Epoch 123/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -223.0607 - MSE: 570.8620\n",
+      "Epoch 124/140\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -224.5289 - MSE: 570.8619\n",
+      "Epoch 125/140\n",
+      "339/339 [==============================] - 0s 71us/sample - loss: -225.9877 - MSE: 570.8618\n",
+      "Epoch 126/140\n",
+      "339/339 [==============================] - 0s 69us/sample - loss: -227.4524 - MSE: 570.8617\n",
+      "Epoch 127/140\n",
+      "339/339 [==============================] - 0s 70us/sample - loss: -228.9255 - MSE: 570.8615\n",
+      "Epoch 128/140\n",
+      "339/339 [==============================] - 0s 70us/sample - loss: -230.3992 - MSE: 570.8615\n",
+      "Epoch 129/140\n",
+      "339/339 [==============================] - 0s 68us/sample - loss: -231.8543 - MSE: 570.8613\n",
+      "Epoch 130/140\n",
+      "339/339 [==============================] - 0s 66us/sample - loss: -233.3251 - MSE: 570.8613\n",
+      "Epoch 131/140\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: -234.7919 - MSE: 570.8613\n",
+      "Epoch 132/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -236.2572 - MSE: 570.8612\n",
+      "Epoch 133/140\n",
+      "339/339 [==============================] - 0s 54us/sample - loss: -237.7068 - MSE: 570.8611\n",
+      "Epoch 134/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -239.1818 - MSE: 570.8611\n",
+      "Epoch 135/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -240.6499 - MSE: 570.8610\n",
+      "Epoch 136/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -242.0884 - MSE: 570.8610\n",
+      "Epoch 137/140\n",
+      "339/339 [==============================] - 0s 55us/sample - loss: -243.5744 - MSE: 570.8610\n",
+      "Epoch 138/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -245.0325 - MSE: 570.8609\n",
+      "Epoch 139/140\n",
+      "339/339 [==============================] - 0s 56us/sample - loss: -246.4912 - MSE: 570.8608\n",
+      "Epoch 140/140\n",
+      "339/339 [==============================] - 0s 57us/sample - loss: -247.9508 - MSE: 570.8608\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x143ac6f10>"
+      ]
+     },
+     "execution_count": 275,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model.fit(X_train, y_train, epochs=140)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 276,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "167/167 [==============================] - 0s 530us/sample - loss: -233.7545 - MSE: 501.8419\n",
+      "MSE: 501.8418884277344\n"
+     ]
+    }
+   ],
+   "source": [
+    "scores = model.evaluate(X_test, y_test)\n",
+    "print(f\"{model.metrics_names[1]}: {scores[1]}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 277,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"sequential_18\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "Dense (Dense)                (None, 13)                182       \n",
+      "_________________________________________________________________\n",
+      "dense_81 (Dense)             (None, 26)                364       \n",
+      "_________________________________________________________________\n",
+      "dense_82 (Dense)             (None, 15)                405       \n",
+      "_________________________________________________________________\n",
+      "dense_83 (Dense)             (None, 7)                 112       \n",
+      "_________________________________________________________________\n",
+      "dense_84 (Dense)             (None, 1)                 8         \n",
+      "_________________________________________________________________\n",
+      "dense_85 (Dense)             (None, 7)                 14        \n",
+      "_________________________________________________________________\n",
+      "dense_86 (Dense)             (None, 5)                 40        \n",
+      "_________________________________________________________________\n",
+      "dense_87 (Dense)             (None, 1)                 6         \n",
+      "=================================================================\n",
+      "Total params: 1,131\n",
+      "Trainable params: 1,131\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "model1 =Sequential()\n",
+    "model1.add(Dense(13, input_dim=13, activation='relu', name='Dense'))\n",
+    "model1.add(Dense(26, activation='relu'))\n",
+    "model1.add(Dense(15, activation='relu'))\n",
+    "model1.add(Dense(7, activation='relu'))\n",
+    "model1.add(Dense(1, activation='relu'))\n",
+    "model1.add(Dense(7, activation='relu'))\n",
+    "model1.add(Dense(5, activation='relu'))\n",
+    "model1.add(Dense(1, activation='relu'))\n",
+    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['MSE'])\n",
+    "model1.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 278,
+   "metadata": {
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    }
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Train on 339 samples\n",
+      "Epoch 1/80\n",
+      "339/339 [==============================] - 0s 1ms/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 2/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 3/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 4/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 5/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 6/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 7/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 8/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 9/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 10/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 11/80\n",
+      "339/339 [==============================] - 0s 72us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 12/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 13/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 14/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 15/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 16/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 17/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 18/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 19/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 20/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 21/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 22/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 23/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 24/80\n",
+      "339/339 [==============================] - 0s 70us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 25/80\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 26/80\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 27/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 28/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 29/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 30/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 31/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 32/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 33/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 34/80\n",
+      "339/339 [==============================] - 0s 85us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 35/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 36/80\n",
+      "339/339 [==============================] - 0s 69us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 37/80\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 38/80\n",
+      "339/339 [==============================] - 0s 65us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 39/80\n",
+      "339/339 [==============================] - 0s 64us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 40/80\n",
+      "339/339 [==============================] - 0s 67us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 41/80\n",
+      "339/339 [==============================] - 0s 63us/sample - loss: 354.3233 - MSE: 615.8019\n",
+      "Epoch 42/80\n",
+      "339/339 [==============================] - 0s 60us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 43/80\n",
+      "339/339 [==============================] - 0s 63us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 44/80\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 45/80\n",
+      "339/339 [==============================] - 0s 58us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 46/80\n",
+      "339/339 [==============================] - 0s 63us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 47/80\n",
+      "339/339 [==============================] - 0s 62us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 48/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 49/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 50/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 51/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 52/80\n",
+      "339/339 [==============================] - 0s 90us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 53/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 54/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 55/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 56/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 57/80\n",
+      "339/339 [==============================] - 0s 71us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 58/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 59/80\n",
+      "339/339 [==============================] - 0s 66us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 60/80\n",
+      "339/339 [==============================] - 0s 68us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 61/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 62/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 63/80\n",
+      "339/339 [==============================] - 0s 81us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 64/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 65/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 66/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 67/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 68/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 69/80\n",
+      "339/339 [==============================] - 0s 68us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 70/80\n",
+      "339/339 [==============================] - 0s 59us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 71/80\n",
+      "339/339 [==============================] - 0s 72us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 72/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 73/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 74/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 75/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 76/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 77/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8018\n",
+      "Epoch 78/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 79/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: 354.3234 - MSE: 615.8019\n",
+      "Epoch 80/80\n",
+      "339/339 [==============================] - 0s 72us/sample - loss: 354.3234 - MSE: 615.8019\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x143cc6290>"
+      ]
+     },
+     "execution_count": 278,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model1.fit(X_train, y_train, epochs=80)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 279,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "167/167 [==============================] - 0s 714us/sample - loss: 333.8532 - MSE: 544.1288\n",
+      "MSE: 544.1287841796875\n"
+     ]
+    }
+   ],
+   "source": [
+    "scores = model1.evaluate(X_test, y_test)\n",
+    "print(f\"{model1.metrics_names[1]}: {scores[1]}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 280,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"sequential_19\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "Dense (Dense)                (None, 130)               1820      \n",
+      "_________________________________________________________________\n",
+      "dense_88 (Dense)             (None, 260)               34060     \n",
+      "_________________________________________________________________\n",
+      "dense_89 (Dense)             (None, 150)               39150     \n",
+      "_________________________________________________________________\n",
+      "dense_90 (Dense)             (None, 75)                11325     \n",
+      "_________________________________________________________________\n",
+      "dense_91 (Dense)             (None, 19)                1444      \n",
+      "_________________________________________________________________\n",
+      "dense_92 (Dense)             (None, 7)                 140       \n",
+      "_________________________________________________________________\n",
+      "dense_93 (Dense)             (None, 5)                 40        \n",
+      "_________________________________________________________________\n",
+      "dense_94 (Dense)             (None, 1)                 6         \n",
+      "=================================================================\n",
+      "Total params: 87,985\n",
+      "Trainable params: 87,985\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "model2 =Sequential()\n",
+    "model2.add(Dense(130, input_dim=13, activation='relu', name='Dense'))\n",
+    "model2.add(Dense(260, activation='relu'))\n",
+    "model2.add(Dense(150, activation='relu'))\n",
+    "model2.add(Dense(75, activation='relu'))\n",
+    "model2.add(Dense(19, activation='relu'))\n",
+    "model2.add(Dense(7, activation='relu'))\n",
+    "model2.add(Dense(5, activation='relu'))\n",
+    "model2.add(Dense(1, activation='relu'))\n",
+    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['MSE'])\n",
+    "model2.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 284,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "CPU times: user 3 µs, sys: 3 µs, total: 6 µs\n",
+      "Wall time: 11.9 µs\n",
+      "Train on 339 samples\n",
+      "Epoch 1/80\n",
+      "339/339 [==============================] - 0s 104us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 2/80\n",
+      "339/339 [==============================] - 0s 85us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 3/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 4/80\n",
+      "339/339 [==============================] - 0s 81us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 5/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 6/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 7/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6403\n",
+      "Epoch 8/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 9/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 10/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 11/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 12/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 13/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 14/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 15/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 16/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 17/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 18/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 19/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 20/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 21/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 22/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 23/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 24/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 25/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 26/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 27/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 28/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 29/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 30/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 31/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 32/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 33/80\n",
+      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 34/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 35/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 36/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 37/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 38/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 39/80\n",
+      "339/339 [==============================] - 0s 87us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 40/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 41/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 42/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 43/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 44/80\n",
+      "339/339 [==============================] - 0s 87us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 45/80\n",
+      "339/339 [==============================] - 0s 92us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 46/80\n",
+      "339/339 [==============================] - 0s 93us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 47/80\n",
+      "339/339 [==============================] - 0s 115us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 48/80\n",
+      "339/339 [==============================] - 0s 121us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 49/80\n",
+      "339/339 [==============================] - 0s 111us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 50/80\n",
+      "339/339 [==============================] - 0s 117us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 51/80\n",
+      "339/339 [==============================] - 0s 118us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 52/80\n",
+      "339/339 [==============================] - 0s 117us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 53/80\n",
+      "339/339 [==============================] - 0s 111us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 54/80\n",
+      "339/339 [==============================] - 0s 91us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 55/80\n",
+      "339/339 [==============================] - 0s 94us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 56/80\n",
+      "339/339 [==============================] - 0s 108us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 57/80\n",
+      "339/339 [==============================] - 0s 94us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 58/80\n",
+      "339/339 [==============================] - 0s 92us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 59/80\n",
+      "339/339 [==============================] - 0s 91us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 60/80\n",
+      "339/339 [==============================] - 0s 85us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 61/80\n",
+      "339/339 [==============================] - 0s 84us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 62/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6403\n",
+      "Epoch 63/80\n",
+      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 64/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 65/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 66/80\n",
+      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 67/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 68/80\n",
+      "339/339 [==============================] - 0s 73us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 69/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 70/80\n",
+      "339/339 [==============================] - 0s 72us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 71/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6403\n",
+      "Epoch 72/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 73/80\n",
+      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 74/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 75/80\n",
+      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6403\n",
+      "Epoch 76/80\n",
+      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 77/80\n",
+      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 78/80\n",
+      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 79/80\n",
+      "339/339 [==============================] - 0s 86us/sample - loss: -335.0379 - MSE: 421.6404\n",
+      "Epoch 80/80\n",
+      "339/339 [==============================] - 0s 92us/sample - loss: -335.0379 - MSE: 421.6404\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x144223710>"
+      ]
+     },
+     "execution_count": 284,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "%time\n",
+    "model2.fit(X_train, y_train, epochs=80)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 285,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "167/167 [==============================] - 0s 63us/sample - loss: -314.8009 - MSE: 4444057.5000\n",
+      "MSE: 4444057.5\n"
+     ]
+    }
+   ],
+   "source": [
+    "scores = model2.evaluate(X_test, y_test)\n",
+    "print(f\"{model2.metrics_names[1]}: {scores[1]}\")"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 286,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "fit = model.fit(X_train, y_train, epochs=300, validation_split=.1, verbose=False)\n",
+    "history = model.fit(X_train, y_train, epochs=100, verbose=False)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 287,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "167/167 [==============================] - 0s 53us/sample - loss: -741.5163 - MSE: 501.8414\n"
+     ]
+    }
+   ],
+   "source": [
+    "scores = model.evaluate(X_test, y_test)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 288,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "MSE: 501.84136962890625\n"
+     ]
+    }
+   ],
+   "source": [
+    "print(f'{model.metrics_names[1]}: {scores[1]}')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 289,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "image/png": "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\n",
+      "text/plain": [
+       "<Figure size 432x288 with 1 Axes>"
+      ]
+     },
+     "metadata": {
+      "needs_background": "light"
+     },
+     "output_type": "display_data"
+    }
+   ],
+   "source": [
+    "import matplotlib.pyplot as plt\n",
+    "\n",
+    "# plt.plot(fit.history['mse'])\n",
+    "# plt.plot(fit.history['val_mse'])\n",
+    "plt.plot(fit.history['loss'])\n",
+    "# plt.plot(fit.history['val_loss'])\n",
+    "plt.title(\"Loss\")\n",
+    "plt.xlabel(\"epoch\")\n",
+    "plt.ylabel('loss')\n",
+    "plt.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# so cranking the density way high or the poisson optimizer did do well. ! after little testing \n",
+    "# it was the optimzer poisson that sucked but cranking the numbers helped"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 297,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# I will run a linear regression model to compare\n",
+    "from sklearn.pipeline import make_pipeline\n",
+    "from sklearn.linear_model import LinearRegression\n",
+    "from sklearn.preprocessing import OneHotEncoder\n",
+    "from sklearn.metrics import mean_squared_error"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 311,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "140199499.60049394"
+      ]
+     },
+     "execution_count": 311,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "\n",
+    "# simpler linear regression model\n",
+    "# from sklearn.linear_model import LinearRegression\n",
+    "# from sklearn.metrics import mean_squared_error\n",
+    "\n",
+    "# linreg = LinearRegression()\n",
+    "# linreg.fit(X_train, y_train)\n",
+    "# linreg_pred = linreg.predict(X_test)\n",
+    "# mean_squared_error(linreg_pred, y_test)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 308,
+   "metadata": {},
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
+      ]
+     },
+     "execution_count": 308,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model3 = LinearRegression()\n",
+    "    \n",
+    "# Fit on train, score on val\n",
+    "\n",
+    "model3.fit(X_train, y_train)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 309,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "y_pred = model3.predict(X_test)\n",
+    "mse = mean_squared_error(y_test, y_pred)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 310,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Test Error: 140199499.60 \n"
+     ]
+    }
+   ],
+   "source": [
+    "print(f'Test Error: {mse:.2f} ')"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 301,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "import category_encoders as ce"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 307,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
+    "X_train = encoder.fit_transform(X_train)"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -70,9 +1879,175 @@
    },
    "outputs": [],
    "source": [
-    "##### Your Code Here #####"
+    "#### Your Code Here ####\n",
+    "# image classification \n",
+    "# bonus lecture 3 on how to load\n",
+    "# hotencode"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 320,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "### Let's do it!\n",
+    "\n",
+    "from tensorflow import keras\n",
+    "from tensorflow.keras.datasets import fashion_mnist\n",
+    "from tensorflow.keras.models import Sequential\n",
+    "from tensorflow.keras.layers import Dense, Dropout\n",
+    "\n",
+    "# Stretch - use dropout \n",
+    "import numpy as np"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 321,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# import matplotlib.pyplot as plt\n",
+    "# plt.imshow(x_train[0])\n",
+    "# plt.show()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 328,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# batch_size = 64\n",
+    "num_classes = 10\n",
+    "# epochs = 20"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 329,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 330,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# reshape data\n",
+    "X_train = X_train.reshape(60000, 784)\n",
+    "X_test = X_test.reshape(10000, 784)\n",
+    "\n",
+    "# X variable type\n",
+    "\n",
+    "X_train = X_train.astype('float32')\n",
+    "X_test = X_test.astype('float32')\n",
+    "\n",
+    "# correct encoding for y\n",
+    "\n",
+    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
+    "y_test = keras.utils.to_categorical(y_train, num_classes)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 331,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"sequential_21\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "dense_100 (Dense)            (None, 16)                12560     \n",
+      "_________________________________________________________________\n",
+      "dense_101 (Dense)            (None, 16)                272       \n",
+      "_________________________________________________________________\n",
+      "dense_102 (Dense)            (None, 16)                272       \n",
+      "_________________________________________________________________\n",
+      "dense_103 (Dense)            (None, 16)                272       \n",
+      "_________________________________________________________________\n",
+      "dense_104 (Dense)            (None, 1)                 17        \n",
+      "=================================================================\n",
+      "Total params: 13,393\n",
+      "Trainable params: 13,393\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "fashion_mnist_model = Sequential()\n",
+    "\n",
+    "# input => Hidden\n",
+    "fashion_mnist_model.add(Dense(16, input_dim = 784, activation = 'relu'))\n",
+    "# hidden\n",
+    "fashion_mnist_model.add(Dense(16, activation = 'relu'))\n",
+    "# hidden\n",
+    "fashion_mnist_model.add(Dense(16, activation = 'relu'))\n",
+    "# hidden\n",
+    "fashion_mnist_model.add(Dense(16, activation = 'relu'))\n",
+    "# output\n",
+    "fashion_mnist_model.add(Dense(1, activation = 'softmax'))\n",
+    "# compile\n",
+    "fashion_mnist_model.compile(loss='categorical_crossentropy',\n",
+    "                           optimizer= 'adam',\n",
+    "                           metrics= ['accuracy'])\n",
+    "# summary\n",
+    "fashion_mnist_model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 327,
+   "metadata": {},
+   "outputs": [
+    {
+     "ename": "ValueError",
+     "evalue": "You are passing a target array of shape (60000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-327-9fb9a9bdaedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfashion_mnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfashion_mnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
+      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    782\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    783\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (60000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
+     ]
+    }
+   ],
+   "source": [
+    "hist = fashion_mnist_model.fit(X_train, y_train, batch_size=100, epochs=20, verbose=False)\n",
+    "scores = fashion_mnist_model.evaluate(X_test, y_test)"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
+  },
   {
    "cell_type": "markdown",
    "metadata": {
@@ -110,9 +2085,9 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.3"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
- "nbformat_minor": 2
+ "nbformat_minor": 4
 }
diff --git a/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb b/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
index abc92b5..e854375 100644
--- a/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
+++ b/module3-Intro-to-Keras/LS_DS_423_Keras_Lecture.ipynb
@@ -75,7 +75,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 1,
+   "execution_count": 20,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -93,7 +93,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 2,
+   "execution_count": 21,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -110,24 +110,24 @@
      "text": [
       "Train on 4 samples\n",
       "Epoch 1/5\n",
-      "4/4 [==============================] - 1s 184ms/sample - loss: 0.7531 - accuracy: 0.2500\n",
+      "4/4 [==============================] - 0s 82ms/sample - loss: 0.7485 - accuracy: 0.2500\n",
       "Epoch 2/5\n",
-      "4/4 [==============================] - 0s 1ms/sample - loss: 0.7527 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 542us/sample - loss: 0.7482 - accuracy: 0.5000\n",
       "Epoch 3/5\n",
-      "4/4 [==============================] - 0s 833us/sample - loss: 0.7524 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 518us/sample - loss: 0.7479 - accuracy: 0.5000\n",
       "Epoch 4/5\n",
-      "4/4 [==============================] - 0s 760us/sample - loss: 0.7520 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 728us/sample - loss: 0.7476 - accuracy: 0.5000\n",
       "Epoch 5/5\n",
-      "4/4 [==============================] - 0s 805us/sample - loss: 0.7517 - accuracy: 0.5000\n"
+      "4/4 [==============================] - 0s 910us/sample - loss: 0.7473 - accuracy: 0.5000\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb4cc4cdeb8>"
+       "<tensorflow.python.keras.callbacks.History at 0x144cc94d0>"
       ]
      },
-     "execution_count": 2,
+     "execution_count": 21,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -145,7 +145,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 3,
+   "execution_count": 22,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -160,7 +160,7 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "4/1 [========================================================================================================================] - 0s 21ms/sample - loss: 0.7514 - accuracy: 0.5000\n",
+      "4/4 [==============================] - 0s 20ms/sample - loss: 0.7470 - accuracy: 0.5000\n",
       "accuracy: 50.0\n"
      ]
     }
@@ -205,7 +205,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 4,
+   "execution_count": 23,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -240,7 +240,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 24,
    "metadata": {},
    "outputs": [
     {
@@ -349,7 +349,7 @@
        "4  0  137  40  35  168  43.1  2.288  33  1"
       ]
      },
-     "execution_count": 5,
+     "execution_count": 24,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -360,7 +360,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
+   "execution_count": 25,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -394,7 +394,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 7,
+   "execution_count": 26,
    "metadata": {},
    "outputs": [
     {
@@ -455,7 +455,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 27,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -483,7 +483,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": 28,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -511,7 +511,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 17,
+   "execution_count": 29,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -545,7 +545,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 18,
+   "execution_count": 30,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -571,7 +571,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 31,
    "metadata": {},
    "outputs": [
     {
@@ -580,314 +580,314 @@
      "text": [
       "Train on 768 samples\n",
       "Epoch 1/150\n",
-      "768/768 [==============================] - 0s 441us/sample - loss: 0.6796 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 335us/sample - loss: 50.8516 - accuracy: 0.6523\n",
       "Epoch 2/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6693 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 34us/sample - loss: 47.5816 - accuracy: 0.6497\n",
       "Epoch 3/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6572 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 37us/sample - loss: 44.3232 - accuracy: 0.6510\n",
       "Epoch 4/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6538 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 41.0424 - accuracy: 0.6510\n",
       "Epoch 5/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6517 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 36us/sample - loss: 37.7278 - accuracy: 0.6510\n",
       "Epoch 6/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6509 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 38us/sample - loss: 34.4411 - accuracy: 0.6497\n",
       "Epoch 7/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6493 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 31.1750 - accuracy: 0.6497\n",
       "Epoch 8/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6486 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 27.9112 - accuracy: 0.6471\n",
       "Epoch 9/150\n",
-      "768/768 [==============================] - 0s 58us/sample - loss: 0.6482 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 41us/sample - loss: 24.6587 - accuracy: 0.6458\n",
       "Epoch 10/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6473 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 21.5525 - accuracy: 0.6419\n",
       "Epoch 11/150\n",
-      "768/768 [==============================] - 0s 57us/sample - loss: 0.6468 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 18.6666 - accuracy: 0.6237\n",
       "Epoch 12/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6469 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 16.1134 - accuracy: 0.5990\n",
       "Epoch 13/150\n",
-      "768/768 [==============================] - 0s 58us/sample - loss: 0.6461 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 14.0249 - accuracy: 0.5859\n",
       "Epoch 14/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6472 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 12.3479 - accuracy: 0.5664\n",
       "Epoch 15/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6459 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 11.1159 - accuracy: 0.5156\n",
       "Epoch 16/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6457 - accuracy: 0.6576\n",
+      "768/768 [==============================] - 0s 41us/sample - loss: 10.2114 - accuracy: 0.4896\n",
       "Epoch 17/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6455 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 39us/sample - loss: 9.5816 - accuracy: 0.4792\n",
       "Epoch 18/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6455 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 9.0973 - accuracy: 0.4661\n",
       "Epoch 19/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6453 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 8.6829 - accuracy: 0.4544\n",
       "Epoch 20/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6453 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 41us/sample - loss: 8.3233 - accuracy: 0.4531\n",
       "Epoch 21/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6452 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 7.9827 - accuracy: 0.4505\n",
       "Epoch 22/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6452 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 7.6592 - accuracy: 0.4544\n",
       "Epoch 23/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6453 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 7.3502 - accuracy: 0.4492\n",
       "Epoch 24/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6450 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 7.0519 - accuracy: 0.4570\n",
       "Epoch 25/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6450 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 41us/sample - loss: 6.7700 - accuracy: 0.4583\n",
       "Epoch 26/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6450 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 6.4903 - accuracy: 0.4661\n",
       "Epoch 27/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6449 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 6.2266 - accuracy: 0.4714\n",
       "Epoch 28/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6448 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 42us/sample - loss: 5.9641 - accuracy: 0.4701\n",
       "Epoch 29/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6448 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 5.7231 - accuracy: 0.4792\n",
       "Epoch 30/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6447 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 5.4958 - accuracy: 0.4857\n",
       "Epoch 31/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6451 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 5.2709 - accuracy: 0.4857\n",
       "Epoch 32/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6446 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 5.0783 - accuracy: 0.4935\n",
       "Epoch 33/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6446 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 62us/sample - loss: 4.8740 - accuracy: 0.4935\n",
       "Epoch 34/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6445 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 4.6938 - accuracy: 0.5052\n",
       "Epoch 35/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6447 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 4.5427 - accuracy: 0.5039\n",
       "Epoch 36/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6446 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 4.3769 - accuracy: 0.5039\n",
       "Epoch 37/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6446 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 4.2126 - accuracy: 0.5169\n",
       "Epoch 38/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6447 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 4.0680 - accuracy: 0.5169\n",
       "Epoch 39/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6442 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 3.9239 - accuracy: 0.5195\n",
       "Epoch 40/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6445 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 3.7823 - accuracy: 0.5091\n",
       "Epoch 41/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6443 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 3.6403 - accuracy: 0.5143\n",
       "Epoch 42/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6448 - accuracy: 0.6497\n",
+      "768/768 [==============================] - 0s 41us/sample - loss: 3.5040 - accuracy: 0.5156\n",
       "Epoch 43/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6440 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 3.3779 - accuracy: 0.5091\n",
       "Epoch 44/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6430 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 3.2412 - accuracy: 0.5208\n",
       "Epoch 45/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6432 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 3.1131 - accuracy: 0.5221\n",
       "Epoch 46/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6420 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 2.9952 - accuracy: 0.5247\n",
       "Epoch 47/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6407 - accuracy: 0.6471\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 2.8719 - accuracy: 0.5260\n",
       "Epoch 48/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6421 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 2.7658 - accuracy: 0.5299\n",
       "Epoch 49/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6390 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 66us/sample - loss: 2.6538 - accuracy: 0.5391\n",
       "Epoch 50/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6410 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 70us/sample - loss: 2.5506 - accuracy: 0.5391\n",
       "Epoch 51/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6394 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 70us/sample - loss: 2.4476 - accuracy: 0.5378\n",
       "Epoch 52/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6390 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 63us/sample - loss: 2.3549 - accuracy: 0.5443\n",
       "Epoch 53/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6369 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 2.2639 - accuracy: 0.5469\n",
       "Epoch 54/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6365 - accuracy: 0.6576\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 2.1770 - accuracy: 0.5547\n",
       "Epoch 55/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6361 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 70us/sample - loss: 2.1025 - accuracy: 0.5638\n",
       "Epoch 56/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6357 - accuracy: 0.6589\n",
+      "768/768 [==============================] - 0s 60us/sample - loss: 2.0218 - accuracy: 0.5651\n",
       "Epoch 57/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6354 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 1.9456 - accuracy: 0.5690\n",
       "Epoch 58/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6350 - accuracy: 0.6523\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 1.8808 - accuracy: 0.5716\n",
       "Epoch 59/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.6345 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 1.8233 - accuracy: 0.5625\n",
       "Epoch 60/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6366 - accuracy: 0.6497\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 1.7535 - accuracy: 0.5742\n",
       "Epoch 61/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6357 - accuracy: 0.6562\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 1.6898 - accuracy: 0.5755\n",
       "Epoch 62/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6338 - accuracy: 0.6549\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 1.6278 - accuracy: 0.5807\n",
       "Epoch 63/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6338 - accuracy: 0.6589\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 1.5758 - accuracy: 0.5807\n",
       "Epoch 64/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.6328 - accuracy: 0.6536\n",
+      "768/768 [==============================] - 0s 64us/sample - loss: 1.5226 - accuracy: 0.5781\n",
       "Epoch 65/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.6326 - accuracy: 0.6589\n",
+      "768/768 [==============================] - 0s 85us/sample - loss: 1.4720 - accuracy: 0.5872\n",
       "Epoch 66/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6320 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 1.4291 - accuracy: 0.5872\n",
       "Epoch 67/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6315 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 1.3756 - accuracy: 0.5859\n",
       "Epoch 68/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6314 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 1.3391 - accuracy: 0.5872\n",
       "Epoch 69/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6306 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 1.2960 - accuracy: 0.5924\n",
       "Epoch 70/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.6303 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 1.2579 - accuracy: 0.5938\n",
       "Epoch 71/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6299 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 1.2188 - accuracy: 0.5924\n",
       "Epoch 72/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6296 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 72us/sample - loss: 1.1875 - accuracy: 0.6081\n",
       "Epoch 73/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6299 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 1.1525 - accuracy: 0.6107\n",
       "Epoch 74/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6290 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 66us/sample - loss: 1.1246 - accuracy: 0.6094\n",
       "Epoch 75/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6291 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 80us/sample - loss: 1.0927 - accuracy: 0.6107\n",
       "Epoch 76/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6286 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 135us/sample - loss: 1.0659 - accuracy: 0.6159\n",
       "Epoch 77/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6283 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 131us/sample - loss: 1.0408 - accuracy: 0.6120\n",
       "Epoch 78/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.6275 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 63us/sample - loss: 1.0106 - accuracy: 0.6224\n",
       "Epoch 79/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6249 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 66us/sample - loss: 0.9965 - accuracy: 0.6120\n",
       "Epoch 80/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6207 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 69us/sample - loss: 0.9918 - accuracy: 0.6224\n",
       "Epoch 81/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.6146 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 76us/sample - loss: 0.9433 - accuracy: 0.6263\n",
       "Epoch 82/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6101 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 76us/sample - loss: 0.9394 - accuracy: 0.6185\n",
       "Epoch 83/150\n",
-      "768/768 [==============================] - 0s 69us/sample - loss: 0.6076 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 76us/sample - loss: 0.9161 - accuracy: 0.6211\n",
       "Epoch 84/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6070 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 92us/sample - loss: 0.8901 - accuracy: 0.6250\n",
       "Epoch 85/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6037 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 76us/sample - loss: 0.8720 - accuracy: 0.6341\n",
       "Epoch 86/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6058 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 67us/sample - loss: 0.8539 - accuracy: 0.6341\n",
       "Epoch 87/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6026 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 85us/sample - loss: 0.8453 - accuracy: 0.6315\n",
       "Epoch 88/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.6050 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 102us/sample - loss: 0.8280 - accuracy: 0.6341\n",
       "Epoch 89/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6014 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 117us/sample - loss: 0.8111 - accuracy: 0.6328\n",
       "Epoch 90/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6011 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 100us/sample - loss: 0.7992 - accuracy: 0.6393\n",
       "Epoch 91/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5990 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 88us/sample - loss: 0.7833 - accuracy: 0.6315\n",
       "Epoch 92/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.6027 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 114us/sample - loss: 0.7706 - accuracy: 0.6406\n",
       "Epoch 93/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5987 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 106us/sample - loss: 0.7567 - accuracy: 0.6432\n",
       "Epoch 94/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.6013 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 71us/sample - loss: 0.7541 - accuracy: 0.6458\n",
       "Epoch 95/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5984 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 78us/sample - loss: 0.7448 - accuracy: 0.6302\n",
       "Epoch 96/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5979 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 88us/sample - loss: 0.7352 - accuracy: 0.6510\n",
       "Epoch 97/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5977 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 72us/sample - loss: 0.7138 - accuracy: 0.6576\n",
       "Epoch 98/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5968 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 60us/sample - loss: 0.7097 - accuracy: 0.6458\n",
       "Epoch 99/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5957 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 85us/sample - loss: 0.7010 - accuracy: 0.6471\n",
       "Epoch 100/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5955 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 82us/sample - loss: 0.6939 - accuracy: 0.6484\n",
       "Epoch 101/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5963 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 101us/sample - loss: 0.6978 - accuracy: 0.6419\n",
       "Epoch 102/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5923 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 174us/sample - loss: 0.6801 - accuracy: 0.6589\n",
       "Epoch 103/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5957 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 201us/sample - loss: 0.6694 - accuracy: 0.6654\n",
       "Epoch 104/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5941 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 215us/sample - loss: 0.6765 - accuracy: 0.6641\n",
       "Epoch 105/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5925 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 134us/sample - loss: 0.6577 - accuracy: 0.6562\n",
       "Epoch 106/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5923 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 109us/sample - loss: 0.6546 - accuracy: 0.6536\n",
       "Epoch 107/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5916 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 64us/sample - loss: 0.6501 - accuracy: 0.6576\n",
       "Epoch 108/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5925 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 74us/sample - loss: 0.6463 - accuracy: 0.6589\n",
       "Epoch 109/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5914 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 68us/sample - loss: 0.6415 - accuracy: 0.6602\n",
       "Epoch 110/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5904 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6377 - accuracy: 0.6732\n",
       "Epoch 111/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5901 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 65us/sample - loss: 0.6312 - accuracy: 0.6784\n",
       "Epoch 112/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5949 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6351 - accuracy: 0.6693\n",
       "Epoch 113/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5920 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 137us/sample - loss: 0.6306 - accuracy: 0.6641\n",
       "Epoch 114/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5904 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 113us/sample - loss: 0.6230 - accuracy: 0.6888\n",
       "Epoch 115/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5902 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 109us/sample - loss: 0.6220 - accuracy: 0.6784\n",
       "Epoch 116/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5893 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 93us/sample - loss: 0.6290 - accuracy: 0.6680\n",
       "Epoch 117/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5892 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 98us/sample - loss: 0.6157 - accuracy: 0.6758\n",
       "Epoch 118/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5889 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 96us/sample - loss: 0.6140 - accuracy: 0.6784\n",
       "Epoch 119/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5877 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 94us/sample - loss: 0.6127 - accuracy: 0.6771\n",
       "Epoch 120/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5878 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 77us/sample - loss: 0.6195 - accuracy: 0.6849\n",
       "Epoch 121/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5880 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 97us/sample - loss: 0.6069 - accuracy: 0.6875\n",
       "Epoch 122/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5869 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 97us/sample - loss: 0.6123 - accuracy: 0.6875\n",
       "Epoch 123/150\n",
-      "768/768 [==============================] - 0s 60us/sample - loss: 0.5872 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.6131 - accuracy: 0.6771\n",
       "Epoch 124/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5874 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 62us/sample - loss: 0.6085 - accuracy: 0.6849\n",
       "Epoch 125/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5902 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 65us/sample - loss: 0.6068 - accuracy: 0.6810\n",
       "Epoch 126/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5862 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 151us/sample - loss: 0.6042 - accuracy: 0.6836\n",
       "Epoch 127/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5864 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 98us/sample - loss: 0.6038 - accuracy: 0.6901\n",
       "Epoch 128/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5858 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.6099 - accuracy: 0.6927\n",
       "Epoch 129/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5851 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 64us/sample - loss: 0.6055 - accuracy: 0.6875\n",
       "Epoch 130/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5868 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6023 - accuracy: 0.6940\n",
       "Epoch 131/150\n",
-      "768/768 [==============================] - 0s 70us/sample - loss: 0.5855 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6029 - accuracy: 0.6914\n",
       "Epoch 132/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.5862 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 62us/sample - loss: 0.6045 - accuracy: 0.6966\n",
       "Epoch 133/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5842 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 126us/sample - loss: 0.6012 - accuracy: 0.6953\n",
       "Epoch 134/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5866 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 93us/sample - loss: 0.6061 - accuracy: 0.6836\n",
       "Epoch 135/150\n",
-      "768/768 [==============================] - 0s 59us/sample - loss: 0.5841 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 62us/sample - loss: 0.6011 - accuracy: 0.6888\n",
       "Epoch 136/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5836 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6019 - accuracy: 0.6979\n",
       "Epoch 137/150\n",
-      "768/768 [==============================] - 0s 61us/sample - loss: 0.5852 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.5962 - accuracy: 0.6979\n",
       "Epoch 138/150\n",
-      "768/768 [==============================] - 0s 62us/sample - loss: 0.5845 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.5981 - accuracy: 0.7031\n",
       "Epoch 139/150\n",
-      "768/768 [==============================] - 0s 75us/sample - loss: 0.5840 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 68us/sample - loss: 0.5981 - accuracy: 0.7005\n",
       "Epoch 140/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.5836 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 76us/sample - loss: 0.5971 - accuracy: 0.6992\n",
       "Epoch 141/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5843 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 69us/sample - loss: 0.6015 - accuracy: 0.6966\n",
       "Epoch 142/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5828 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6070 - accuracy: 0.6914\n",
       "Epoch 143/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5829 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6030 - accuracy: 0.7018\n",
       "Epoch 144/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5829 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.5987 - accuracy: 0.6849\n",
       "Epoch 145/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5818 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.5983 - accuracy: 0.6927\n",
       "Epoch 146/150\n",
-      "768/768 [==============================] - 0s 63us/sample - loss: 0.5832 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6005 - accuracy: 0.6862\n",
       "Epoch 147/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5813 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.5985 - accuracy: 0.7044\n",
       "Epoch 148/150\n",
-      "768/768 [==============================] - 0s 65us/sample - loss: 0.5832 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.5978 - accuracy: 0.6927\n",
       "Epoch 149/150\n",
-      "768/768 [==============================] - 0s 66us/sample - loss: 0.5808 - accuracy: 0.6510\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.5950 - accuracy: 0.6992\n",
       "Epoch 150/150\n",
-      "768/768 [==============================] - 0s 64us/sample - loss: 0.5806 - accuracy: 0.6510\n"
+      "768/768 [==============================] - 0s 61us/sample - loss: 0.5927 - accuracy: 0.7044\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb4cc230668>"
+       "<tensorflow.python.keras.callbacks.History at 0x144e9f190>"
       ]
      },
-     "execution_count": 19,
+     "execution_count": 31,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -898,7 +898,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 13,
+   "execution_count": 32,
    "metadata": {},
    "outputs": [
     {
@@ -907,7 +907,7 @@
        "(768,)"
       ]
      },
-     "execution_count": 13,
+     "execution_count": 32,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -925,7 +925,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 14,
+   "execution_count": 33,
    "metadata": {},
    "outputs": [
     {
@@ -936,7 +936,7 @@
        "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.])"
       ]
      },
-     "execution_count": 14,
+     "execution_count": 33,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -947,7 +947,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 15,
+   "execution_count": 34,
    "metadata": {},
    "outputs": [
     {
@@ -956,7 +956,7 @@
        "0.3489583333333333"
       ]
      },
-     "execution_count": 15,
+     "execution_count": 34,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -968,15 +968,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 36,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 142us/sample - loss: 0.6205 - accuracy: 0.6510\n",
-      "accuracy: 65.10416865348816\n"
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.5903 - accuracy: 0.7057\n",
+      "accuracy: 70.57291865348816\n"
      ]
     }
    ],
@@ -1036,7 +1036,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 22,
+   "execution_count": 37,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -1053,9 +1053,9 @@
       "=================================================================\n",
       "Dense1 (Dense)               (None, 4)                 36        \n",
       "_________________________________________________________________\n",
-      "dense_6 (Dense)              (None, 3)                 15        \n",
+      "dense_5 (Dense)              (None, 3)                 15        \n",
       "_________________________________________________________________\n",
-      "dense_7 (Dense)              (None, 1)                 4         \n",
+      "dense_6 (Dense)              (None, 1)                 4         \n",
       "=================================================================\n",
       "Total params: 55\n",
       "Trainable params: 55\n",
@@ -1082,16 +1082,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 24,
+   "execution_count": 38,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb4306ce2e8>"
+       "<tensorflow.python.keras.callbacks.History at 0x145080a50>"
       ]
      },
-     "execution_count": 24,
+     "execution_count": 38,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1102,15 +1102,15 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 28,
+   "execution_count": 39,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 68us/sample - loss: 0.5155 - accuracy: 0.7578\n",
-      "accuracy: 75.78125\n"
+      "768/768 [==============================] - 0s 161us/sample - loss: 0.5891 - accuracy: 0.6771\n",
+      "accuracy: 67.70833134651184\n"
      ]
     }
    ],
@@ -1128,7 +1128,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 29,
+   "execution_count": 40,
    "metadata": {},
    "outputs": [
     {
@@ -1141,11 +1141,11 @@
       "=================================================================\n",
       "Dense1 (Dense)               (None, 4)                 36        \n",
       "_________________________________________________________________\n",
-      "dense_8 (Dense)              (None, 3)                 15        \n",
+      "dense_7 (Dense)              (None, 3)                 15        \n",
       "_________________________________________________________________\n",
-      "dense_9 (Dense)              (None, 3)                 12        \n",
+      "dense_8 (Dense)              (None, 3)                 12        \n",
       "_________________________________________________________________\n",
-      "dense_10 (Dense)             (None, 1)                 4         \n",
+      "dense_9 (Dense)              (None, 1)                 4         \n",
       "=================================================================\n",
       "Total params: 67\n",
       "Trainable params: 67\n",
@@ -1173,16 +1173,16 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 30,
+   "execution_count": 41,
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fb430342ac8>"
+       "<tensorflow.python.keras.callbacks.History at 0x145204bd0>"
       ]
      },
-     "execution_count": 30,
+     "execution_count": 41,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1193,23 +1193,23 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 31,
+   "execution_count": 42,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 169us/sample - loss: 0.6716 - accuracy: 0.6510\n"
+      "768/768 [==============================] - 0s 140us/sample - loss: 0.6071 - accuracy: 0.6628\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "[0.639621468881766, 0.6510417]"
+       "[0.6071027877430121, 0.66276044]"
       ]
      },
-     "execution_count": 31,
+     "execution_count": 42,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -1228,7 +1228,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 32,
+   "execution_count": 43,
    "metadata": {},
    "outputs": [
     {
@@ -1241,11 +1241,11 @@
       "=================================================================\n",
       "Dense1 (Dense)               (None, 8)                 72        \n",
       "_________________________________________________________________\n",
-      "dense_11 (Dense)             (None, 8)                 72        \n",
+      "dense_10 (Dense)             (None, 8)                 72        \n",
       "_________________________________________________________________\n",
-      "dense_12 (Dense)             (None, 8)                 72        \n",
+      "dense_11 (Dense)             (None, 8)                 72        \n",
       "_________________________________________________________________\n",
-      "dense_13 (Dense)             (None, 1)                 9         \n",
+      "dense_12 (Dense)             (None, 1)                 9         \n",
       "=================================================================\n",
       "Total params: 225\n",
       "Trainable params: 225\n",
@@ -1271,9 +1271,66 @@
     "model_improved.summary()"
    ]
   },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "model_improved.fit(X,y, epochs=250, validation_split=.10)"
+   ]
+  },
   {
    "cell_type": "code",
-   "execution_count": 37,
+   "execution_count": 45,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "768/768 [==============================] - 0s 175us/sample - loss: 0.5041 - accuracy: 0.7552\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "[0.5040544047951698, 0.7552083]"
+      ]
+     },
+     "execution_count": 45,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model_improved.evaluate(X,y)\n",
+    "#print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "### Student experiments\n",
+    "10 minutes researching the keras api documentation or general neural network architectures. \n",
+    "1 . Optimizers\n",
+    "2 . Loss Metrics\n",
+    "3 . Activation Functions\n",
+    "4 . Parameters for any of the above\n",
+    "5 . No. of layers/no. of Neurons"
+   ]
+  },
+  {
+   "cell_type": "markdown",
+   "metadata": {},
+   "source": [
+    "## Challenge\n",
+    "\n",
+    "You will have to choose your own architectures in today's module project. "
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 50,
    "metadata": {},
    "outputs": [
     {
@@ -1281,559 +1338,651 @@
      "output_type": "stream",
      "text": [
       "Train on 691 samples, validate on 77 samples\n",
+      "691/691 [==============================] - 0s 660us/sample - loss: 0.6690 - accuracy: 0.6570 - val_loss: 0.7069 - val_accuracy: 0.5974\n",
+      "Model: \"SGD\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "dense_19 (Dense)             (None, 4)                 36        \n",
+      "_________________________________________________________________\n",
+      "dense_20 (Dense)             (None, 3)                 15        \n",
+      "_________________________________________________________________\n",
+      "dense_21 (Dense)             (None, 1)                 4         \n",
+      "=================================================================\n",
+      "Total params: 55\n",
+      "Trainable params: 55\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
+   "source": [
+    "# Stochatic optimizer \n",
+    "\n",
+    "model = Sequential(name=\"SGD\")\n",
+    "\n",
+    "model.add(Dense(4, input_dim=8, activation='relu'))\n",
+    "model.add(Dense(3, activation='sigmoid'))\n",
+    "model.add(Dense(1, activation='sigmoid'))\n",
+    "\n",
+    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
+    "\n",
+    "model.fit(X, y, validation_split=.10)\n",
+    "              \n",
+    "model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 52,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "768/768 [==============================] - 0s 163us/sample - loss: 0.6696 - accuracy: 0.6510\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "[0.6696186835567156, 0.6510417]"
+      ]
+     },
+     "execution_count": 52,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
+   "source": [
+    "model.evaluate(X,y)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "# model = Sequentia()\n",
+    "\n",
+    "# model.add()\n",
+    "\n",
+    "# model.compile()\n",
+    "\n",
+    "# model.fit(X, y, validation_split=.10)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 55,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from sklearn.preprocessing import Normalizer\n",
+    "norm = Normalizer()\n",
+    "\n",
+    "\n",
+    "X_n = norm.fit_transform(X)\n",
+    "# model = Sequential(name=\"SGD\")\n",
+    "\n",
+    "# model.add(Dense(4, input_dim=8, activation='relu'))\n",
+    "# model.add(Dense(3, activation='sigmoid'))\n",
+    "# model.add(Dense(1, activation='sigmoid'))\n",
+    "\n",
+    "# model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
+    "\n",
+    "# model.summary()"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 56,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Train on 768 samples\n",
       "Epoch 1/250\n",
-      "691/691 [==============================] - 0s 132us/sample - loss: 0.4450 - accuracy: 0.7844 - val_loss: 0.5430 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 312us/sample - loss: 0.6603 - accuracy: 0.6510\n",
       "Epoch 2/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4350 - accuracy: 0.7931 - val_loss: 0.5368 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6568 - accuracy: 0.6510\n",
       "Epoch 3/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4341 - accuracy: 0.8003 - val_loss: 0.5612 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6539 - accuracy: 0.6510\n",
       "Epoch 4/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4432 - accuracy: 0.7786 - val_loss: 0.5810 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6517 - accuracy: 0.6510\n",
       "Epoch 5/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4378 - accuracy: 0.7858 - val_loss: 0.5426 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6499 - accuracy: 0.6510\n",
       "Epoch 6/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4341 - accuracy: 0.7815 - val_loss: 0.5429 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6486 - accuracy: 0.6510\n",
       "Epoch 7/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4304 - accuracy: 0.8075 - val_loss: 0.5942 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6474 - accuracy: 0.6510\n",
       "Epoch 8/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4388 - accuracy: 0.7829 - val_loss: 0.5631 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6465 - accuracy: 0.6510\n",
       "Epoch 9/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4377 - accuracy: 0.7728 - val_loss: 0.5333 - val_accuracy: 0.8052\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6458 - accuracy: 0.6510\n",
       "Epoch 10/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4476 - accuracy: 0.7815 - val_loss: 0.5622 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6452 - accuracy: 0.6510\n",
       "Epoch 11/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4351 - accuracy: 0.7945 - val_loss: 0.6104 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6448 - accuracy: 0.6510\n",
       "Epoch 12/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4468 - accuracy: 0.7902 - val_loss: 0.5943 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6444 - accuracy: 0.6510\n",
       "Epoch 13/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4304 - accuracy: 0.7873 - val_loss: 0.5597 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6441 - accuracy: 0.6510\n",
       "Epoch 14/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4454 - accuracy: 0.7800 - val_loss: 0.5892 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6439 - accuracy: 0.6510\n",
       "Epoch 15/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4544 - accuracy: 0.7670 - val_loss: 0.5805 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6437 - accuracy: 0.6510\n",
       "Epoch 16/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4406 - accuracy: 0.7829 - val_loss: 0.5796 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 40us/sample - loss: 0.6435 - accuracy: 0.6510\n",
       "Epoch 17/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4325 - accuracy: 0.7887 - val_loss: 0.5502 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6435 - accuracy: 0.6510\n",
       "Epoch 18/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4376 - accuracy: 0.7800 - val_loss: 0.5716 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6433 - accuracy: 0.6510\n",
       "Epoch 19/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4293 - accuracy: 0.7844 - val_loss: 0.5916 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6433 - accuracy: 0.6510\n",
       "Epoch 20/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4372 - accuracy: 0.7902 - val_loss: 0.5735 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6432 - accuracy: 0.6510\n",
       "Epoch 21/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4367 - accuracy: 0.7844 - val_loss: 0.5870 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6431 - accuracy: 0.6510\n",
       "Epoch 22/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4360 - accuracy: 0.7916 - val_loss: 0.5698 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6431 - accuracy: 0.6510\n",
       "Epoch 23/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4375 - accuracy: 0.7873 - val_loss: 0.5782 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6430 - accuracy: 0.6510\n",
       "Epoch 24/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4428 - accuracy: 0.7887 - val_loss: 0.5882 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6430 - accuracy: 0.6510\n",
       "Epoch 25/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4320 - accuracy: 0.7916 - val_loss: 0.5487 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 26/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4361 - accuracy: 0.7959 - val_loss: 0.6589 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6430 - accuracy: 0.6510\n",
       "Epoch 27/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4681 - accuracy: 0.7902 - val_loss: 0.5854 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 28/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4400 - accuracy: 0.7800 - val_loss: 0.5625 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 29/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4297 - accuracy: 0.7858 - val_loss: 0.5486 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 30/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4304 - accuracy: 0.7931 - val_loss: 0.5471 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 31/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4308 - accuracy: 0.7916 - val_loss: 0.5568 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 32/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4303 - accuracy: 0.8017 - val_loss: 0.5375 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6429 - accuracy: 0.6510\n",
       "Epoch 33/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4359 - accuracy: 0.7800 - val_loss: 0.5793 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 34/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4341 - accuracy: 0.7873 - val_loss: 0.5466 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 35/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4293 - accuracy: 0.7858 - val_loss: 0.5446 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 36/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4400 - accuracy: 0.7887 - val_loss: 0.5486 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 37/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4373 - accuracy: 0.7815 - val_loss: 0.5552 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 38/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4380 - accuracy: 0.7916 - val_loss: 0.5937 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 64us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 39/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4321 - accuracy: 0.7902 - val_loss: 0.5762 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 60us/sample - loss: 0.6428 - accuracy: 0.6510\n",
       "Epoch 40/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4344 - accuracy: 0.7844 - val_loss: 0.5833 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 41/250\n",
-      "691/691 [==============================] - 0s 122us/sample - loss: 0.4319 - accuracy: 0.7945 - val_loss: 0.5713 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 42/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4466 - accuracy: 0.7713 - val_loss: 0.5554 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 43/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4512 - accuracy: 0.7699 - val_loss: 0.5515 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 44/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4295 - accuracy: 0.7916 - val_loss: 0.5527 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 45/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4415 - accuracy: 0.7858 - val_loss: 0.5779 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 46/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4273 - accuracy: 0.7988 - val_loss: 0.5882 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6427 - accuracy: 0.6510\n",
       "Epoch 47/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4365 - accuracy: 0.7916 - val_loss: 0.5730 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 48/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4475 - accuracy: 0.7771 - val_loss: 0.6129 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 49/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4330 - accuracy: 0.7844 - val_loss: 0.5698 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 50/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4265 - accuracy: 0.7786 - val_loss: 0.6032 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 51/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4355 - accuracy: 0.7873 - val_loss: 0.6040 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 52/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4307 - accuracy: 0.7829 - val_loss: 0.5508 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 53/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4514 - accuracy: 0.7786 - val_loss: 0.5587 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 54/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4318 - accuracy: 0.7902 - val_loss: 0.5885 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 55/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4512 - accuracy: 0.7815 - val_loss: 0.5813 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 56/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4363 - accuracy: 0.7844 - val_loss: 0.5499 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6426 - accuracy: 0.6510\n",
       "Epoch 57/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4327 - accuracy: 0.7902 - val_loss: 0.5899 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 61us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 58/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4376 - accuracy: 0.7945 - val_loss: 0.5644 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 59/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4308 - accuracy: 0.7902 - val_loss: 0.5926 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 60/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4364 - accuracy: 0.7844 - val_loss: 0.5701 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 61/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4334 - accuracy: 0.7945 - val_loss: 0.5964 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 62/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4349 - accuracy: 0.7829 - val_loss: 0.5404 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 63/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4290 - accuracy: 0.7959 - val_loss: 0.5447 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 64/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4327 - accuracy: 0.7959 - val_loss: 0.5909 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6424 - accuracy: 0.6510\n",
       "Epoch 65/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4352 - accuracy: 0.7916 - val_loss: 0.5384 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6425 - accuracy: 0.6510\n",
       "Epoch 66/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4386 - accuracy: 0.7800 - val_loss: 0.5673 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6424 - accuracy: 0.6510\n",
       "Epoch 67/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4465 - accuracy: 0.7800 - val_loss: 0.5630 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6424 - accuracy: 0.6510\n",
       "Epoch 68/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4288 - accuracy: 0.7887 - val_loss: 0.5608 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6424 - accuracy: 0.6510\n",
       "Epoch 69/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4379 - accuracy: 0.7771 - val_loss: 0.5875 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6424 - accuracy: 0.6510\n",
       "Epoch 70/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4208 - accuracy: 0.7931 - val_loss: 0.5458 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6424 - accuracy: 0.6510\n",
       "Epoch 71/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4375 - accuracy: 0.7858 - val_loss: 0.5575 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 72/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4269 - accuracy: 0.7902 - val_loss: 0.5839 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 73/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4352 - accuracy: 0.7887 - val_loss: 0.5685 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 74/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4353 - accuracy: 0.7800 - val_loss: 0.5833 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 75/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4339 - accuracy: 0.7873 - val_loss: 0.6230 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 76/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4517 - accuracy: 0.7844 - val_loss: 0.5550 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 77/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4286 - accuracy: 0.7916 - val_loss: 0.5464 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6423 - accuracy: 0.6510\n",
       "Epoch 78/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4311 - accuracy: 0.7829 - val_loss: 0.5428 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 79/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4317 - accuracy: 0.7873 - val_loss: 0.5425 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 80/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4291 - accuracy: 0.7844 - val_loss: 0.5598 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 81/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4272 - accuracy: 0.7931 - val_loss: 0.5490 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 82/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4406 - accuracy: 0.7786 - val_loss: 0.5553 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 83/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4359 - accuracy: 0.7902 - val_loss: 0.5498 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 84/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4296 - accuracy: 0.7887 - val_loss: 0.5613 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 64us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 85/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4312 - accuracy: 0.7829 - val_loss: 0.5609 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 86/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4378 - accuracy: 0.7829 - val_loss: 0.5826 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.6422 - accuracy: 0.6510\n",
       "Epoch 87/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4405 - accuracy: 0.7815 - val_loss: 0.5750 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 88/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4417 - accuracy: 0.7916 - val_loss: 0.5690 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 89/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4369 - accuracy: 0.7815 - val_loss: 0.5858 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 90/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4374 - accuracy: 0.7800 - val_loss: 0.5682 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 91/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4356 - accuracy: 0.7844 - val_loss: 0.5611 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 92/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4474 - accuracy: 0.7815 - val_loss: 0.6017 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 93/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4294 - accuracy: 0.7945 - val_loss: 0.5852 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 94/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4310 - accuracy: 0.7844 - val_loss: 0.5812 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6421 - accuracy: 0.6510\n",
       "Epoch 95/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4336 - accuracy: 0.7902 - val_loss: 0.5876 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6420 - accuracy: 0.6510\n",
       "Epoch 96/250\n",
-      "691/691 [==============================] - 0s 100us/sample - loss: 0.4403 - accuracy: 0.7713 - val_loss: 0.6064 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6420 - accuracy: 0.6510\n",
       "Epoch 97/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4504 - accuracy: 0.7800 - val_loss: 0.5883 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6420 - accuracy: 0.6510\n",
       "Epoch 98/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4353 - accuracy: 0.7988 - val_loss: 0.5885 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 99/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4357 - accuracy: 0.7757 - val_loss: 0.5428 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6420 - accuracy: 0.6510\n",
       "Epoch 100/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4440 - accuracy: 0.7728 - val_loss: 0.5738 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 101/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4327 - accuracy: 0.7829 - val_loss: 0.5985 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 102/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4354 - accuracy: 0.7887 - val_loss: 0.5707 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 103/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4529 - accuracy: 0.7829 - val_loss: 0.6645 - val_accuracy: 0.6364\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 104/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4368 - accuracy: 0.7829 - val_loss: 0.5762 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 105/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4291 - accuracy: 0.7887 - val_loss: 0.5867 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 106/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4273 - accuracy: 0.7959 - val_loss: 0.6049 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6419 - accuracy: 0.6510\n",
       "Epoch 107/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4344 - accuracy: 0.7945 - val_loss: 0.5747 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 108/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4319 - accuracy: 0.7959 - val_loss: 0.6106 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 109/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4308 - accuracy: 0.7887 - val_loss: 0.5425 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 110/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4328 - accuracy: 0.7945 - val_loss: 0.5476 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 111/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4234 - accuracy: 0.7959 - val_loss: 0.7005 - val_accuracy: 0.6234\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 112/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4554 - accuracy: 0.7728 - val_loss: 0.6428 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 113/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4315 - accuracy: 0.7945 - val_loss: 0.5558 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 114/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4346 - accuracy: 0.7916 - val_loss: 0.5521 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6418 - accuracy: 0.6510\n",
       "Epoch 115/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4328 - accuracy: 0.8003 - val_loss: 0.5748 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6417 - accuracy: 0.6510\n",
       "Epoch 116/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4331 - accuracy: 0.7988 - val_loss: 0.5338 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6417 - accuracy: 0.6510\n",
       "Epoch 117/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4311 - accuracy: 0.7959 - val_loss: 0.6022 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6417 - accuracy: 0.6510\n",
       "Epoch 118/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4380 - accuracy: 0.7771 - val_loss: 0.5797 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6417 - accuracy: 0.6510\n",
       "Epoch 119/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4304 - accuracy: 0.7858 - val_loss: 0.5868 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6417 - accuracy: 0.6510\n",
       "Epoch 120/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4309 - accuracy: 0.7786 - val_loss: 0.5755 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 121/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4284 - accuracy: 0.7887 - val_loss: 0.5662 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 122/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4307 - accuracy: 0.7902 - val_loss: 0.5881 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 123/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4381 - accuracy: 0.7873 - val_loss: 0.6287 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 124/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4290 - accuracy: 0.7829 - val_loss: 0.5570 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 125/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4253 - accuracy: 0.7873 - val_loss: 0.5644 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 126/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4249 - accuracy: 0.7931 - val_loss: 0.5739 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 127/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4247 - accuracy: 0.7959 - val_loss: 0.5544 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 53us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 128/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4322 - accuracy: 0.7844 - val_loss: 0.5472 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 129/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4285 - accuracy: 0.7916 - val_loss: 0.5964 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6416 - accuracy: 0.6510\n",
       "Epoch 130/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4282 - accuracy: 0.7959 - val_loss: 0.5719 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6415 - accuracy: 0.6510\n",
       "Epoch 131/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4262 - accuracy: 0.7873 - val_loss: 0.5588 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6415 - accuracy: 0.6510\n",
       "Epoch 132/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4320 - accuracy: 0.7959 - val_loss: 0.5484 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6415 - accuracy: 0.6510\n",
       "Epoch 133/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4397 - accuracy: 0.7844 - val_loss: 0.5603 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6415 - accuracy: 0.6510\n",
       "Epoch 134/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4287 - accuracy: 0.7858 - val_loss: 0.5460 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6415 - accuracy: 0.6510\n",
       "Epoch 135/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4373 - accuracy: 0.7916 - val_loss: 0.5439 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6414 - accuracy: 0.6510\n",
       "Epoch 136/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4378 - accuracy: 0.7902 - val_loss: 0.5814 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6414 - accuracy: 0.6510\n",
       "Epoch 137/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4412 - accuracy: 0.7858 - val_loss: 0.5689 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6414 - accuracy: 0.6510\n",
       "Epoch 138/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4304 - accuracy: 0.7916 - val_loss: 0.5558 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6414 - accuracy: 0.6510\n",
       "Epoch 139/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4278 - accuracy: 0.7974 - val_loss: 0.5501 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6414 - accuracy: 0.6510\n",
       "Epoch 140/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4295 - accuracy: 0.7844 - val_loss: 0.5862 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6414 - accuracy: 0.6510\n",
       "Epoch 141/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4348 - accuracy: 0.8003 - val_loss: 0.6358 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 142/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4422 - accuracy: 0.7800 - val_loss: 0.5796 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 143/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4374 - accuracy: 0.7988 - val_loss: 0.5926 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 144/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4365 - accuracy: 0.7844 - val_loss: 0.5565 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 145/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4276 - accuracy: 0.7959 - val_loss: 0.5469 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 146/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4273 - accuracy: 0.7945 - val_loss: 0.5813 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 147/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4271 - accuracy: 0.7844 - val_loss: 0.5794 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 60us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 148/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4253 - accuracy: 0.7974 - val_loss: 0.5377 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 149/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4295 - accuracy: 0.7959 - val_loss: 0.5592 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6413 - accuracy: 0.6510\n",
       "Epoch 150/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4446 - accuracy: 0.7873 - val_loss: 0.5472 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 57us/sample - loss: 0.6412 - accuracy: 0.6510\n",
       "Epoch 151/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4275 - accuracy: 0.7959 - val_loss: 0.5726 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6412 - accuracy: 0.6510\n",
       "Epoch 152/250\n",
-      "691/691 [==============================] - 0s 101us/sample - loss: 0.4350 - accuracy: 0.7887 - val_loss: 0.5549 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6412 - accuracy: 0.6510\n",
       "Epoch 153/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4352 - accuracy: 0.7887 - val_loss: 0.5623 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 154/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4381 - accuracy: 0.7873 - val_loss: 0.6157 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 58us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 155/250\n",
-      "691/691 [==============================] - 0s 100us/sample - loss: 0.4451 - accuracy: 0.7786 - val_loss: 0.5586 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 156/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4294 - accuracy: 0.7902 - val_loss: 0.5669 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 157/250\n",
-      "691/691 [==============================] - 0s 104us/sample - loss: 0.4446 - accuracy: 0.7728 - val_loss: 0.5718 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 158/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4357 - accuracy: 0.7945 - val_loss: 0.5919 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 159/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4244 - accuracy: 0.7959 - val_loss: 0.6961 - val_accuracy: 0.5974\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6411 - accuracy: 0.6510\n",
       "Epoch 160/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4460 - accuracy: 0.7815 - val_loss: 0.6046 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6410 - accuracy: 0.6510\n",
       "Epoch 161/250\n",
-      "691/691 [==============================] - 0s 119us/sample - loss: 0.4392 - accuracy: 0.7800 - val_loss: 0.6032 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6410 - accuracy: 0.6510\n",
       "Epoch 162/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4257 - accuracy: 0.7844 - val_loss: 0.6199 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6410 - accuracy: 0.6510\n",
       "Epoch 163/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4304 - accuracy: 0.7916 - val_loss: 0.6495 - val_accuracy: 0.6364\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6410 - accuracy: 0.6510\n",
       "Epoch 164/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4354 - accuracy: 0.7988 - val_loss: 0.5772 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6410 - accuracy: 0.6510\n",
       "Epoch 165/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4322 - accuracy: 0.7757 - val_loss: 0.5907 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6410 - accuracy: 0.6510\n",
       "Epoch 166/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4383 - accuracy: 0.7844 - val_loss: 0.5795 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 167/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4323 - accuracy: 0.7931 - val_loss: 0.6260 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 168/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4343 - accuracy: 0.7945 - val_loss: 0.5615 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 169/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4289 - accuracy: 0.7974 - val_loss: 0.5703 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 170/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4280 - accuracy: 0.7902 - val_loss: 0.5718 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 60us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 171/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4257 - accuracy: 0.7873 - val_loss: 0.5567 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 172/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4307 - accuracy: 0.7945 - val_loss: 0.5513 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 173/250\n",
-      "691/691 [==============================] - 0s 125us/sample - loss: 0.4239 - accuracy: 0.8017 - val_loss: 0.5830 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6409 - accuracy: 0.6510\n",
       "Epoch 174/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4255 - accuracy: 0.7974 - val_loss: 0.5419 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6408 - accuracy: 0.6510\n",
       "Epoch 175/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4613 - accuracy: 0.7713 - val_loss: 0.5484 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6408 - accuracy: 0.6510\n",
       "Epoch 176/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4456 - accuracy: 0.7916 - val_loss: 0.5627 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6408 - accuracy: 0.6510\n",
       "Epoch 177/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4381 - accuracy: 0.7916 - val_loss: 0.5449 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6408 - accuracy: 0.6510\n",
       "Epoch 178/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4512 - accuracy: 0.7771 - val_loss: 0.5280 - val_accuracy: 0.7922\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6408 - accuracy: 0.6510\n",
       "Epoch 179/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4309 - accuracy: 0.7988 - val_loss: 0.5866 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 180/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4230 - accuracy: 0.7873 - val_loss: 0.5618 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 181/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4261 - accuracy: 0.7931 - val_loss: 0.5515 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 182/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4267 - accuracy: 0.7974 - val_loss: 0.5404 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 183/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4336 - accuracy: 0.7974 - val_loss: 0.5765 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 184/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4317 - accuracy: 0.7858 - val_loss: 0.5615 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 185/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4294 - accuracy: 0.7916 - val_loss: 0.5702 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6407 - accuracy: 0.6510\n",
       "Epoch 186/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4235 - accuracy: 0.7959 - val_loss: 0.5582 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6406 - accuracy: 0.6510\n",
       "Epoch 187/250\n",
-      "691/691 [==============================] - 0s 102us/sample - loss: 0.4248 - accuracy: 0.7959 - val_loss: 0.5515 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6406 - accuracy: 0.6510\n",
       "Epoch 188/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4296 - accuracy: 0.7916 - val_loss: 0.6143 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6406 - accuracy: 0.6510\n",
       "Epoch 189/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4264 - accuracy: 0.7887 - val_loss: 0.5686 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6406 - accuracy: 0.6510\n",
       "Epoch 190/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4224 - accuracy: 0.8003 - val_loss: 0.5524 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6405 - accuracy: 0.6510\n",
       "Epoch 191/250\n",
-      "691/691 [==============================] - 0s 103us/sample - loss: 0.4298 - accuracy: 0.7945 - val_loss: 0.5425 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6405 - accuracy: 0.6510\n",
       "Epoch 192/250\n",
-      "691/691 [==============================] - 0s 105us/sample - loss: 0.4248 - accuracy: 0.7974 - val_loss: 0.5546 - val_accuracy: 0.7792\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6405 - accuracy: 0.6510\n",
       "Epoch 193/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4309 - accuracy: 0.7916 - val_loss: 0.6047 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6405 - accuracy: 0.6510\n",
       "Epoch 194/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4375 - accuracy: 0.8003 - val_loss: 0.5604 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6405 - accuracy: 0.6510\n",
       "Epoch 195/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4221 - accuracy: 0.7959 - val_loss: 0.5587 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6405 - accuracy: 0.6510\n",
       "Epoch 196/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4223 - accuracy: 0.7902 - val_loss: 0.5423 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 197/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4304 - accuracy: 0.7916 - val_loss: 0.6116 - val_accuracy: 0.6623\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 198/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4300 - accuracy: 0.7815 - val_loss: 0.5754 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 199/250\n",
-      "691/691 [==============================] - 0s 116us/sample - loss: 0.4248 - accuracy: 0.7959 - val_loss: 0.5608 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 200/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4347 - accuracy: 0.7815 - val_loss: 0.5849 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 201/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4316 - accuracy: 0.7858 - val_loss: 0.6119 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 202/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4332 - accuracy: 0.7902 - val_loss: 0.5676 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 203/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4353 - accuracy: 0.7742 - val_loss: 0.5462 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6404 - accuracy: 0.6510\n",
       "Epoch 204/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4218 - accuracy: 0.7959 - val_loss: 0.5536 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6403 - accuracy: 0.6510\n",
       "Epoch 205/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4249 - accuracy: 0.7974 - val_loss: 0.5566 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6403 - accuracy: 0.6510\n",
       "Epoch 206/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4309 - accuracy: 0.7988 - val_loss: 0.6227 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6403 - accuracy: 0.6510\n",
       "Epoch 207/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4426 - accuracy: 0.8017 - val_loss: 0.5527 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6403 - accuracy: 0.6510\n",
       "Epoch 208/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4240 - accuracy: 0.7945 - val_loss: 0.5741 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6402 - accuracy: 0.6510\n",
       "Epoch 209/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4220 - accuracy: 0.8003 - val_loss: 0.5496 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6402 - accuracy: 0.6510\n",
       "Epoch 210/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4335 - accuracy: 0.7858 - val_loss: 0.5671 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6402 - accuracy: 0.6510\n",
       "Epoch 211/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4296 - accuracy: 0.7931 - val_loss: 0.5924 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6402 - accuracy: 0.6510\n",
       "Epoch 212/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4445 - accuracy: 0.7771 - val_loss: 0.5769 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6402 - accuracy: 0.6510\n",
       "Epoch 213/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4326 - accuracy: 0.7945 - val_loss: 0.5796 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6402 - accuracy: 0.6510\n",
       "Epoch 214/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4225 - accuracy: 0.7974 - val_loss: 0.5894 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6401 - accuracy: 0.6510\n",
       "Epoch 215/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4219 - accuracy: 0.7988 - val_loss: 0.5632 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6401 - accuracy: 0.6510\n",
       "Epoch 216/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4329 - accuracy: 0.7844 - val_loss: 0.5483 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 44us/sample - loss: 0.6401 - accuracy: 0.6510\n",
       "Epoch 217/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4232 - accuracy: 0.8032 - val_loss: 0.5585 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6401 - accuracy: 0.6510\n",
       "Epoch 218/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4485 - accuracy: 0.7728 - val_loss: 0.5537 - val_accuracy: 0.8052\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 219/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4373 - accuracy: 0.7988 - val_loss: 0.5409 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6401 - accuracy: 0.6510\n",
       "Epoch 220/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4216 - accuracy: 0.8003 - val_loss: 0.5648 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 46us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 221/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4290 - accuracy: 0.7988 - val_loss: 0.5723 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 222/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4247 - accuracy: 0.7916 - val_loss: 0.5825 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 43us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 223/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4257 - accuracy: 0.7931 - val_loss: 0.5462 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 224/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4377 - accuracy: 0.7931 - val_loss: 0.5473 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 45us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 225/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4260 - accuracy: 0.7988 - val_loss: 0.5481 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6400 - accuracy: 0.6510\n",
       "Epoch 226/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4254 - accuracy: 0.8003 - val_loss: 0.5611 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6399 - accuracy: 0.6510\n",
       "Epoch 227/250\n",
-      "691/691 [==============================] - 0s 112us/sample - loss: 0.4325 - accuracy: 0.7988 - val_loss: 0.5663 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 60us/sample - loss: 0.6399 - accuracy: 0.6510\n",
       "Epoch 228/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4480 - accuracy: 0.7873 - val_loss: 0.5549 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 67us/sample - loss: 0.6399 - accuracy: 0.6510\n",
       "Epoch 229/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4264 - accuracy: 0.7931 - val_loss: 0.5778 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6399 - accuracy: 0.6510\n",
       "Epoch 230/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4295 - accuracy: 0.7931 - val_loss: 0.5351 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6398 - accuracy: 0.6510\n",
       "Epoch 231/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4355 - accuracy: 0.7873 - val_loss: 0.5499 - val_accuracy: 0.7662\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6398 - accuracy: 0.6510\n",
       "Epoch 232/250\n",
-      "691/691 [==============================] - 0s 122us/sample - loss: 0.4338 - accuracy: 0.7945 - val_loss: 0.5842 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 47us/sample - loss: 0.6398 - accuracy: 0.6510\n",
       "Epoch 233/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4278 - accuracy: 0.7945 - val_loss: 0.5997 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6398 - accuracy: 0.6510\n",
       "Epoch 234/250\n",
-      "691/691 [==============================] - 0s 111us/sample - loss: 0.4360 - accuracy: 0.7800 - val_loss: 0.5947 - val_accuracy: 0.6753\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6398 - accuracy: 0.6510\n",
       "Epoch 235/250\n",
-      "691/691 [==============================] - 0s 113us/sample - loss: 0.4275 - accuracy: 0.7988 - val_loss: 0.5561 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6398 - accuracy: 0.6510\n",
       "Epoch 236/250\n",
-      "691/691 [==============================] - 0s 114us/sample - loss: 0.4229 - accuracy: 0.7902 - val_loss: 0.5543 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 55us/sample - loss: 0.6397 - accuracy: 0.6510\n",
       "Epoch 237/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4232 - accuracy: 0.7887 - val_loss: 0.5724 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 56us/sample - loss: 0.6397 - accuracy: 0.6510\n",
       "Epoch 238/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4278 - accuracy: 0.8003 - val_loss: 0.5673 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6397 - accuracy: 0.6510\n",
       "Epoch 239/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4350 - accuracy: 0.8017 - val_loss: 0.5767 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 54us/sample - loss: 0.6397 - accuracy: 0.6510\n",
       "Epoch 240/250\n",
-      "691/691 [==============================] - 0s 109us/sample - loss: 0.4240 - accuracy: 0.7844 - val_loss: 0.5844 - val_accuracy: 0.6883\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6396 - accuracy: 0.6510\n",
       "Epoch 241/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4246 - accuracy: 0.7988 - val_loss: 0.5608 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6397 - accuracy: 0.6510\n",
       "Epoch 242/250\n",
-      "691/691 [==============================] - 0s 107us/sample - loss: 0.4225 - accuracy: 0.8017 - val_loss: 0.5395 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 52us/sample - loss: 0.6396 - accuracy: 0.6510\n",
       "Epoch 243/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4318 - accuracy: 0.7974 - val_loss: 0.5513 - val_accuracy: 0.7403\n",
+      "768/768 [==============================] - 0s 59us/sample - loss: 0.6396 - accuracy: 0.6510\n",
       "Epoch 244/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4237 - accuracy: 0.7974 - val_loss: 0.5530 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6396 - accuracy: 0.6510\n",
       "Epoch 245/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4369 - accuracy: 0.7887 - val_loss: 0.5769 - val_accuracy: 0.7532\n",
+      "768/768 [==============================] - 0s 48us/sample - loss: 0.6396 - accuracy: 0.6510\n",
       "Epoch 246/250\n",
-      "691/691 [==============================] - 0s 110us/sample - loss: 0.4223 - accuracy: 0.7858 - val_loss: 0.5797 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6396 - accuracy: 0.6510\n",
       "Epoch 247/250\n",
-      "691/691 [==============================] - 0s 108us/sample - loss: 0.4224 - accuracy: 0.7945 - val_loss: 0.5757 - val_accuracy: 0.7013\n",
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6395 - accuracy: 0.6510\n",
       "Epoch 248/250\n",
-      "691/691 [==============================] - 0s 119us/sample - loss: 0.4231 - accuracy: 0.7988 - val_loss: 0.5811 - val_accuracy: 0.7143\n",
+      "768/768 [==============================] - 0s 49us/sample - loss: 0.6395 - accuracy: 0.6510\n",
       "Epoch 249/250\n",
-      "691/691 [==============================] - 0s 115us/sample - loss: 0.4202 - accuracy: 0.7988 - val_loss: 0.5648 - val_accuracy: 0.7273\n",
+      "768/768 [==============================] - 0s 51us/sample - loss: 0.6395 - accuracy: 0.6510\n",
       "Epoch 250/250\n",
-      "691/691 [==============================] - 0s 106us/sample - loss: 0.4216 - accuracy: 0.7988 - val_loss: 0.5674 - val_accuracy: 0.7013\n"
+      "768/768 [==============================] - 0s 50us/sample - loss: 0.6394 - accuracy: 0.6510\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7fafff74d048>"
+       "<tensorflow.python.keras.callbacks.History at 0x1471982d0>"
       ]
      },
-     "execution_count": 37,
+     "execution_count": 56,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "model_improved.fit(X,y, epochs=250, validation_split=.10)"
+    "model.fit(X_n, y, epochs=250)"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 34,
+   "execution_count": null,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "768/1 [================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 0s 161us/sample - loss: 0.4706 - accuracy: 0.7630\n"
-     ]
-    },
-    {
-     "data": {
-      "text/plain": [
-       "[0.4735589859386285, 0.7630208]"
-      ]
-     },
-     "execution_count": 34,
-     "metadata": {},
-     "output_type": "execute_result"
-    }
-   ],
-   "source": [
-    "model_improved.evaluate(X,y)\n",
-    "#print(f\"{model_improved.metrics_names[1]}: {scores[1]*100}\")"
-   ]
+   "outputs": [],
+   "source": []
   },
   {
-   "cell_type": "markdown",
+   "cell_type": "code",
+   "execution_count": null,
    "metadata": {},
-   "source": [
-    "## Challenge\n",
-    "\n",
-    "You will have to choose your own architectures in today's module project. "
-   ]
+   "outputs": [],
+   "source": []
+  },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": []
   },
   {
    "cell_type": "markdown",
@@ -2003,7 +2152,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 38,
+   "execution_count": 59,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -2024,7 +2173,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 39,
+   "execution_count": 60,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2036,18 +2185,9 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 40,
+   "execution_count": 61,
    "metadata": {},
-   "outputs": [
-    {
-     "name": "stdout",
-     "output_type": "stream",
-     "text": [
-      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
-      "11493376/11490434 [==============================] - 0s 0us/step\n"
-     ]
-    }
-   ],
+   "outputs": [],
    "source": [
     "# Load the Data\n",
     "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
@@ -2055,7 +2195,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 41,
+   "execution_count": 62,
    "metadata": {},
    "outputs": [
     {
@@ -2064,7 +2204,7 @@
        "(28, 28)"
       ]
      },
-     "execution_count": 41,
+     "execution_count": 62,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2075,25 +2215,130 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 63,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(60000, 28, 28)"
+      ]
+     },
+     "execution_count": 63,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "X_train.shape"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 64,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
+       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
+       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
+       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
+       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
+       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
+       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
+       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
+       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
+       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
+       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
+       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
+       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
+       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
+       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0],\n",
+       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
+       "          0,   0]], dtype=uint8)"
+      ]
+     },
+     "execution_count": 64,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "X_train[0]"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 42,
+   "execution_count": 65,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2104,7 +2349,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 43,
+   "execution_count": 66,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2115,7 +2360,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 44,
+   "execution_count": 67,
    "metadata": {},
    "outputs": [
     {
@@ -2124,7 +2369,7 @@
        "4"
       ]
      },
-     "execution_count": 44,
+     "execution_count": 67,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2135,7 +2380,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 45,
+   "execution_count": 68,
    "metadata": {},
    "outputs": [
     {
@@ -2144,7 +2389,7 @@
        "4"
       ]
      },
-     "execution_count": 45,
+     "execution_count": 68,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2155,7 +2400,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 47,
+   "execution_count": 69,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2168,7 +2413,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 48,
+   "execution_count": 70,
    "metadata": {},
    "outputs": [
     {
@@ -2177,7 +2422,7 @@
        "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32)"
       ]
      },
-     "execution_count": 48,
+     "execution_count": 70,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -2188,26 +2433,26 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 49,
+   "execution_count": 71,
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Model: \"sequential_3\"\n",
+      "Model: \"sequential_6\"\n",
       "_________________________________________________________________\n",
       "Layer (type)                 Output Shape              Param #   \n",
       "=================================================================\n",
-      "dense_14 (Dense)             (None, 16)                12560     \n",
+      "dense_25 (Dense)             (None, 16)                12560     \n",
       "_________________________________________________________________\n",
-      "dense_15 (Dense)             (None, 16)                272       \n",
+      "dense_26 (Dense)             (None, 16)                272       \n",
       "_________________________________________________________________\n",
-      "dense_16 (Dense)             (None, 16)                272       \n",
+      "dense_27 (Dense)             (None, 16)                272       \n",
       "_________________________________________________________________\n",
-      "dense_17 (Dense)             (None, 16)                272       \n",
+      "dense_28 (Dense)             (None, 16)                272       \n",
       "_________________________________________________________________\n",
-      "dense_18 (Dense)             (None, 10)                170       \n",
+      "dense_29 (Dense)             (None, 10)                170       \n",
       "=================================================================\n",
       "Total params: 13,546\n",
       "Trainable params: 13,546\n",
@@ -2240,7 +2485,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 72,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -2255,47 +2500,54 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 73,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "12544"
+      ]
+     },
+     "execution_count": 73,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "16 *  784"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 74,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/plain": [
+       "(10000, 10)"
+      ]
+     },
+     "execution_count": 74,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "y_test.shape"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 50,
+   "execution_count": 75,
    "metadata": {},
    "outputs": [
     {
-     "ename": "KeyboardInterrupt",
-     "evalue": "",
-     "output_type": "error",
-     "traceback": [
-      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
-      "\u001b[0;32m<ipython-input-50-fd0808510929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#print(f'{mnist_model.metrics_names[1]}: {scores[1]*100}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
-      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
-      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "10000/10000 [==============================] - 0s 28us/sample - loss: 0.2631 - accuracy: 0.9433\n"
      ]
     }
    ],
@@ -2319,9 +2571,34 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 57,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Model: \"sequential_5\"\n",
+      "_________________________________________________________________\n",
+      "Layer (type)                 Output Shape              Param #   \n",
+      "=================================================================\n",
+      "dense_22 (Dense)             (None, 32)                25120     \n",
+      "_________________________________________________________________\n",
+      "dropout (Dropout)            (None, 32)                0         \n",
+      "_________________________________________________________________\n",
+      "dense_23 (Dense)             (None, 16)                528       \n",
+      "_________________________________________________________________\n",
+      "dropout_1 (Dropout)          (None, 16)                0         \n",
+      "_________________________________________________________________\n",
+      "dense_24 (Dense)             (None, 10)                170       \n",
+      "=================================================================\n",
+      "Total params: 25,818\n",
+      "Trainable params: 25,818\n",
+      "Non-trainable params: 0\n",
+      "_________________________________________________________________\n"
+     ]
+    }
+   ],
    "source": [
     "### Let's do it!\n",
     "from tensorflow import keras \n",
@@ -2349,9 +2626,21 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 58,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "ename": "NameError",
+     "evalue": "name 'X_train' is not defined",
+     "output_type": "error",
+     "traceback": [
+      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
+      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
+      "\u001b[0;32m<ipython-input-58-6fc9a66c1d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{mnist_model.metrics_names[1]}: {scores[1]*100}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
+      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
+     ]
+    }
+   ],
    "source": [
     "history = mnist_model.fit(X_train, y_train, batch_size=32, epochs=epochs, validation_split=.1, verbose=0)\n",
     "scores = mnist_model.evaluate(X_test, y_test)\n",
@@ -2390,7 +2679,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.8"
+   "version": "3.7.4"
   },
   "toc-autonumbering": false,
   "toc-showmarkdowntxt": false
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
index 4bb18e9..6146fbc 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
@@ -187,161 +187,161 @@
      "text": [
       "Train on 404 samples, validate on 102 samples\n",
       "Epoch 1/75\n",
-      "404/404 [==============================] - 2s 4ms/sample - loss: 498.2045 - mse: 498.2046 - mae: 20.2543 - val_loss: 421.5039 - val_mse: 421.5038 - val_mae: 18.3349\n",
+      "404/404 [==============================] - 0s 1ms/sample - loss: 505.3136 - mse: 505.3135 - mae: 20.2977 - val_loss: 433.1483 - val_mse: 433.1483 - val_mae: 18.4231\n",
       "Epoch 2/75\n",
-      "404/404 [==============================] - 0s 347us/sample - loss: 249.6985 - mse: 249.6985 - mae: 13.2672 - val_loss: 111.3743 - val_mse: 111.3743 - val_mae: 8.6210\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 278.9926 - mse: 278.9926 - mae: 14.1701 - val_loss: 155.1293 - val_mse: 155.1293 - val_mae: 10.6601\n",
       "Epoch 3/75\n",
-      "404/404 [==============================] - 0s 344us/sample - loss: 56.6755 - mse: 56.6755 - mae: 5.4817 - val_loss: 39.1997 - val_mse: 39.1997 - val_mae: 4.9872\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 65.8106 - mse: 65.8106 - mae: 6.1497 - val_loss: 44.0741 - val_mse: 44.0741 - val_mae: 4.9582\n",
       "Epoch 4/75\n",
-      "404/404 [==============================] - 0s 364us/sample - loss: 28.3243 - mse: 28.3243 - mae: 3.7054 - val_loss: 26.9866 - val_mse: 26.9866 - val_mae: 4.0796\n",
+      "404/404 [==============================] - 0s 149us/sample - loss: 29.6452 - mse: 29.6452 - mae: 3.7868 - val_loss: 30.2783 - val_mse: 30.2783 - val_mae: 4.2386\n",
       "Epoch 5/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 20.5281 - mse: 20.5281 - mae: 3.1209 - val_loss: 24.6172 - val_mse: 24.6172 - val_mae: 3.8052\n",
+      "404/404 [==============================] - 0s 152us/sample - loss: 21.3930 - mse: 21.3930 - mae: 3.1720 - val_loss: 25.5349 - val_mse: 25.5349 - val_mae: 3.9198\n",
       "Epoch 6/75\n",
-      "404/404 [==============================] - 0s 393us/sample - loss: 17.9283 - mse: 17.9283 - mae: 2.8665 - val_loss: 23.6524 - val_mse: 23.6524 - val_mae: 3.6746\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 18.4121 - mse: 18.4121 - mae: 2.9768 - val_loss: 24.0613 - val_mse: 24.0613 - val_mae: 3.7427\n",
       "Epoch 7/75\n",
-      "404/404 [==============================] - 0s 440us/sample - loss: 16.9179 - mse: 16.9179 - mae: 2.8781 - val_loss: 23.4620 - val_mse: 23.4620 - val_mae: 3.5778\n",
+      "404/404 [==============================] - 0s 177us/sample - loss: 16.7432 - mse: 16.7432 - mae: 2.7739 - val_loss: 24.5914 - val_mse: 24.5914 - val_mae: 3.7805\n",
       "Epoch 8/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 15.1579 - mse: 15.1579 - mae: 2.6440 - val_loss: 24.1374 - val_mse: 24.1374 - val_mae: 3.5929\n",
+      "404/404 [==============================] - 0s 158us/sample - loss: 15.6736 - mse: 15.6736 - mae: 2.7473 - val_loss: 23.7507 - val_mse: 23.7507 - val_mae: 3.6371\n",
       "Epoch 9/75\n",
-      "404/404 [==============================] - 0s 367us/sample - loss: 14.1717 - mse: 14.1717 - mae: 2.5937 - val_loss: 24.4829 - val_mse: 24.4829 - val_mae: 3.5639\n",
+      "404/404 [==============================] - 0s 149us/sample - loss: 14.4908 - mse: 14.4908 - mae: 2.5846 - val_loss: 23.5098 - val_mse: 23.5098 - val_mae: 3.6197\n",
       "Epoch 10/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 13.5002 - mse: 13.5002 - mae: 2.5633 - val_loss: 25.0170 - val_mse: 25.0170 - val_mae: 3.5601\n",
+      "404/404 [==============================] - 0s 155us/sample - loss: 13.7474 - mse: 13.7474 - mae: 2.5291 - val_loss: 23.6900 - val_mse: 23.6900 - val_mae: 3.5534\n",
       "Epoch 11/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.8641 - mse: 12.8641 - mae: 2.4963 - val_loss: 25.1162 - val_mse: 25.1162 - val_mae: 3.5449\n",
+      "404/404 [==============================] - 0s 150us/sample - loss: 12.9206 - mse: 12.9206 - mae: 2.4912 - val_loss: 23.8405 - val_mse: 23.8404 - val_mae: 3.4925\n",
       "Epoch 12/75\n",
-      "404/404 [==============================] - 0s 351us/sample - loss: 12.4033 - mse: 12.4033 - mae: 2.5224 - val_loss: 25.0382 - val_mse: 25.0382 - val_mae: 3.4858\n",
+      "404/404 [==============================] - 0s 149us/sample - loss: 12.4098 - mse: 12.4098 - mae: 2.4486 - val_loss: 23.2974 - val_mse: 23.2974 - val_mae: 3.4180\n",
       "Epoch 13/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 12.2653 - mse: 12.2653 - mae: 2.4637 - val_loss: 26.7274 - val_mse: 26.7274 - val_mae: 3.6054\n",
+      "404/404 [==============================] - 0s 152us/sample - loss: 12.0234 - mse: 12.0234 - mae: 2.4259 - val_loss: 23.3725 - val_mse: 23.3725 - val_mae: 3.3946\n",
       "Epoch 14/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 11.8249 - mse: 11.8249 - mae: 2.4648 - val_loss: 25.2347 - val_mse: 25.2347 - val_mae: 3.4602\n",
+      "404/404 [==============================] - 0s 150us/sample - loss: 11.6473 - mse: 11.6473 - mae: 2.4016 - val_loss: 23.1234 - val_mse: 23.1234 - val_mae: 3.3131\n",
       "Epoch 15/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 11.3965 - mse: 11.3965 - mae: 2.4134 - val_loss: 25.3070 - val_mse: 25.3070 - val_mae: 3.4305\n",
+      "404/404 [==============================] - 0s 151us/sample - loss: 11.4575 - mse: 11.4575 - mae: 2.3531 - val_loss: 23.6392 - val_mse: 23.6392 - val_mae: 3.3418\n",
       "Epoch 16/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 11.0982 - mse: 11.0982 - mae: 2.3616 - val_loss: 25.0599 - val_mse: 25.0599 - val_mae: 3.3784\n",
+      "404/404 [==============================] - 0s 149us/sample - loss: 10.8582 - mse: 10.8582 - mae: 2.3010 - val_loss: 23.6823 - val_mse: 23.6823 - val_mae: 3.2878\n",
       "Epoch 17/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 11.1969 - mse: 11.1969 - mae: 2.3806 - val_loss: 25.1976 - val_mse: 25.1976 - val_mae: 3.3732\n",
+      "404/404 [==============================] - 0s 150us/sample - loss: 10.7240 - mse: 10.7240 - mae: 2.3069 - val_loss: 23.0996 - val_mse: 23.0996 - val_mae: 3.2409\n",
       "Epoch 18/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 10.9278 - mse: 10.9278 - mae: 2.3653 - val_loss: 24.2875 - val_mse: 24.2875 - val_mae: 3.3114\n",
+      "404/404 [==============================] - 0s 149us/sample - loss: 10.4865 - mse: 10.4865 - mae: 2.2777 - val_loss: 24.2108 - val_mse: 24.2108 - val_mae: 3.2859\n",
       "Epoch 19/75\n",
-      "404/404 [==============================] - 0s 365us/sample - loss: 10.5854 - mse: 10.5854 - mae: 2.3170 - val_loss: 26.1450 - val_mse: 26.1450 - val_mae: 3.3971\n",
+      "404/404 [==============================] - 0s 149us/sample - loss: 10.2137 - mse: 10.2137 - mae: 2.2466 - val_loss: 23.5406 - val_mse: 23.5406 - val_mae: 3.2136\n",
       "Epoch 20/75\n",
-      "404/404 [==============================] - 0s 401us/sample - loss: 10.2546 - mse: 10.2546 - mae: 2.2813 - val_loss: 26.5278 - val_mse: 26.5278 - val_mae: 3.4465\n",
+      "404/404 [==============================] - 0s 154us/sample - loss: 10.1944 - mse: 10.1944 - mae: 2.2683 - val_loss: 23.8730 - val_mse: 23.8730 - val_mae: 3.2047\n",
       "Epoch 21/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 10.1321 - mse: 10.1321 - mae: 2.2866 - val_loss: 24.0363 - val_mse: 24.0363 - val_mae: 3.2792\n",
+      "404/404 [==============================] - 0s 148us/sample - loss: 10.0502 - mse: 10.0502 - mae: 2.2321 - val_loss: 21.7029 - val_mse: 21.7029 - val_mae: 3.0798\n",
       "Epoch 22/75\n",
-      "404/404 [==============================] - 0s 421us/sample - loss: 9.9169 - mse: 9.9169 - mae: 2.2907 - val_loss: 23.7310 - val_mse: 23.7310 - val_mae: 3.2334\n",
+      "404/404 [==============================] - 0s 150us/sample - loss: 9.9268 - mse: 9.9268 - mae: 2.2052 - val_loss: 23.6522 - val_mse: 23.6522 - val_mae: 3.1710\n",
       "Epoch 23/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.6588 - mse: 9.6588 - mae: 2.2284 - val_loss: 23.6472 - val_mse: 23.6472 - val_mae: 3.2013\n",
+      "404/404 [==============================] - 0s 156us/sample - loss: 9.8027 - mse: 9.8027 - mae: 2.2272 - val_loss: 22.6522 - val_mse: 22.6522 - val_mae: 3.1102\n",
       "Epoch 24/75\n",
-      "404/404 [==============================] - 0s 363us/sample - loss: 9.6887 - mse: 9.6887 - mae: 2.2468 - val_loss: 23.5379 - val_mse: 23.5379 - val_mae: 3.1921\n",
+      "404/404 [==============================] - 0s 161us/sample - loss: 9.5643 - mse: 9.5643 - mae: 2.1923 - val_loss: 23.1735 - val_mse: 23.1735 - val_mae: 3.1265\n",
       "Epoch 25/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 9.4049 - mse: 9.4049 - mae: 2.1999 - val_loss: 23.7713 - val_mse: 23.7713 - val_mae: 3.2273\n",
+      "404/404 [==============================] - 0s 151us/sample - loss: 9.6084 - mse: 9.6084 - mae: 2.2046 - val_loss: 23.1333 - val_mse: 23.1333 - val_mae: 3.0996\n",
       "Epoch 26/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 9.2304 - mse: 9.2304 - mae: 2.1946 - val_loss: 23.5093 - val_mse: 23.5093 - val_mae: 3.2072\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 9.2413 - mse: 9.2413 - mae: 2.1563 - val_loss: 22.0914 - val_mse: 22.0914 - val_mae: 3.0377\n",
       "Epoch 27/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 9.0493 - mse: 9.0493 - mae: 2.1528 - val_loss: 23.7969 - val_mse: 23.7969 - val_mae: 3.2005\n",
+      "404/404 [==============================] - 0s 150us/sample - loss: 9.2439 - mse: 9.2439 - mae: 2.1626 - val_loss: 23.1887 - val_mse: 23.1887 - val_mae: 3.1145\n",
       "Epoch 28/75\n",
-      "404/404 [==============================] - 0s 359us/sample - loss: 8.9363 - mse: 8.9363 - mae: 2.1475 - val_loss: 22.1030 - val_mse: 22.1030 - val_mae: 3.0707\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 9.1371 - mse: 9.1371 - mae: 2.1351 - val_loss: 22.9150 - val_mse: 22.9150 - val_mae: 3.0693\n",
       "Epoch 29/75\n",
-      "404/404 [==============================] - 0s 373us/sample - loss: 8.7834 - mse: 8.7834 - mae: 2.1231 - val_loss: 22.5153 - val_mse: 22.5153 - val_mae: 3.1532\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 8.9822 - mse: 8.9822 - mae: 2.1192 - val_loss: 22.4008 - val_mse: 22.4008 - val_mae: 3.0681\n",
       "Epoch 30/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.7925 - mse: 8.7925 - mae: 2.1531 - val_loss: 22.0449 - val_mse: 22.0449 - val_mae: 3.1245\n",
+      "404/404 [==============================] - 0s 148us/sample - loss: 9.0207 - mse: 9.0207 - mae: 2.1182 - val_loss: 22.2923 - val_mse: 22.2923 - val_mae: 2.9982\n",
       "Epoch 31/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 9.1879 - mse: 9.1879 - mae: 2.2029 - val_loss: 22.1780 - val_mse: 22.1780 - val_mae: 3.0623\n",
+      "404/404 [==============================] - 0s 157us/sample - loss: 8.6941 - mse: 8.6941 - mae: 2.0839 - val_loss: 21.9520 - val_mse: 21.9520 - val_mae: 3.0383\n",
       "Epoch 32/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.7136 - mse: 8.7136 - mae: 2.1164 - val_loss: 21.9815 - val_mse: 21.9815 - val_mae: 3.0969\n",
+      "404/404 [==============================] - 0s 168us/sample - loss: 8.6190 - mse: 8.6190 - mae: 2.1015 - val_loss: 22.5713 - val_mse: 22.5713 - val_mae: 3.0435\n",
       "Epoch 33/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 8.3018 - mse: 8.3018 - mae: 2.0639 - val_loss: 21.0477 - val_mse: 21.0477 - val_mae: 2.9645\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 8.6088 - mse: 8.6088 - mae: 2.0683 - val_loss: 22.5695 - val_mse: 22.5695 - val_mae: 3.0027\n",
       "Epoch 34/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 8.4156 - mse: 8.4156 - mae: 2.0970 - val_loss: 22.6659 - val_mse: 22.6659 - val_mae: 3.1235\n",
+      "404/404 [==============================] - 0s 152us/sample - loss: 8.5591 - mse: 8.5591 - mae: 2.0990 - val_loss: 22.0828 - val_mse: 22.0828 - val_mae: 3.1104\n",
       "Epoch 35/75\n",
-      "404/404 [==============================] - 0s 350us/sample - loss: 8.2938 - mse: 8.2938 - mae: 2.0567 - val_loss: 20.9574 - val_mse: 20.9574 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 155us/sample - loss: 8.4946 - mse: 8.4946 - mae: 2.0356 - val_loss: 21.2450 - val_mse: 21.2450 - val_mae: 2.9690\n",
       "Epoch 36/75\n",
-      "404/404 [==============================] - 0s 357us/sample - loss: 8.0515 - mse: 8.0515 - mae: 2.0591 - val_loss: 23.2063 - val_mse: 23.2063 - val_mae: 3.1980\n",
+      "404/404 [==============================] - 0s 155us/sample - loss: 8.2106 - mse: 8.2106 - mae: 2.0183 - val_loss: 21.6792 - val_mse: 21.6792 - val_mae: 2.9471\n",
       "Epoch 37/75\n",
-      "404/404 [==============================] - 0s 381us/sample - loss: 8.1403 - mse: 8.1403 - mae: 2.0584 - val_loss: 24.5238 - val_mse: 24.5237 - val_mae: 3.3531\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 8.1918 - mse: 8.1918 - mae: 2.0019 - val_loss: 21.7580 - val_mse: 21.7580 - val_mae: 2.9635\n",
       "Epoch 38/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 8.0043 - mse: 8.0043 - mae: 2.0776 - val_loss: 22.5424 - val_mse: 22.5424 - val_mae: 3.1494\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 8.2612 - mse: 8.2612 - mae: 2.0109 - val_loss: 22.3964 - val_mse: 22.3964 - val_mae: 3.0138\n",
       "Epoch 39/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 8.1182 - mse: 8.1182 - mae: 2.0683 - val_loss: 19.7576 - val_mse: 19.7576 - val_mae: 2.8799\n",
+      "404/404 [==============================] - 0s 155us/sample - loss: 7.9546 - mse: 7.9546 - mae: 1.9827 - val_loss: 20.7719 - val_mse: 20.7719 - val_mae: 2.8678\n",
       "Epoch 40/75\n",
-      "404/404 [==============================] - 0s 374us/sample - loss: 7.8578 - mse: 7.8578 - mae: 2.0131 - val_loss: 20.7728 - val_mse: 20.7728 - val_mae: 2.9499\n",
+      "404/404 [==============================] - 0s 151us/sample - loss: 7.9387 - mse: 7.9387 - mae: 1.9691 - val_loss: 20.6656 - val_mse: 20.6656 - val_mae: 2.8838\n",
       "Epoch 41/75\n",
-      "404/404 [==============================] - 0s 382us/sample - loss: 7.5711 - mse: 7.5711 - mae: 1.9896 - val_loss: 20.6170 - val_mse: 20.6170 - val_mae: 2.9936\n",
+      "404/404 [==============================] - 0s 157us/sample - loss: 7.8032 - mse: 7.8032 - mae: 1.9485 - val_loss: 21.1426 - val_mse: 21.1426 - val_mae: 2.8863\n",
       "Epoch 42/75\n",
-      "404/404 [==============================] - 0s 385us/sample - loss: 7.5822 - mse: 7.5822 - mae: 1.9683 - val_loss: 20.8541 - val_mse: 20.8541 - val_mae: 3.0054\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 7.6134 - mse: 7.6134 - mae: 1.9386 - val_loss: 21.0124 - val_mse: 21.0124 - val_mae: 2.9329\n",
       "Epoch 43/75\n",
-      "404/404 [==============================] - 0s 408us/sample - loss: 7.4533 - mse: 7.4533 - mae: 1.9645 - val_loss: 20.4473 - val_mse: 20.4473 - val_mae: 2.8861\n",
+      "404/404 [==============================] - 0s 152us/sample - loss: 7.6575 - mse: 7.6575 - mae: 1.9421 - val_loss: 19.7353 - val_mse: 19.7353 - val_mae: 2.8636\n",
       "Epoch 44/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 7.5226 - mse: 7.5226 - mae: 1.9509 - val_loss: 20.5193 - val_mse: 20.5193 - val_mae: 2.9619\n",
+      "404/404 [==============================] - 0s 150us/sample - loss: 7.9195 - mse: 7.9195 - mae: 1.9671 - val_loss: 19.4245 - val_mse: 19.4245 - val_mae: 2.7962\n",
       "Epoch 45/75\n",
-      "404/404 [==============================] - 0s 355us/sample - loss: 7.2819 - mse: 7.2819 - mae: 1.9350 - val_loss: 21.4862 - val_mse: 21.4862 - val_mae: 2.9908\n",
+      "404/404 [==============================] - 0s 153us/sample - loss: 7.5055 - mse: 7.5055 - mae: 1.9355 - val_loss: 19.6390 - val_mse: 19.6390 - val_mae: 2.8381\n",
       "Epoch 46/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 7.0130 - mse: 7.0130 - mae: 1.9152 - val_loss: 20.1577 - val_mse: 20.1577 - val_mae: 2.9370\n",
+      "404/404 [==============================] - 0s 155us/sample - loss: 7.2808 - mse: 7.2808 - mae: 1.8774 - val_loss: 21.4724 - val_mse: 21.4724 - val_mae: 3.0259\n",
       "Epoch 47/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.9431 - mse: 6.9431 - mae: 1.8819 - val_loss: 21.1210 - val_mse: 21.1210 - val_mae: 2.9746\n",
+      "404/404 [==============================] - 0s 158us/sample - loss: 7.2536 - mse: 7.2536 - mae: 1.8849 - val_loss: 20.7757 - val_mse: 20.7757 - val_mae: 2.8823\n",
       "Epoch 48/75\n",
-      "404/404 [==============================] - 0s 371us/sample - loss: 6.8982 - mse: 6.8982 - mae: 1.9037 - val_loss: 19.2999 - val_mse: 19.2999 - val_mae: 2.8638\n",
+      "404/404 [==============================] - 0s 179us/sample - loss: 7.1382 - mse: 7.1382 - mae: 1.8795 - val_loss: 19.3882 - val_mse: 19.3882 - val_mae: 2.8026\n",
       "Epoch 49/75\n",
-      "404/404 [==============================] - 0s 368us/sample - loss: 6.9521 - mse: 6.9521 - mae: 1.8862 - val_loss: 20.7825 - val_mse: 20.7825 - val_mae: 2.9369\n",
+      "404/404 [==============================] - 0s 156us/sample - loss: 7.0862 - mse: 7.0862 - mae: 1.8595 - val_loss: 20.4907 - val_mse: 20.4907 - val_mae: 2.9141\n",
       "Epoch 50/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.8718 - mse: 6.8718 - mae: 1.8889 - val_loss: 20.0288 - val_mse: 20.0288 - val_mae: 2.8915\n",
+      "404/404 [==============================] - 0s 159us/sample - loss: 7.3083 - mse: 7.3083 - mae: 1.9399 - val_loss: 18.9536 - val_mse: 18.9536 - val_mae: 2.8255\n",
       "Epoch 51/75\n",
-      "404/404 [==============================] - 0s 354us/sample - loss: 6.7111 - mse: 6.7111 - mae: 1.8702 - val_loss: 20.4913 - val_mse: 20.4913 - val_mae: 3.0116\n",
+      "404/404 [==============================] - 0s 187us/sample - loss: 7.3754 - mse: 7.3754 - mae: 1.9287 - val_loss: 18.8721 - val_mse: 18.8721 - val_mae: 2.8750\n",
       "Epoch 52/75\n",
-      "404/404 [==============================] - 0s 361us/sample - loss: 6.7492 - mse: 6.7492 - mae: 1.8482 - val_loss: 18.3008 - val_mse: 18.3008 - val_mae: 2.7362\n",
+      "404/404 [==============================] - 0s 194us/sample - loss: 6.8835 - mse: 6.8835 - mae: 1.8438 - val_loss: 19.7538 - val_mse: 19.7538 - val_mae: 2.8315\n",
       "Epoch 53/75\n",
-      "404/404 [==============================] - 0s 356us/sample - loss: 6.6262 - mse: 6.6262 - mae: 1.8395 - val_loss: 18.1885 - val_mse: 18.1885 - val_mae: 2.6920\n",
+      "404/404 [==============================] - 0s 203us/sample - loss: 7.2997 - mse: 7.2997 - mae: 1.9070 - val_loss: 18.6390 - val_mse: 18.6390 - val_mae: 2.7801\n",
       "Epoch 54/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 6.7148 - mse: 6.7148 - mae: 1.8611 - val_loss: 18.5764 - val_mse: 18.5764 - val_mae: 2.6977\n",
+      "404/404 [==============================] - 0s 184us/sample - loss: 6.8694 - mse: 6.8694 - mae: 1.8407 - val_loss: 18.4135 - val_mse: 18.4135 - val_mae: 2.7024\n",
       "Epoch 55/75\n",
-      "404/404 [==============================] - 0s 358us/sample - loss: 6.5425 - mse: 6.5425 - mae: 1.8522 - val_loss: 19.5772 - val_mse: 19.5772 - val_mae: 2.8326\n",
+      "404/404 [==============================] - 0s 194us/sample - loss: 6.7154 - mse: 6.7154 - mae: 1.8223 - val_loss: 19.7860 - val_mse: 19.7860 - val_mae: 2.8403\n",
       "Epoch 56/75\n",
-      "404/404 [==============================] - 0s 423us/sample - loss: 6.3349 - mse: 6.3349 - mae: 1.8175 - val_loss: 19.0932 - val_mse: 19.0932 - val_mae: 2.8260\n",
+      "404/404 [==============================] - 0s 200us/sample - loss: 6.5568 - mse: 6.5568 - mae: 1.7853 - val_loss: 18.8225 - val_mse: 18.8225 - val_mae: 2.7511\n",
       "Epoch 57/75\n",
-      "404/404 [==============================] - 0s 375us/sample - loss: 6.4253 - mse: 6.4253 - mae: 1.7972 - val_loss: 20.4036 - val_mse: 20.4036 - val_mae: 2.9258\n",
+      "404/404 [==============================] - 0s 202us/sample - loss: 6.4982 - mse: 6.4982 - mae: 1.7829 - val_loss: 19.3884 - val_mse: 19.3884 - val_mae: 2.8043\n",
       "Epoch 58/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.2897 - mse: 6.2897 - mae: 1.7785 - val_loss: 21.2845 - val_mse: 21.2845 - val_mae: 3.0715\n",
+      "404/404 [==============================] - 0s 183us/sample - loss: 6.4081 - mse: 6.4081 - mae: 1.7839 - val_loss: 20.7371 - val_mse: 20.7371 - val_mae: 2.9978\n",
       "Epoch 59/75\n",
-      "404/404 [==============================] - 0s 378us/sample - loss: 6.7839 - mse: 6.7839 - mae: 1.9027 - val_loss: 18.6853 - val_mse: 18.6853 - val_mae: 2.7709\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 6.8380 - mse: 6.8380 - mae: 1.8368 - val_loss: 18.4390 - val_mse: 18.4390 - val_mae: 2.8176\n",
       "Epoch 60/75\n",
-      "404/404 [==============================] - 0s 395us/sample - loss: 6.7178 - mse: 6.7178 - mae: 1.8871 - val_loss: 19.5394 - val_mse: 19.5394 - val_mae: 2.8101\n",
+      "404/404 [==============================] - 0s 199us/sample - loss: 6.4031 - mse: 6.4031 - mae: 1.7753 - val_loss: 19.4259 - val_mse: 19.4259 - val_mae: 2.8486\n",
       "Epoch 61/75\n",
-      "404/404 [==============================] - 0s 366us/sample - loss: 6.4152 - mse: 6.4152 - mae: 1.8175 - val_loss: 18.2377 - val_mse: 18.2377 - val_mae: 2.7450\n",
+      "404/404 [==============================] - 0s 192us/sample - loss: 6.2540 - mse: 6.2540 - mae: 1.7468 - val_loss: 19.0309 - val_mse: 19.0309 - val_mae: 2.8289\n",
       "Epoch 62/75\n",
-      "404/404 [==============================] - 0s 384us/sample - loss: 5.9727 - mse: 5.9727 - mae: 1.7630 - val_loss: 19.0252 - val_mse: 19.0252 - val_mae: 2.7960\n",
+      "404/404 [==============================] - 0s 196us/sample - loss: 6.2659 - mse: 6.2659 - mae: 1.7855 - val_loss: 19.3749 - val_mse: 19.3749 - val_mae: 2.7856\n",
       "Epoch 63/75\n",
-      "404/404 [==============================] - 0s 380us/sample - loss: 6.0973 - mse: 6.0973 - mae: 1.8071 - val_loss: 18.8069 - val_mse: 18.8069 - val_mae: 2.8894\n",
+      "404/404 [==============================] - 0s 207us/sample - loss: 6.1824 - mse: 6.1824 - mae: 1.7643 - val_loss: 18.7447 - val_mse: 18.7447 - val_mae: 2.7986\n",
       "Epoch 64/75\n",
-      "404/404 [==============================] - 0s 362us/sample - loss: 6.1074 - mse: 6.1074 - mae: 1.7978 - val_loss: 18.4702 - val_mse: 18.4702 - val_mae: 2.7851\n",
+      "404/404 [==============================] - 0s 188us/sample - loss: 5.9718 - mse: 5.9718 - mae: 1.7081 - val_loss: 17.2795 - val_mse: 17.2795 - val_mae: 2.6928\n",
       "Epoch 65/75\n",
-      "404/404 [==============================] - 0s 369us/sample - loss: 5.9329 - mse: 5.9329 - mae: 1.7545 - val_loss: 18.5321 - val_mse: 18.5321 - val_mae: 2.7933\n",
+      "404/404 [==============================] - 0s 201us/sample - loss: 6.1408 - mse: 6.1408 - mae: 1.7509 - val_loss: 17.8569 - val_mse: 17.8569 - val_mae: 2.7060\n",
       "Epoch 66/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.7473 - mse: 5.7473 - mae: 1.7211 - val_loss: 18.5536 - val_mse: 18.5536 - val_mae: 2.8010\n",
+      "404/404 [==============================] - 0s 189us/sample - loss: 5.9173 - mse: 5.9173 - mae: 1.7159 - val_loss: 18.3334 - val_mse: 18.3334 - val_mae: 2.7595\n",
       "Epoch 67/75\n",
-      "404/404 [==============================] - 0s 339us/sample - loss: 5.8866 - mse: 5.8866 - mae: 1.7224 - val_loss: 18.0067 - val_mse: 18.0067 - val_mae: 2.7054\n",
+      "404/404 [==============================] - 0s 202us/sample - loss: 5.8289 - mse: 5.8289 - mae: 1.6830 - val_loss: 18.3416 - val_mse: 18.3416 - val_mae: 2.7866\n",
       "Epoch 68/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.7885 - mse: 5.7885 - mae: 1.7391 - val_loss: 17.5502 - val_mse: 17.5502 - val_mae: 2.6767\n",
+      "404/404 [==============================] - 0s 191us/sample - loss: 5.7902 - mse: 5.7902 - mae: 1.6988 - val_loss: 17.6917 - val_mse: 17.6917 - val_mae: 2.6530\n",
       "Epoch 69/75\n",
-      "404/404 [==============================] - 0s 331us/sample - loss: 5.8809 - mse: 5.8809 - mae: 1.7542 - val_loss: 17.0280 - val_mse: 17.0280 - val_mae: 2.6404\n",
+      "404/404 [==============================] - 0s 189us/sample - loss: 5.8760 - mse: 5.8760 - mae: 1.7328 - val_loss: 17.2111 - val_mse: 17.2111 - val_mae: 2.6473\n",
       "Epoch 70/75\n",
-      "404/404 [==============================] - 0s 343us/sample - loss: 5.6028 - mse: 5.6028 - mae: 1.6972 - val_loss: 17.7188 - val_mse: 17.7188 - val_mae: 2.6979\n",
+      "404/404 [==============================] - 0s 197us/sample - loss: 5.9228 - mse: 5.9228 - mae: 1.6975 - val_loss: 16.4747 - val_mse: 16.4747 - val_mae: 2.6431\n",
       "Epoch 71/75\n",
-      "404/404 [==============================] - 0s 337us/sample - loss: 5.4361 - mse: 5.4361 - mae: 1.6741 - val_loss: 16.8852 - val_mse: 16.8852 - val_mae: 2.6126\n",
+      "404/404 [==============================] - 0s 202us/sample - loss: 5.6002 - mse: 5.6002 - mae: 1.6822 - val_loss: 17.9558 - val_mse: 17.9558 - val_mae: 2.7636\n",
       "Epoch 72/75\n",
-      "404/404 [==============================] - 0s 345us/sample - loss: 5.5608 - mse: 5.5608 - mae: 1.7252 - val_loss: 16.7483 - val_mse: 16.7483 - val_mae: 2.6063\n",
+      "404/404 [==============================] - 0s 189us/sample - loss: 5.4279 - mse: 5.4279 - mae: 1.6472 - val_loss: 16.5193 - val_mse: 16.5193 - val_mae: 2.5988\n",
       "Epoch 73/75\n",
-      "404/404 [==============================] - 0s 341us/sample - loss: 5.5022 - mse: 5.5022 - mae: 1.6912 - val_loss: 17.6786 - val_mse: 17.6786 - val_mae: 2.7316\n",
+      "404/404 [==============================] - 0s 199us/sample - loss: 5.4043 - mse: 5.4043 - mae: 1.6368 - val_loss: 18.1246 - val_mse: 18.1246 - val_mae: 2.7434\n",
       "Epoch 74/75\n",
-      "404/404 [==============================] - 0s 396us/sample - loss: 5.2794 - mse: 5.2794 - mae: 1.6478 - val_loss: 17.6115 - val_mse: 17.6115 - val_mae: 2.6773\n",
+      "404/404 [==============================] - 0s 189us/sample - loss: 5.6090 - mse: 5.6090 - mae: 1.6745 - val_loss: 17.7949 - val_mse: 17.7949 - val_mae: 2.7356\n",
       "Epoch 75/75\n",
-      "404/404 [==============================] - 0s 338us/sample - loss: 5.4796 - mse: 5.4796 - mae: 1.6876 - val_loss: 17.2835 - val_mse: 17.2835 - val_mae: 2.7126\n"
+      "404/404 [==============================] - 0s 197us/sample - loss: 5.3422 - mse: 5.3422 - mae: 1.6459 - val_loss: 17.5469 - val_mse: 17.5469 - val_mae: 2.7655\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f36340c6b38>"
+       "<tensorflow.python.keras.callbacks.History at 0x14000eed0>"
       ]
      },
      "execution_count": 3,
@@ -464,7 +464,7 @@
      "name": "stderr",
      "output_type": "stream",
      "text": [
-      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
+      "/Users/Medy/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
       "  warnings.warn(CV_WARNING, FutureWarning)\n"
      ]
     },
@@ -472,13 +472,13 @@
      "name": "stdout",
      "output_type": "stream",
      "text": [
-      "Best: 0.65234375 using {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.65234375, Stdev: 0.033298728782667764 with: {'batch_size': 10, 'epochs': 20}\n",
-      "Means: 0.6263020833333334, Stdev: 0.01813592223591682 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6041666666666666, Stdev: 0.037782859709757574 with: {'batch_size': 40, 'epochs': 20}\n",
-      "Means: 0.5533854166666666, Stdev: 0.03210632293213009 with: {'batch_size': 60, 'epochs': 20}\n",
-      "Means: 0.61328125, Stdev: 0.024079742199097563 with: {'batch_size': 80, 'epochs': 20}\n",
-      "Means: 0.5611979166666666, Stdev: 0.038450060052691144 with: {'batch_size': 100, 'epochs': 20}\n"
+      "Best: 0.6497395833333334 using {'batch_size': 60, 'epochs': 20}\n",
+      "Means: 0.62109375, Stdev: 0.07293991444875478 with: {'batch_size': 10, 'epochs': 20}\n",
+      "Means: 0.6341145833333334, Stdev: 0.04525559165880928 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.62109375, Stdev: 0.020914555213813815 with: {'batch_size': 40, 'epochs': 20}\n",
+      "Means: 0.6497395833333334, Stdev: 0.020751793555350555 with: {'batch_size': 60, 'epochs': 20}\n",
+      "Means: 0.5546875, Stdev: 0.11878254762292172 with: {'batch_size': 80, 'epochs': 20}\n",
+      "Means: 0.52734375, Stdev: 0.03866990209613932 with: {'batch_size': 100, 'epochs': 20}\n"
      ]
     }
    ],
@@ -525,7 +525,7 @@
     "              'epochs': [20]}\n",
     "\n",
     "# Create Grid Search\n",
-    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1)\n",
+    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
     "grid_result = grid.fit(X, Y)\n",
     "\n",
     "# Report Results\n",
@@ -567,10 +567,10 @@
      "output_type": "stream",
      "text": [
       "Best: 0.7044270833333334 using {'batch_size': 20, 'epochs': 200}\n",
-      "Means: 0.6666666666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 20}\n",
-      "Means: 0.6588541666666666, Stdev: 0.028940248399600087 with: {'batch_size': 20, 'epochs': 40}\n",
-      "Means: 0.6848958333333334, Stdev: 0.03498705427745938 with: {'batch_size': 20, 'epochs': 60}\n",
-      "Means: 0.7044270833333334, Stdev: 0.018414239093399672 with: {'batch_size': 20, 'epochs': 200}\n"
+      "Means: 0.5703125, Stdev: 0.035516098169234 with: {'batch_size': 20, 'epochs': 20}\n",
+      "Means: 0.6497395833333334, Stdev: 0.018414239093399672 with: {'batch_size': 20, 'epochs': 40}\n",
+      "Means: 0.6484375, Stdev: 0.036225072248030094 with: {'batch_size': 20, 'epochs': 60}\n",
+      "Means: 0.7044270833333334, Stdev: 0.012889967365379774 with: {'batch_size': 20, 'epochs': 200}\n"
      ]
     }
    ],
@@ -730,44 +730,98 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 6,
-   "metadata": {},
+   "execution_count": 7,
+   "metadata": {
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    }
+   },
    "outputs": [
     {
-     "data": {
-      "text/html": [
-       "\n",
-       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro</a><br/>\n",
-       "            "
-      ],
-      "text/plain": [
-       "<IPython.core.display.HTML object>"
-      ]
-     },
-     "metadata": {},
-     "output_type": "display_data"
-    },
-    {
-     "data": {
-      "text/plain": [
-       "W&B Run: https://app.wandb.ai/lambda-ds7/boston/runs/whw09rro"
-      ]
-     },
-     "execution_count": 6,
-     "metadata": {},
-     "output_type": "execute_result"
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Collecting wandb\n",
+      "  Using cached https://files.pythonhosted.org/packages/62/7c/cd5ef2bc3aa0597b5fd5645357f0207c02345ec2f3569dcbb56e6dafa070/wandb-0.8.27-py2.py3-none-any.whl\n",
+      "Collecting sentry-sdk>=0.4.0 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/23/5a/f1b0c63e40517b06bc21744a94013ca05de21de2687a59de889ea20a9ebd/sentry_sdk-0.14.1-py2.py3-none-any.whl\n",
+      "Collecting GitPython>=1.0.0 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/9d/38/e11e9376a91d55502ad153ce9391b06fa59741357b9e9d5cc2fc9c23ce93/GitPython-3.0.8-py3-none-any.whl\n",
+      "Requirement already satisfied: PyYAML>=3.10 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from wandb) (5.1.2)\n",
+      "Requirement already satisfied: Click>=7.0 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from wandb) (7.0)\n",
+      "Requirement already satisfied: psutil>=5.0.0 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from wandb) (5.6.3)\n",
+      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
+      "Collecting nvidia-ml-py3>=7.352.0 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/6d/64/cce82bddb80c0b0f5c703bbdafa94bfb69a1c5ad7a79cff00b482468f0d3/nvidia-ml-py3-7.352.0.tar.gz\n",
+      "Collecting shortuuid>=0.5.0 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/80/d7/2bfc9332e68d3e15ea97b9b1588b3899ad565120253d3fd71c8f7f13b4fe/shortuuid-0.5.0.tar.gz\n",
+      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from wandb) (2.8.0)\n",
+      "Requirement already satisfied: six>=1.10.0 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from wandb) (1.12.0)\n",
+      "Collecting watchdog>=0.8.3 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/73/c3/ed6d992006837e011baca89476a4bbffb0a91602432f73bd4473816c76e2/watchdog-0.10.2.tar.gz\n",
+      "Collecting subprocess32>=3.5.3 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz\n",
+      "Requirement already satisfied: requests>=2.0.0 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from wandb) (2.22.0)\n",
+      "Collecting configparser>=3.8.1 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/7a/2a/95ed0501cf5d8709490b1d3a3f9b5cf340da6c433f896bbe9ce08dbe6785/configparser-4.0.2-py2.py3-none-any.whl\n",
+      "Collecting gql==0.2.0 (from wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/c4/6f/cf9a3056045518f06184e804bae89390eb706168349daa9dff8ac609962a/gql-0.2.0.tar.gz\n",
+      "Requirement already satisfied: urllib3>=1.10.0 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from sentry-sdk>=0.4.0->wandb) (1.24.2)\n",
+      "Requirement already satisfied: certifi in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from sentry-sdk>=0.4.0->wandb) (2019.9.11)\n",
+      "Collecting gitdb2>=3 (from GitPython>=1.0.0->wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/18/a8/370c8767e9d4133cdb1192ac42f377d09975bd0494e6a896b21462cc4679/gitdb2-3.0.2-py2.py3-none-any.whl\n",
+      "Collecting pathtools>=0.1.1 (from watchdog>=0.8.3->wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
+      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->wandb) (2.8)\n",
+      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/Medy/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
+      "Collecting graphql-core<2,>=0.5.0 (from gql==0.2.0->wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/b0/89/00ad5e07524d8c523b14d70c685e0299a8b0de6d0727e368c41b89b7ed0b/graphql-core-1.1.tar.gz\n",
+      "Collecting promise<3,>=2.0 (from gql==0.2.0->wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/cf/9c/fb5d48abfe5d791cd496e4242ebcf87a4bb2e0c3dcd6e0ae68c11426a528/promise-2.3.tar.gz\n",
+      "Collecting smmap2>=2.0.0 (from gitdb2>=3->GitPython>=1.0.0->wandb)\n",
+      "  Using cached https://files.pythonhosted.org/packages/55/d2/866d45e3a121ee15a1dc013824d58072fd5c7799c9c34d01378eb262ca8f/smmap2-2.0.5-py2.py3-none-any.whl\n",
+      "Building wheels for collected packages: nvidia-ml-py3, shortuuid, watchdog, subprocess32, gql, pathtools, graphql-core, promise\n",
+      "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-cp37-none-any.whl size=19192 sha256=9abb24d08a988f6b710919efcdbcd28ee539fa518b622ae43db2f8a60d9ade96\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/e4/1d/06/640c93f5270d67d0247f30be91f232700d19023f9e66d735c7\n",
+      "  Building wheel for shortuuid (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for shortuuid: filename=shortuuid-0.5.0-cp37-none-any.whl size=5499 sha256=9b44961eb1c4db156aa9be34e24c903b73a10e9565bc0a830161ac679b35f5a7\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/3f/eb/fd/69e5177f67b505e44acbd1aedfbe44b91768ee0c4cd5636576\n",
+      "  Building wheel for watchdog (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for watchdog: filename=watchdog-0.10.2-cp37-cp37m-macosx_10_9_x86_64.whl size=78282 sha256=c9c4c3a8d3f17bb5d9bb4d99d35985206afc784564bffc7225f27e9bab38e92a\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/bc/ed/6c/028dea90d31b359cd2a7c8b0da4db80e41d24a59614154072e\n",
+      "  Building wheel for subprocess32 (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6490 sha256=1ba7159453e44f9df1039465f2a051640249bf99d933c2490441c64b2b23ec12\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
+      "  Building wheel for gql (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for gql: filename=gql-0.2.0-cp37-none-any.whl size=7630 sha256=08859530d667aee686696c61fcae86cc8573a68395e71ad74459f86b2ac8ad1b\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/ce/0e/7b/58a8a5268655b3ad74feef5aa97946f0addafb3cbb6bd2da23\n",
+      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=20053af261dd1a560380418abdb6797437a40896b853c276a34c9e5b849cac50\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
+      "  Building wheel for graphql-core (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for graphql-core: filename=graphql_core-1.1-cp37-none-any.whl size=104651 sha256=393624d870461aba1fa5c737ecb81a66aa377a08ecb70d8a4f3caa377bab7bf2\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/45/99/d7/c424029bb0fe910c63b68dbf2aa20d3283d023042521bcd7d5\n",
+      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
+      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-cp37-none-any.whl size=21494 sha256=ac4a9d7e7dff0d6121424ea9e79095fc8edc799b80fe3045f885284c3aab6f8d\n",
+      "  Stored in directory: /Users/Medy/Library/Caches/pip/wheels/19/49/34/c3c1e78bcb954c49e5ec0d31784fe63d14d427f316b12fbde9\n",
+      "Successfully built nvidia-ml-py3 shortuuid watchdog subprocess32 gql pathtools graphql-core promise\n",
+      "Installing collected packages: sentry-sdk, smmap2, gitdb2, GitPython, docker-pycreds, nvidia-ml-py3, shortuuid, pathtools, watchdog, subprocess32, configparser, promise, graphql-core, gql, wandb\n",
+      "Successfully installed GitPython-3.0.8 configparser-4.0.2 docker-pycreds-0.4.0 gitdb2-3.0.2 gql-0.2.0 graphql-core-1.1 nvidia-ml-py3-7.352.0 pathtools-0.1.2 promise-2.3 sentry-sdk-0.14.1 shortuuid-0.5.0 smmap2-2.0.5 subprocess32-3.5.4 wandb-0.8.27 watchdog-0.10.2\n"
+     ]
     }
    ],
    "source": [
+    "!pip install wandb\n",
     "import wandb\n",
     "from wandb.keras import WandbCallback"
    ]
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 9,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -778,13 +832,34 @@
     "outputId": "b05e251e-508f-46e6-865b-f869ae2a5dc4"
    },
    "outputs": [
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: ERROR Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
+     ]
+    },
+    {
+     "name": "stdin",
+     "output_type": "stream",
+     "text": [
+      "API Key:  ········································\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: Appending key for api.wandb.ai to your netrc file: /Users/Medy/.netrc\n"
+     ]
+    },
     {
      "data": {
       "text/html": [
        "\n",
        "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
-       "                Project page: <a href=\"https://app.wandb.ai/lambda-ds7/boston\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston</a><br/>\n",
-       "                Run page: <a href=\"https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31\" target=\"_blank\">https://app.wandb.ai/lambda-ds7/boston/runs/kkgdtc31</a><br/>\n",
+       "                Project page: <a href=\"https://app.wandb.ai/ds8/ds10_inclass\" target=\"_blank\">https://app.wandb.ai/ds8/ds10_inclass</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/ds8/ds10_inclass/runs/ehoraz8w\" target=\"_blank\">https://app.wandb.ai/ds8/ds10_inclass/runs/ehoraz8w</a><br/>\n",
        "            "
       ],
       "text/plain": [
@@ -800,120 +875,122 @@
      "text": [
       "Train on 270 samples, validate on 134 samples\n",
       "Epoch 1/50\n",
-      "270/270 [==============================] - 1s 3ms/sample - loss: 492.3539 - mse: 492.3539 - mae: 20.3197 - val_loss: 481.5445 - val_mse: 481.5445 - val_mae: 19.6138\n",
+      "270/270 [==============================] - 1s 2ms/sample - loss: 498.2942 - mse: 498.2943 - mae: 20.5189 - val_loss: 486.0071 - val_mse: 486.0071 - val_mae: 19.7923\n",
       "Epoch 2/50\n",
-      "270/270 [==============================] - 0s 591us/sample - loss: 239.4999 - mse: 239.4999 - mae: 12.8064 - val_loss: 113.8561 - val_mse: 113.8561 - val_mae: 8.2962\n",
+      "270/270 [==============================] - 0s 380us/sample - loss: 252.2446 - mse: 252.2446 - mae: 13.4180 - val_loss: 132.7781 - val_mse: 132.7782 - val_mae: 8.6736\n",
       "Epoch 3/50\n",
-      "270/270 [==============================] - 0s 618us/sample - loss: 56.2921 - mse: 56.2921 - mae: 5.8988 - val_loss: 62.7912 - val_mse: 62.7912 - val_mae: 5.6465\n",
+      "270/270 [==============================] - 0s 418us/sample - loss: 64.2867 - mse: 64.2867 - mae: 6.0175 - val_loss: 66.9466 - val_mse: 66.9466 - val_mae: 6.0974\n",
       "Epoch 4/50\n",
-      "270/270 [==============================] - 0s 613us/sample - loss: 29.4994 - mse: 29.4994 - mae: 3.9653 - val_loss: 37.9256 - val_mse: 37.9256 - val_mae: 4.1730\n",
+      "270/270 [==============================] - 0s 441us/sample - loss: 33.3175 - mse: 33.3176 - mae: 4.2253 - val_loss: 42.1238 - val_mse: 42.1238 - val_mae: 4.4530\n",
       "Epoch 5/50\n",
-      "270/270 [==============================] - 0s 608us/sample - loss: 20.6919 - mse: 20.6919 - mae: 3.3022 - val_loss: 31.7489 - val_mse: 31.7489 - val_mae: 3.7113\n",
+      "270/270 [==============================] - 0s 423us/sample - loss: 21.2557 - mse: 21.2557 - mae: 3.3677 - val_loss: 29.6810 - val_mse: 29.6810 - val_mae: 3.6970\n",
       "Epoch 6/50\n",
-      "270/270 [==============================] - 0s 602us/sample - loss: 17.2701 - mse: 17.2701 - mae: 3.0291 - val_loss: 27.3921 - val_mse: 27.3921 - val_mae: 3.4958\n",
+      "270/270 [==============================] - 0s 428us/sample - loss: 17.3445 - mse: 17.3445 - mae: 3.0095 - val_loss: 25.0756 - val_mse: 25.0756 - val_mae: 3.3614\n",
       "Epoch 7/50\n",
-      "270/270 [==============================] - 0s 671us/sample - loss: 15.5172 - mse: 15.5172 - mae: 2.8537 - val_loss: 25.3208 - val_mse: 25.3208 - val_mae: 3.3650\n",
+      "270/270 [==============================] - 0s 363us/sample - loss: 15.0026 - mse: 15.0026 - mae: 2.8043 - val_loss: 23.1811 - val_mse: 23.1811 - val_mae: 3.1743\n",
       "Epoch 8/50\n",
-      "270/270 [==============================] - 0s 661us/sample - loss: 13.7548 - mse: 13.7548 - mae: 2.7089 - val_loss: 23.8920 - val_mse: 23.8920 - val_mae: 3.2746\n",
+      "270/270 [==============================] - 0s 371us/sample - loss: 13.5016 - mse: 13.5016 - mae: 2.6876 - val_loss: 21.7962 - val_mse: 21.7962 - val_mae: 3.0188\n",
       "Epoch 9/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 12.3745 - mse: 12.3745 - mae: 2.5662 - val_loss: 22.1294 - val_mse: 22.1294 - val_mae: 3.1509\n",
+      "270/270 [==============================] - 0s 300us/sample - loss: 12.6361 - mse: 12.6361 - mae: 2.6260 - val_loss: 21.8546 - val_mse: 21.8546 - val_mae: 3.0023\n",
       "Epoch 10/50\n",
-      "270/270 [==============================] - 0s 614us/sample - loss: 11.2424 - mse: 11.2424 - mae: 2.4804 - val_loss: 20.5718 - val_mse: 20.5718 - val_mae: 3.0461\n",
+      "270/270 [==============================] - 0s 443us/sample - loss: 12.2522 - mse: 12.2522 - mae: 2.5996 - val_loss: 19.1793 - val_mse: 19.1793 - val_mae: 2.8183\n",
       "Epoch 11/50\n",
-      "270/270 [==============================] - 0s 605us/sample - loss: 10.6098 - mse: 10.6098 - mae: 2.4178 - val_loss: 20.3467 - val_mse: 20.3467 - val_mae: 3.0251\n",
+      "270/270 [==============================] - 0s 469us/sample - loss: 10.9517 - mse: 10.9517 - mae: 2.4586 - val_loss: 18.1045 - val_mse: 18.1045 - val_mae: 2.7688\n",
       "Epoch 12/50\n",
-      "270/270 [==============================] - 0s 576us/sample - loss: 10.0011 - mse: 10.0011 - mae: 2.3257 - val_loss: 18.4283 - val_mse: 18.4283 - val_mae: 2.8938\n",
+      "270/270 [==============================] - 0s 454us/sample - loss: 10.2551 - mse: 10.2551 - mae: 2.4091 - val_loss: 17.8173 - val_mse: 17.8173 - val_mae: 2.7970\n",
       "Epoch 13/50\n",
-      "270/270 [==============================] - 0s 666us/sample - loss: 9.1287 - mse: 9.1287 - mae: 2.2384 - val_loss: 18.2024 - val_mse: 18.2024 - val_mae: 2.9116\n",
+      "270/270 [==============================] - 0s 372us/sample - loss: 9.9049 - mse: 9.9049 - mae: 2.4060 - val_loss: 17.7149 - val_mse: 17.7149 - val_mae: 2.7521\n",
       "Epoch 14/50\n",
-      "270/270 [==============================] - 0s 603us/sample - loss: 8.6211 - mse: 8.6211 - mae: 2.1980 - val_loss: 17.4749 - val_mse: 17.4749 - val_mae: 2.8290\n",
+      "270/270 [==============================] - 0s 377us/sample - loss: 9.4655 - mse: 9.4655 - mae: 2.3557 - val_loss: 17.4696 - val_mse: 17.4696 - val_mae: 2.7279\n",
       "Epoch 15/50\n",
-      "270/270 [==============================] - 0s 463us/sample - loss: 8.4558 - mse: 8.4558 - mae: 2.2087 - val_loss: 17.7878 - val_mse: 17.7878 - val_mae: 2.8516\n",
+      "270/270 [==============================] - 0s 368us/sample - loss: 9.1189 - mse: 9.1189 - mae: 2.2518 - val_loss: 16.6399 - val_mse: 16.6399 - val_mae: 2.7049\n",
       "Epoch 16/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 8.3626 - mse: 8.3626 - mae: 2.2031 - val_loss: 16.7101 - val_mse: 16.7101 - val_mae: 2.7820\n",
+      "270/270 [==============================] - 0s 444us/sample - loss: 8.9746 - mse: 8.9746 - mae: 2.2997 - val_loss: 16.2888 - val_mse: 16.2888 - val_mae: 2.6501\n",
       "Epoch 17/50\n",
-      "270/270 [==============================] - 0s 607us/sample - loss: 7.9180 - mse: 7.9180 - mae: 2.1265 - val_loss: 16.6064 - val_mse: 16.6064 - val_mae: 2.7419\n",
+      "270/270 [==============================] - 0s 349us/sample - loss: 8.6089 - mse: 8.6089 - mae: 2.2615 - val_loss: 16.3454 - val_mse: 16.3454 - val_mae: 2.6575\n",
       "Epoch 18/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 7.5552 - mse: 7.5552 - mae: 2.0235 - val_loss: 17.2872 - val_mse: 17.2872 - val_mae: 2.8539\n",
+      "270/270 [==============================] - 0s 279us/sample - loss: 8.4496 - mse: 8.4496 - mae: 2.2172 - val_loss: 16.8456 - val_mse: 16.8456 - val_mae: 2.7705\n",
       "Epoch 19/50\n",
-      "270/270 [==============================] - 0s 616us/sample - loss: 7.0971 - mse: 7.0971 - mae: 2.0038 - val_loss: 16.5110 - val_mse: 16.5110 - val_mae: 2.8042\n",
+      "270/270 [==============================] - 0s 413us/sample - loss: 8.1690 - mse: 8.1690 - mae: 2.1764 - val_loss: 15.9263 - val_mse: 15.9263 - val_mae: 2.6247\n",
       "Epoch 20/50\n",
-      "270/270 [==============================] - 0s 606us/sample - loss: 6.7068 - mse: 6.7068 - mae: 1.9539 - val_loss: 15.5886 - val_mse: 15.5886 - val_mae: 2.7048\n",
+      "270/270 [==============================] - 0s 314us/sample - loss: 7.9765 - mse: 7.9765 - mae: 2.1426 - val_loss: 16.0577 - val_mse: 16.0577 - val_mae: 2.6912\n",
       "Epoch 21/50\n",
-      "270/270 [==============================] - 0s 461us/sample - loss: 6.8542 - mse: 6.8542 - mae: 1.9979 - val_loss: 17.2378 - val_mse: 17.2378 - val_mae: 2.8853\n",
+      "270/270 [==============================] - 0s 438us/sample - loss: 7.7312 - mse: 7.7312 - mae: 2.1348 - val_loss: 15.3937 - val_mse: 15.3937 - val_mae: 2.5881\n",
       "Epoch 22/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 6.5719 - mse: 6.5719 - mae: 1.9312 - val_loss: 16.3043 - val_mse: 16.3043 - val_mae: 2.7756\n",
+      "270/270 [==============================] - 0s 267us/sample - loss: 7.5357 - mse: 7.5357 - mae: 2.1164 - val_loss: 15.7274 - val_mse: 15.7274 - val_mae: 2.6367\n",
       "Epoch 23/50\n",
-      "270/270 [==============================] - 0s 478us/sample - loss: 6.6161 - mse: 6.6161 - mae: 1.9572 - val_loss: 15.7992 - val_mse: 15.7992 - val_mae: 2.7219\n",
+      "270/270 [==============================] - 0s 376us/sample - loss: 7.4007 - mse: 7.4007 - mae: 2.0909 - val_loss: 15.3527 - val_mse: 15.3527 - val_mae: 2.5956\n",
       "Epoch 24/50\n",
-      "270/270 [==============================] - 0s 491us/sample - loss: 7.1269 - mse: 7.1269 - mae: 2.0137 - val_loss: 16.5402 - val_mse: 16.5402 - val_mae: 2.8005\n",
+      "270/270 [==============================] - 0s 373us/sample - loss: 7.1959 - mse: 7.1959 - mae: 2.0427 - val_loss: 15.1225 - val_mse: 15.1224 - val_mae: 2.5838\n",
       "Epoch 25/50\n",
-      "270/270 [==============================] - 0s 479us/sample - loss: 6.3382 - mse: 6.3382 - mae: 1.8540 - val_loss: 16.5034 - val_mse: 16.5034 - val_mae: 2.7864\n",
+      "270/270 [==============================] - 0s 322us/sample - loss: 6.9660 - mse: 6.9660 - mae: 2.0042 - val_loss: 15.2860 - val_mse: 15.2860 - val_mae: 2.5961\n",
       "Epoch 26/50\n",
-      "270/270 [==============================] - 0s 488us/sample - loss: 5.9442 - mse: 5.9442 - mae: 1.8251 - val_loss: 15.6558 - val_mse: 15.6558 - val_mae: 2.7102\n",
+      "270/270 [==============================] - 0s 270us/sample - loss: 6.9847 - mse: 6.9847 - mae: 2.0423 - val_loss: 15.2981 - val_mse: 15.2981 - val_mae: 2.6202\n",
       "Epoch 27/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 5.5832 - mse: 5.5832 - mae: 1.7432 - val_loss: 15.3021 - val_mse: 15.3021 - val_mae: 2.6862\n",
+      "270/270 [==============================] - 0s 303us/sample - loss: 6.5799 - mse: 6.5799 - mae: 1.9457 - val_loss: 15.1740 - val_mse: 15.1740 - val_mae: 2.5838\n",
       "Epoch 28/50\n",
-      "270/270 [==============================] - 0s 436us/sample - loss: 5.4530 - mse: 5.4530 - mae: 1.7354 - val_loss: 15.4570 - val_mse: 15.4570 - val_mae: 2.6846\n",
+      "270/270 [==============================] - 0s 410us/sample - loss: 7.0830 - mse: 7.0830 - mae: 2.0199 - val_loss: 15.0156 - val_mse: 15.0156 - val_mae: 2.5775\n",
       "Epoch 29/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 5.3070 - mse: 5.3070 - mae: 1.7079 - val_loss: 15.8510 - val_mse: 15.8510 - val_mae: 2.7644\n",
+      "270/270 [==============================] - 0s 256us/sample - loss: 6.4519 - mse: 6.4519 - mae: 1.9778 - val_loss: 15.1387 - val_mse: 15.1387 - val_mae: 2.6098\n",
       "Epoch 30/50\n",
-      "270/270 [==============================] - 0s 477us/sample - loss: 5.4157 - mse: 5.4157 - mae: 1.7321 - val_loss: 15.9160 - val_mse: 15.9160 - val_mae: 2.7134\n",
+      "270/270 [==============================] - 0s 391us/sample - loss: 6.6461 - mse: 6.6461 - mae: 1.9628 - val_loss: 14.6683 - val_mse: 14.6683 - val_mae: 2.5251\n",
       "Epoch 31/50\n",
-      "270/270 [==============================] - 0s 452us/sample - loss: 5.2639 - mse: 5.2639 - mae: 1.6981 - val_loss: 15.3554 - val_mse: 15.3554 - val_mae: 2.6662\n",
+      "270/270 [==============================] - 0s 368us/sample - loss: 6.3484 - mse: 6.3484 - mae: 1.9341 - val_loss: 14.6111 - val_mse: 14.6111 - val_mae: 2.5012\n",
       "Epoch 32/50\n",
-      "270/270 [==============================] - 0s 475us/sample - loss: 5.7687 - mse: 5.7687 - mae: 1.8045 - val_loss: 15.7151 - val_mse: 15.7151 - val_mae: 2.6867\n",
+      "270/270 [==============================] - 0s 248us/sample - loss: 6.3857 - mse: 6.3857 - mae: 1.9295 - val_loss: 14.7763 - val_mse: 14.7763 - val_mae: 2.5361\n",
       "Epoch 33/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 5.5210 - mse: 5.5210 - mae: 1.7367 - val_loss: 15.4227 - val_mse: 15.4227 - val_mae: 2.6561\n",
+      "270/270 [==============================] - 0s 291us/sample - loss: 5.8491 - mse: 5.8491 - mae: 1.8578 - val_loss: 14.7988 - val_mse: 14.7988 - val_mae: 2.5904\n",
       "Epoch 34/50\n",
-      "270/270 [==============================] - 0s 474us/sample - loss: 5.5663 - mse: 5.5663 - mae: 1.7294 - val_loss: 15.3376 - val_mse: 15.3376 - val_mae: 2.6991\n",
+      "270/270 [==============================] - 0s 233us/sample - loss: 5.7845 - mse: 5.7845 - mae: 1.8211 - val_loss: 14.8456 - val_mse: 14.8456 - val_mae: 2.5414\n",
       "Epoch 35/50\n",
-      "270/270 [==============================] - 0s 626us/sample - loss: 5.0063 - mse: 5.0063 - mae: 1.6196 - val_loss: 15.2642 - val_mse: 15.2642 - val_mae: 2.6796\n",
+      "270/270 [==============================] - 0s 271us/sample - loss: 6.0509 - mse: 6.0509 - mae: 1.8778 - val_loss: 14.7619 - val_mse: 14.7619 - val_mae: 2.5335\n",
       "Epoch 36/50\n",
-      "270/270 [==============================] - 0s 459us/sample - loss: 4.7251 - mse: 4.7251 - mae: 1.5727 - val_loss: 15.4858 - val_mse: 15.4858 - val_mae: 2.7288\n",
+      "270/270 [==============================] - 0s 270us/sample - loss: 5.7594 - mse: 5.7594 - mae: 1.8511 - val_loss: 14.7488 - val_mse: 14.7488 - val_mae: 2.5355\n",
       "Epoch 37/50\n",
-      "270/270 [==============================] - 0s 604us/sample - loss: 4.6394 - mse: 4.6394 - mae: 1.5854 - val_loss: 15.1139 - val_mse: 15.1139 - val_mae: 2.6305\n",
+      "270/270 [==============================] - 0s 381us/sample - loss: 5.3358 - mse: 5.3358 - mae: 1.7497 - val_loss: 14.1933 - val_mse: 14.1933 - val_mae: 2.4454\n",
       "Epoch 38/50\n",
-      "270/270 [==============================] - 0s 592us/sample - loss: 4.5669 - mse: 4.5669 - mae: 1.5548 - val_loss: 14.9898 - val_mse: 14.9898 - val_mae: 2.6340\n",
+      "270/270 [==============================] - 0s 244us/sample - loss: 5.2591 - mse: 5.2591 - mae: 1.7467 - val_loss: 14.7518 - val_mse: 14.7518 - val_mae: 2.5461\n",
       "Epoch 39/50\n",
-      "270/270 [==============================] - 0s 458us/sample - loss: 4.4480 - mse: 4.4480 - mae: 1.5334 - val_loss: 15.6389 - val_mse: 15.6389 - val_mae: 2.7337\n",
+      "270/270 [==============================] - 0s 245us/sample - loss: 5.1493 - mse: 5.1493 - mae: 1.7381 - val_loss: 14.2647 - val_mse: 14.2647 - val_mae: 2.4660\n",
       "Epoch 40/50\n",
-      "270/270 [==============================] - 0s 455us/sample - loss: 4.4119 - mse: 4.4119 - mae: 1.5426 - val_loss: 15.0723 - val_mse: 15.0723 - val_mae: 2.6709\n",
+      "270/270 [==============================] - 0s 290us/sample - loss: 4.9535 - mse: 4.9535 - mae: 1.6774 - val_loss: 14.6520 - val_mse: 14.6520 - val_mae: 2.5023\n",
       "Epoch 41/50\n",
-      "270/270 [==============================] - 0s 473us/sample - loss: 4.0797 - mse: 4.0797 - mae: 1.4725 - val_loss: 15.4706 - val_mse: 15.4706 - val_mae: 2.6707\n",
+      "270/270 [==============================] - 0s 259us/sample - loss: 4.9499 - mse: 4.9499 - mae: 1.6674 - val_loss: 14.2618 - val_mse: 14.2618 - val_mae: 2.4672\n",
       "Epoch 42/50\n",
-      "270/270 [==============================] - 0s 449us/sample - loss: 4.0619 - mse: 4.0619 - mae: 1.4692 - val_loss: 15.2423 - val_mse: 15.2423 - val_mae: 2.6165\n",
+      "270/270 [==============================] - 0s 254us/sample - loss: 4.6686 - mse: 4.6686 - mae: 1.6199 - val_loss: 14.3855 - val_mse: 14.3855 - val_mae: 2.4932\n",
       "Epoch 43/50\n",
-      "270/270 [==============================] - 0s 465us/sample - loss: 4.1861 - mse: 4.1861 - mae: 1.5076 - val_loss: 15.7510 - val_mse: 15.7510 - val_mae: 2.7279\n",
+      "270/270 [==============================] - 0s 292us/sample - loss: 4.7633 - mse: 4.7633 - mae: 1.6736 - val_loss: 14.2871 - val_mse: 14.2871 - val_mae: 2.4610\n",
       "Epoch 44/50\n",
-      "270/270 [==============================] - 0s 462us/sample - loss: 4.1128 - mse: 4.1128 - mae: 1.4810 - val_loss: 15.4814 - val_mse: 15.4814 - val_mae: 2.6562\n",
+      "270/270 [==============================] - 0s 300us/sample - loss: 4.8230 - mse: 4.8230 - mae: 1.6844 - val_loss: 14.6300 - val_mse: 14.6300 - val_mae: 2.5152\n",
       "Epoch 45/50\n",
-      "270/270 [==============================] - 0s 441us/sample - loss: 4.2171 - mse: 4.2171 - mae: 1.5205 - val_loss: 16.3839 - val_mse: 16.3839 - val_mae: 2.8194\n",
+      "270/270 [==============================] - 0s 284us/sample - loss: 4.8192 - mse: 4.8192 - mae: 1.7054 - val_loss: 15.0184 - val_mse: 15.0184 - val_mae: 2.6116\n",
       "Epoch 46/50\n",
-      "270/270 [==============================] - 0s 422us/sample - loss: 4.2609 - mse: 4.2609 - mae: 1.5548 - val_loss: 15.3587 - val_mse: 15.3587 - val_mae: 2.7161\n",
+      "270/270 [==============================] - 0s 305us/sample - loss: 4.6228 - mse: 4.6228 - mae: 1.6161 - val_loss: 14.6888 - val_mse: 14.6888 - val_mae: 2.5272\n",
       "Epoch 47/50\n",
-      "270/270 [==============================] - 0s 454us/sample - loss: 4.4635 - mse: 4.4635 - mae: 1.5440 - val_loss: 15.7736 - val_mse: 15.7736 - val_mae: 2.7184\n",
+      "270/270 [==============================] - 0s 267us/sample - loss: 4.2253 - mse: 4.2253 - mae: 1.5679 - val_loss: 14.4713 - val_mse: 14.4713 - val_mae: 2.4671\n",
       "Epoch 48/50\n",
-      "270/270 [==============================] - 0s 426us/sample - loss: 3.7406 - mse: 3.7406 - mae: 1.4147 - val_loss: 15.6718 - val_mse: 15.6718 - val_mae: 2.7468\n",
+      "270/270 [==============================] - 0s 273us/sample - loss: 4.0277 - mse: 4.0277 - mae: 1.5407 - val_loss: 14.8140 - val_mse: 14.8140 - val_mae: 2.5884\n",
       "Epoch 49/50\n",
-      "270/270 [==============================] - 0s 445us/sample - loss: 3.6173 - mse: 3.6173 - mae: 1.3816 - val_loss: 15.7291 - val_mse: 15.7291 - val_mae: 2.7789\n",
+      "270/270 [==============================] - 0s 258us/sample - loss: 3.9287 - mse: 3.9287 - mae: 1.5031 - val_loss: 14.5323 - val_mse: 14.5323 - val_mae: 2.5237\n",
       "Epoch 50/50\n",
-      "270/270 [==============================] - 0s 430us/sample - loss: 3.6303 - mse: 3.6303 - mae: 1.4266 - val_loss: 15.4937 - val_mse: 15.4937 - val_mae: 2.7390\n"
+      "270/270 [==============================] - 0s 340us/sample - loss: 4.0444 - mse: 4.0444 - mae: 1.4934 - val_loss: 14.4696 - val_mse: 14.4696 - val_mae: 2.4843\n"
      ]
     },
     {
      "data": {
       "text/plain": [
-       "<tensorflow.python.keras.callbacks.History at 0x7f315c319be0>"
+       "<tensorflow.python.keras.callbacks.History at 0x141d5f350>"
       ]
      },
-     "execution_count": 8,
+     "execution_count": 9,
      "metadata": {},
      "output_type": "execute_result"
     }
    ],
    "source": [
-    "wandb.init(project=\"boston\", entity=\"lambda-ds7\") #Initializes and Experiment\n",
+    "# entity= 'ds8'\n",
+    "# project = 'ds10_inclass'\n",
+    "wandb.init(project=\"ds10_inclass\", entity=\"ds8\") #Initializes and Experiment\n",
     "\n",
     "# Important Hyperparameters\n",
     "X =  x_train\n",
@@ -975,7 +1052,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 9,
+   "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -1153,9 +1230,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "conda_tensorflow_p36",
+   "display_name": "Python 3",
    "language": "python",
-   "name": "conda_tensorflow_p36"
+   "name": "python3"
   },
   "language_info": {
    "codemirror_mode": {
@@ -1167,7 +1244,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.6.5"
+   "version": "3.7.4"
   }
  },
  "nbformat": 4,
