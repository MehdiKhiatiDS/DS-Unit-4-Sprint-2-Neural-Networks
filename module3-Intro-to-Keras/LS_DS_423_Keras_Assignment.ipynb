{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBQsZEJmubLs"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "\n",
    "# Neural Network Framework (Keras)\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignmnet 3*\n",
    "\n",
    "## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset\n",
    "\n",
    "- The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. \n",
    "- Normalize the data (all features should have roughly the same scale)\n",
    "- Import the type of model and layers that you will need from Keras.\n",
    "- Instantiate a model object and use `model.add()` to add layers to your model\n",
    "- Since this is a regression model you will have a single output node in the final layer.\n",
    "- Use activation functions that are appropriate for this task\n",
    "- Compile your model\n",
    "- Fit your model and report its accuracy in terms of Mean Squared Error\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Run this same data through a linear regression model. Which achieves higher accuracy?\n",
    "- Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)\n",
    "- After feature engineering, which model sees a greater accuracy boost due to the new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NLTAR87uYJ-"
   },
   "outputs": [],
   "source": [
    "##### Your Code Here #####\n",
    "# fit 2 model, regression and an image\n",
    "# normalize data no need for activation for last layer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[[1.23247, 0.0, 8.14, 0.0, 0.538, 6.142, 91.7,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[15.2, 42.3, 50.0, 21.1, 17.7, 18.5, 11.3, 15....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [[1.23247, 0.0, 8.14, 0.0, 0.538, 6.142, 91.7,...\n",
       "1  [15.2, 42.3, 50.0, 21.1, 17.7, 18.5, 11.3, 15...."
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame((x_train, y_train))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1     2    3      4      5     6       7    8      9    10  \\\n",
       "0  0.00632  18.0  2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3   \n",
       "1  0.02731   0.0  7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8   \n",
       "2  0.02729   0.0  7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8   \n",
       "3  0.03237   0.0  2.18  0.0  0.458  6.998  45.8  6.0622  3.0  222.0  18.7   \n",
       "4  0.06905   0.0  2.18  0.0  0.458  7.147  54.2  6.0622  3.0  222.0  18.7   \n",
       "\n",
       "       11    12  \n",
       "0  396.90  4.98  \n",
       "1  396.90  9.14  \n",
       "2  392.83  4.03  \n",
       "3  394.63  2.94  \n",
       "4  396.90  5.33  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston1 = pd.DataFrame(boston.data)\n",
    "boston1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston1.columns = boston.feature_names\n",
    "boston1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston1['PRICE'] = boston.target\n",
    "boston1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into X and y\n",
    "\n",
    "X = boston1.drop('PRICE', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = boston1['PRICE']\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X, Y, test_size = 0.33, random_state = 5)\n",
    "# print(X_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_train.shape)\n",
    "# print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  splitting the data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((339, 13), (339,))"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "norm = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = norm.fit_transform(X_train)\n",
    "# y_train = norm.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(13, input_dim=13, activation='relu', name='Dense'))\n",
    "model.add(Dense(26, activation='sigmoid'))\n",
    "model.add(Dense(15, activation='relu'))\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(1, input_dim=8, activation=\"sigmoid\")) #Relu is valid option. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense (Dense)                (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 26)                364       \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 15)                405       \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 7)                 112       \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 1,071\n",
      "Trainable params: 1,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mae', optimizer='nadam', metrics=['MSE'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 339 samples\n",
      "Epoch 1/140\n",
      "339/339 [==============================] - 1s 3ms/sample - loss: 22.4552 - MSE: 592.3795\n",
      "Epoch 2/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 22.4025 - MSE: 590.0137\n",
      "Epoch 3/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 22.3469 - MSE: 587.5545\n",
      "Epoch 4/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 22.2955 - MSE: 585.2293\n",
      "Epoch 5/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 22.2529 - MSE: 583.3076\n",
      "Epoch 6/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 22.2177 - MSE: 581.7603\n",
      "Epoch 7/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 22.1888 - MSE: 580.4866\n",
      "Epoch 8/140\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 22.1660 - MSE: 579.4767\n",
      "Epoch 9/140\n",
      "339/339 [==============================] - 0s 96us/sample - loss: 22.1479 - MSE: 578.6716\n",
      "Epoch 10/140\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 22.1335 - MSE: 578.0304\n",
      "Epoch 11/140\n",
      "339/339 [==============================] - 0s 93us/sample - loss: 22.1217 - MSE: 577.5120\n",
      "Epoch 12/140\n",
      "339/339 [==============================] - 0s 96us/sample - loss: 22.1118 - MSE: 577.0737\n",
      "Epoch 13/140\n",
      "339/339 [==============================] - 0s 89us/sample - loss: 22.1030 - MSE: 576.6846\n",
      "Epoch 14/140\n",
      "339/339 [==============================] - 0s 110us/sample - loss: 22.0951 - MSE: 576.3344\n",
      "Epoch 15/140\n",
      "339/339 [==============================] - 0s 96us/sample - loss: 22.0877 - MSE: 576.0101\n",
      "Epoch 16/140\n",
      "339/339 [==============================] - 0s 96us/sample - loss: 22.0805 - MSE: 575.6893\n",
      "Epoch 17/140\n",
      "339/339 [==============================] - 0s 96us/sample - loss: 22.0735 - MSE: 575.3854\n",
      "Epoch 18/140\n",
      "339/339 [==============================] - 0s 95us/sample - loss: 22.0666 - MSE: 575.0797\n",
      "Epoch 19/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 22.0599 - MSE: 574.7869\n",
      "Epoch 20/140\n",
      "339/339 [==============================] - 0s 92us/sample - loss: 22.0536 - MSE: 574.5021\n",
      "Epoch 21/140\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 22.0476 - MSE: 574.2405\n",
      "Epoch 22/140\n",
      "339/339 [==============================] - 0s 95us/sample - loss: 22.0423 - MSE: 574.0029\n",
      "Epoch 23/140\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 22.0374 - MSE: 573.7925\n",
      "Epoch 24/140\n",
      "339/339 [==============================] - 0s 93us/sample - loss: 22.0332 - MSE: 573.6046\n",
      "Epoch 25/140\n",
      "339/339 [==============================] - 0s 96us/sample - loss: 22.0295 - MSE: 573.4406\n",
      "Epoch 26/140\n",
      "339/339 [==============================] - 0s 93us/sample - loss: 22.0262 - MSE: 573.2960\n",
      "Epoch 27/140\n",
      "339/339 [==============================] - 0s 98us/sample - loss: 22.0233 - MSE: 573.1703\n",
      "Epoch 28/140\n",
      "339/339 [==============================] - 0s 107us/sample - loss: 22.0208 - MSE: 573.0583\n",
      "Epoch 29/140\n",
      "339/339 [==============================] - 0s 98us/sample - loss: 22.0185 - MSE: 572.9576\n",
      "Epoch 30/140\n",
      "339/339 [==============================] - 0s 101us/sample - loss: 22.0165 - MSE: 572.8676\n",
      "Epoch 31/140\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 22.0146 - MSE: 572.7870\n",
      "Epoch 32/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 22.0130 - MSE: 572.7142\n",
      "Epoch 33/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 22.0114 - MSE: 572.6469\n",
      "Epoch 34/140\n",
      "339/339 [==============================] - 0s 84us/sample - loss: 22.0100 - MSE: 572.5848\n",
      "Epoch 35/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 22.0087 - MSE: 572.5267\n",
      "Epoch 36/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 22.0075 - MSE: 572.4738\n",
      "Epoch 37/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 22.0064 - MSE: 572.4244\n",
      "Epoch 38/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 22.0053 - MSE: 572.3774\n",
      "Epoch 39/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 22.0043 - MSE: 572.3337\n",
      "Epoch 40/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 22.0034 - MSE: 572.2923\n",
      "Epoch 41/140\n",
      "339/339 [==============================] - 0s 76us/sample - loss: 22.0025 - MSE: 572.2525\n",
      "Epoch 42/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 22.0016 - MSE: 572.2162\n",
      "Epoch 43/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 22.0008 - MSE: 572.1803\n",
      "Epoch 44/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 22.0001 - MSE: 572.1470\n",
      "Epoch 45/140\n",
      "339/339 [==============================] - 0s 76us/sample - loss: 21.9993 - MSE: 572.1151\n",
      "Epoch 46/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9986 - MSE: 572.0842\n",
      "Epoch 47/140\n",
      "339/339 [==============================] - 0s 71us/sample - loss: 21.9980 - MSE: 572.0551\n",
      "Epoch 48/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9973 - MSE: 572.0268\n",
      "Epoch 49/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9967 - MSE: 572.0001\n",
      "Epoch 50/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9961 - MSE: 571.9740\n",
      "Epoch 51/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9956 - MSE: 571.9491\n",
      "Epoch 52/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 21.9950 - MSE: 571.9250\n",
      "Epoch 53/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9945 - MSE: 571.9020\n",
      "Epoch 54/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 21.9940 - MSE: 571.8799\n",
      "Epoch 55/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9935 - MSE: 571.8583\n",
      "Epoch 56/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9930 - MSE: 571.8375\n",
      "Epoch 57/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9926 - MSE: 571.8174\n",
      "Epoch 58/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9921 - MSE: 571.7980\n",
      "Epoch 59/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9917 - MSE: 571.7792\n",
      "Epoch 60/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9913 - MSE: 571.7612\n",
      "Epoch 61/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9909 - MSE: 571.7432\n",
      "Epoch 62/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9905 - MSE: 571.7262\n",
      "Epoch 63/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9901 - MSE: 571.7098\n",
      "Epoch 64/140\n",
      "339/339 [==============================] - 0s 83us/sample - loss: 21.9898 - MSE: 571.6936\n",
      "Epoch 65/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9894 - MSE: 571.6782\n",
      "Epoch 66/140\n",
      "339/339 [==============================] - 0s 73us/sample - loss: 21.9891 - MSE: 571.6631\n",
      "Epoch 67/140\n",
      "339/339 [==============================] - 0s 76us/sample - loss: 21.9887 - MSE: 571.6484\n",
      "Epoch 68/140\n",
      "339/339 [==============================] - 0s 86us/sample - loss: 21.9884 - MSE: 571.6340\n",
      "Epoch 69/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9881 - MSE: 571.6202\n",
      "Epoch 70/140\n",
      "339/339 [==============================] - 0s 83us/sample - loss: 21.9878 - MSE: 571.6068\n",
      "Epoch 71/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 21.9875 - MSE: 571.5934\n",
      "Epoch 72/140\n",
      "339/339 [==============================] - 0s 83us/sample - loss: 21.9872 - MSE: 571.5807\n",
      "Epoch 73/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9869 - MSE: 571.5683\n",
      "Epoch 74/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 21.9866 - MSE: 571.5562\n",
      "Epoch 75/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9864 - MSE: 571.5444\n",
      "Epoch 76/140\n",
      "339/339 [==============================] - 0s 83us/sample - loss: 21.9861 - MSE: 571.5328\n",
      "Epoch 77/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 21.9858 - MSE: 571.5215\n",
      "Epoch 78/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9856 - MSE: 571.5107\n",
      "Epoch 79/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9854 - MSE: 571.5001\n",
      "Epoch 80/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9851 - MSE: 571.4896\n",
      "Epoch 81/140\n",
      "339/339 [==============================] - 0s 76us/sample - loss: 21.9849 - MSE: 571.4794\n",
      "Epoch 82/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 21.9847 - MSE: 571.4695\n",
      "Epoch 83/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9844 - MSE: 571.4600\n",
      "Epoch 84/140\n",
      "339/339 [==============================] - 0s 76us/sample - loss: 21.9842 - MSE: 571.4504\n",
      "Epoch 85/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9840 - MSE: 571.4412\n",
      "Epoch 86/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 21.9838 - MSE: 571.4323\n",
      "Epoch 87/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9836 - MSE: 571.4235\n",
      "Epoch 88/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9834 - MSE: 571.4150\n",
      "Epoch 89/140\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 21.9832 - MSE: 571.4066\n",
      "Epoch 90/140\n",
      "339/339 [==============================] - 0s 84us/sample - loss: 21.9830 - MSE: 571.3983\n",
      "Epoch 91/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9829 - MSE: 571.3903\n",
      "Epoch 92/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9827 - MSE: 571.3824\n",
      "Epoch 93/140\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 21.9825 - MSE: 571.3747\n",
      "Epoch 94/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9823 - MSE: 571.3671\n",
      "Epoch 95/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9822 - MSE: 571.3598\n",
      "Epoch 96/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9820 - MSE: 571.3526\n",
      "Epoch 97/140\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 21.9818 - MSE: 571.3455\n",
      "Epoch 98/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9817 - MSE: 571.3386\n",
      "Epoch 99/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9815 - MSE: 571.3320\n",
      "Epoch 100/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9814 - MSE: 571.3253\n",
      "Epoch 101/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9812 - MSE: 571.3188\n",
      "Epoch 102/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9811 - MSE: 571.3124\n",
      "Epoch 103/140\n",
      "339/339 [==============================] - 0s 83us/sample - loss: 21.9809 - MSE: 571.3063\n",
      "Epoch 104/140\n",
      "339/339 [==============================] - 0s 92us/sample - loss: 21.9808 - MSE: 571.3002\n",
      "Epoch 105/140\n",
      "339/339 [==============================] - 0s 85us/sample - loss: 21.9807 - MSE: 571.2943\n",
      "Epoch 106/140\n",
      "339/339 [==============================] - 0s 86us/sample - loss: 21.9805 - MSE: 571.2883\n",
      "Epoch 107/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9804 - MSE: 571.2826\n",
      "Epoch 108/140\n",
      "339/339 [==============================] - 0s 88us/sample - loss: 21.9803 - MSE: 571.2771\n",
      "Epoch 109/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9802 - MSE: 571.2714\n",
      "Epoch 110/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9800 - MSE: 571.2661\n",
      "Epoch 111/140\n",
      "339/339 [==============================] - 0s 84us/sample - loss: 21.9799 - MSE: 571.2608\n",
      "Epoch 112/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 21.9798 - MSE: 571.2556\n",
      "Epoch 113/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9797 - MSE: 571.2505\n",
      "Epoch 114/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9796 - MSE: 571.2454\n",
      "Epoch 115/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9794 - MSE: 571.2404\n",
      "Epoch 116/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9793 - MSE: 571.2356\n",
      "Epoch 117/140\n",
      "339/339 [==============================] - 0s 91us/sample - loss: 21.9792 - MSE: 571.2309\n",
      "Epoch 118/140\n",
      "339/339 [==============================] - 0s 92us/sample - loss: 21.9791 - MSE: 571.2262\n",
      "Epoch 119/140\n",
      "339/339 [==============================] - 0s 120us/sample - loss: 21.9790 - MSE: 571.2216\n",
      "Epoch 120/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 21.9789 - MSE: 571.2172\n",
      "Epoch 121/140\n",
      "339/339 [==============================] - 0s 84us/sample - loss: 21.9788 - MSE: 571.2127\n",
      "Epoch 122/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9787 - MSE: 571.2084\n",
      "Epoch 123/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9786 - MSE: 571.2042\n",
      "Epoch 124/140\n",
      "339/339 [==============================] - 0s 75us/sample - loss: 21.9785 - MSE: 571.2000\n",
      "Epoch 125/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9784 - MSE: 571.1959\n",
      "Epoch 126/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9783 - MSE: 571.1918\n",
      "Epoch 127/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9783 - MSE: 571.1878\n",
      "Epoch 128/140\n",
      "339/339 [==============================] - 0s 82us/sample - loss: 21.9782 - MSE: 571.1839\n",
      "Epoch 129/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9781 - MSE: 571.1802\n",
      "Epoch 130/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9780 - MSE: 571.1763\n",
      "Epoch 131/140\n",
      "339/339 [==============================] - 0s 81us/sample - loss: 21.9779 - MSE: 571.1726\n",
      "Epoch 132/140\n",
      "339/339 [==============================] - 0s 76us/sample - loss: 21.9778 - MSE: 571.1691\n",
      "Epoch 133/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9777 - MSE: 571.1654\n",
      "Epoch 134/140\n",
      "339/339 [==============================] - 0s 87us/sample - loss: 21.9777 - MSE: 571.1619\n",
      "Epoch 135/140\n",
      "339/339 [==============================] - 0s 79us/sample - loss: 21.9776 - MSE: 571.1584\n",
      "Epoch 136/140\n",
      "339/339 [==============================] - 0s 74us/sample - loss: 21.9775 - MSE: 571.1550\n",
      "Epoch 137/140\n",
      "339/339 [==============================] - 0s 78us/sample - loss: 21.9774 - MSE: 571.1516\n",
      "Epoch 138/140\n",
      "339/339 [==============================] - 0s 80us/sample - loss: 21.9774 - MSE: 571.1483\n",
      "Epoch 139/140\n",
      "339/339 [==============================] - 0s 77us/sample - loss: 21.9773 - MSE: 571.1450\n",
      "Epoch 140/140\n",
      "339/339 [==============================] - 0s 94us/sample - loss: 21.9772 - MSE: 571.1418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14791c290>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 660us/sample - loss: 20.6500 - MSE: 502.1028\n",
      "MSE: 502.1028137207031\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)\n",
    "print(f\"{model.metrics_names[1]}: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'metrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-397-426125a4747c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#plot training & validation loss values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MSE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'metrics'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plot training & validation accuracy values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['MSE'])\n",
    "\n",
    "# plt.plot(fit.history['val_loss'])\n",
    "plt.title(\"Model mse\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#plot training & validation loss values\n",
    "\n",
    "plt.plot(history.history['metrics'])\n",
    "plt.plot(history.history['MSE'])\n",
    "\n",
    "# plt.plot(fit.history['val_loss'])\n",
    "plt.title(\"Model mse\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('mse')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense (Dense)                (None, 13)                182       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 26)                364       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 15)                405       \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 7)                 112       \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 1)                 8         \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 7)                 14        \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,131\n",
      "Trainable params: 1,131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 =Sequential()\n",
    "model1.add(Dense(13, input_dim=13, activation='relu', name='Dense'))\n",
    "model1.add(Dense(26, activation='relu'))\n",
    "model1.add(Dense(15, activation='relu'))\n",
    "model1.add(Dense(7, activation='relu'))\n",
    "model1.add(Dense(1, activation='relu'))\n",
    "model1.add(Dense(7, activation='relu'))\n",
    "model1.add(Dense(5, activation='relu'))\n",
    "model1.add(Dense(1, activation='relu'))\n",
    "model1.compile(loss='mae', optimizer='nadam', metrics=['MSE'])\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected Dense_input to have shape (13,) but got array with shape (784,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-348335d5c376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    580\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected Dense_input to have shape (13,) but got array with shape (784,)"
     ]
    }
   ],
   "source": [
    "model1.fit(X_train, y_train, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 714us/sample - loss: 333.8532 - MSE: 544.1288\n",
      "MSE: 544.1287841796875\n"
     ]
    }
   ],
   "source": [
    "scores = model1.evaluate(X_test, y_test)\n",
    "print(f\"{model1.metrics_names[1]}: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense (Dense)                (None, 130)               1820      \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 260)               34060     \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 150)               39150     \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 75)                11325     \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 19)                1444      \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 7)                 140       \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 5)                 40        \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 87,985\n",
      "Trainable params: 87,985\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 =Sequential()\n",
    "model2.add(Dense(130, input_dim=13, activation='relu', name='Dense'))\n",
    "model2.add(Dense(260, activation='relu'))\n",
    "model2.add(Dense(150, activation='relu'))\n",
    "model2.add(Dense(75, activation='relu'))\n",
    "model2.add(Dense(19, activation='relu'))\n",
    "model2.add(Dense(7, activation='relu'))\n",
    "model2.add(Dense(5, activation='relu'))\n",
    "model2.add(Dense(1, activation='relu'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['MSE'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 3 µs, total: 6 µs\n",
      "Wall time: 11.9 µs\n",
      "Train on 339 samples\n",
      "Epoch 1/80\n",
      "339/339 [==============================] - 0s 104us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 2/80\n",
      "339/339 [==============================] - 0s 85us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 3/80\n",
      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 4/80\n",
      "339/339 [==============================] - 0s 81us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 5/80\n",
      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 6/80\n",
      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 7/80\n",
      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6403\n",
      "Epoch 8/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 9/80\n",
      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 10/80\n",
      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 11/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 12/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 13/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 14/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 15/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 16/80\n",
      "339/339 [==============================] - 0s 73us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 17/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 18/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 19/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 20/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 21/80\n",
      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 22/80\n",
      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 23/80\n",
      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 24/80\n",
      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 25/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 26/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 27/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 28/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 29/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 30/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 31/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 32/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 33/80\n",
      "339/339 [==============================] - 0s 78us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 34/80\n",
      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 35/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 36/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 37/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 38/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 39/80\n",
      "339/339 [==============================] - 0s 87us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 40/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 41/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 42/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 43/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 44/80\n",
      "339/339 [==============================] - 0s 87us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 45/80\n",
      "339/339 [==============================] - 0s 92us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 46/80\n",
      "339/339 [==============================] - 0s 93us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 47/80\n",
      "339/339 [==============================] - 0s 115us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 48/80\n",
      "339/339 [==============================] - 0s 121us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 49/80\n",
      "339/339 [==============================] - 0s 111us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 50/80\n",
      "339/339 [==============================] - 0s 117us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 51/80\n",
      "339/339 [==============================] - 0s 118us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 52/80\n",
      "339/339 [==============================] - 0s 117us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 53/80\n",
      "339/339 [==============================] - 0s 111us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 54/80\n",
      "339/339 [==============================] - 0s 91us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 55/80\n",
      "339/339 [==============================] - 0s 94us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 56/80\n",
      "339/339 [==============================] - 0s 108us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 57/80\n",
      "339/339 [==============================] - 0s 94us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 58/80\n",
      "339/339 [==============================] - 0s 92us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 59/80\n",
      "339/339 [==============================] - 0s 91us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 60/80\n",
      "339/339 [==============================] - 0s 85us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 61/80\n",
      "339/339 [==============================] - 0s 84us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 62/80\n",
      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6403\n",
      "Epoch 63/80\n",
      "339/339 [==============================] - 0s 80us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 64/80\n",
      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 65/80\n",
      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 66/80\n",
      "339/339 [==============================] - 0s 82us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 67/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 68/80\n",
      "339/339 [==============================] - 0s 73us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 69/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 70/80\n",
      "339/339 [==============================] - 0s 72us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 71/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6403\n",
      "Epoch 72/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 73/80\n",
      "339/339 [==============================] - 0s 77us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 74/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 75/80\n",
      "339/339 [==============================] - 0s 74us/sample - loss: -335.0379 - MSE: 421.6403\n",
      "Epoch 76/80\n",
      "339/339 [==============================] - 0s 75us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 77/80\n",
      "339/339 [==============================] - 0s 76us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 78/80\n",
      "339/339 [==============================] - 0s 79us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 79/80\n",
      "339/339 [==============================] - 0s 86us/sample - loss: -335.0379 - MSE: 421.6404\n",
      "Epoch 80/80\n",
      "339/339 [==============================] - 0s 92us/sample - loss: -335.0379 - MSE: 421.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x144223710>"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time\n",
    "model2.fit(X_train, y_train, epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 63us/sample - loss: -314.8009 - MSE: 4444057.5000\n",
      "MSE: 4444057.5\n"
     ]
    }
   ],
   "source": [
    "scores = model2.evaluate(X_test, y_test)\n",
    "print(f\"{model2.metrics_names[1]}: {scores[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model.fit(X_train, y_train, epochs=300, validation_split=.1, verbose=False)\n",
    "history = model.fit(X_train, y_train, epochs=100, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167/167 [==============================] - 0s 53us/sample - loss: -741.5163 - MSE: 501.8414\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 501.84136962890625\n"
     ]
    }
   ],
   "source": [
    "print(f'{model.metrics_names[1]}: {scores[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(fit.history['mse'])\n",
    "# plt.plot(fit.history['val_mse'])\n",
    "plt.plot(fit.history['loss'])\n",
    "# plt.plot(fit.history['val_loss'])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so cranking the density way high or the poisson optimizer did do well. ! after little testing \n",
    "# it was the optimzer poisson that sucked but cranking the numbers helped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will run a linear regression model to compare\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140199499.60049394"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# simpler linear regression model\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# linreg = LinearRegression()\n",
    "# linreg.fit(X_train, y_train)\n",
    "# linreg_pred = linreg.predict(X_test)\n",
    "# mean_squared_error(linreg_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = LinearRegression()\n",
    "    \n",
    "# Fit on train, score on val\n",
    "\n",
    "model3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model3.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 140199499.60 \n"
     ]
    }
   ],
   "source": [
    "print(f'Test Error: {mse:.2f} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
    "X_train = encoder.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SfcFnOONyuNm"
   },
   "source": [
    "## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)\n",
    "\n",
    "- Load and preprocess the image data similar to how we preprocessed the MNIST data in class.\n",
    "- Make sure to one-hot encode your category labels\n",
    "- Make sure to have your final layer have as many nodes as the number of classes that you want to predict.\n",
    "- Try different hyperparameters. What is the highest accuracy that you are able to achieve.\n",
    "- Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. \n",
    "- Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szi6-IpuzaH1"
   },
   "outputs": [],
   "source": [
    "#### Your Code Here ####\n",
    "# image classification \n",
    "# bonus lecture 3 on how to load\n",
    "# hotencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's do it!\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Stretch - use dropout \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(x_train[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 64\n",
    "num_classes = 10\n",
    "# epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# X variable type\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# correct encoding for y\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_100 (Dense)            (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 13,393\n",
      "Trainable params: 13,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist_model = Sequential()\n",
    "\n",
    "# input => Hidden\n",
    "fashion_mnist_model.add(Dense(16, input_dim = 784, activation = 'relu'))\n",
    "# hidden\n",
    "fashion_mnist_model.add(Dense(16, activation = 'relu'))\n",
    "# hidden\n",
    "fashion_mnist_model.add(Dense(16, activation = 'relu'))\n",
    "# hidden\n",
    "fashion_mnist_model.add(Dense(16, activation = 'relu'))\n",
    "# output\n",
    "fashion_mnist_model.add(Dense(1, activation = 'softmax'))\n",
    "# compile\n",
    "fashion_mnist_model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer= 'adam',\n",
    "                           metrics= ['accuracy'])\n",
    "# summary\n",
    "fashion_mnist_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (60000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-327-9fb9a9bdaedd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfashion_mnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfashion_mnist_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2487\u001b[0m           \u001b[0;31m# Additional checks to avoid users mistakenly using improper loss fns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m           training_utils.check_loss_and_target_compatibility(\n\u001b[0;32m-> 2489\u001b[0;31m               y, self._feed_loss_fns, feed_output_shapes)\n\u001b[0m\u001b[1;32m   2490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m       sample_weights, _, _ = training_utils.handle_partial_sample_weights(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, loss_fns, output_shapes)\u001b[0m\n\u001b[1;32m    782\u001b[0m         raise ValueError('You are passing a target array of shape ' +\n\u001b[1;32m    783\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m                          \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m                          \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                          \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (60000, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "hist = fashion_mnist_model.fit(X_train, y_train, batch_size=100, epochs=20, verbose=False)\n",
    "scores = fashion_mnist_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zv_3xNMjzdLI"
   },
   "source": [
    "## Stretch Goals:\n",
    "\n",
    "- Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)\n",
    "- Use Cross Validation techniques to get more consistent results with your model.\n",
    "- Use GridSearchCV to try different combinations of hyperparameters. \n",
    "- Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_433_Keras_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
